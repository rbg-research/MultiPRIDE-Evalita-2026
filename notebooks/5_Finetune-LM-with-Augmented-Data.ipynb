{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d25684-e586-487d-98b1-238b28f88468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n",
      "training files: ['train_en.csv', 'train_it.csv', 'train_es.csv']\n",
      "Total training samples: 2988\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 2560 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 428 (14.3%)\n",
      "  Total: 2988\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=938, Class 1=88, Total=1026\n",
      "  ES: Class 0=743, Class 1=133, Total=876\n",
      "  IT: Class 0=879, Class 1=207, Total=1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from src.baseline.baseline import train_df, figures_root\n",
    "from src.finetune.finetuner import main\n",
    "from src.baseline.utils import calculate_class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00741d54-c2c7-4631-8b57-db05296b47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbe48fa-4751-4a12-92bd-8337b71f51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5) (5976, 8)\n"
     ]
    }
   ],
   "source": [
    "original_data = train_df\n",
    "augmented_data = pd.read_csv(\"../data/augmented_multilingual_tweets.csv\")\n",
    "print(original_data.shape, augmented_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27231170-0d78-446d-aa0e-23dea9c8d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8964, 5)\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.concat([original_data, augmented_data[list(original_data.columns)]], ignore_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c6197b-8700-4659-9e27-e6ada9529d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd97a2a-12b3-4a25-a8c0-045f562b7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 5120 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 856 (14.3%)\n",
      "  Total: 5976\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=1622, Class 1=340, Total=1962\n",
      "  ES: Class 0=1817, Class 1=295, Total=2112\n",
      "  IT: Class 0=1681, Class 1=221, Total=1902\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_class_distribution(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a2e02-03da-4580-8e39-1c90af03de75",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f0123e-cffa-4119-be97-d6476b7a91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training would be start on the device: cuda\n",
      "Training would be start on the device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for fine-tuning\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"  # Base model\n",
    "    NUM_LABELS = 2  # Binary classification\n",
    "    MAX_LENGTH = 128  # Maximum sequence length\n",
    "    NUM_FROZEN_LAYERS = 3  # Number of initial layers to freeze (0 = only train classification head)\n",
    "\n",
    "    # Training configuration\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01 \n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    WARMUP_RATIO = 0.15  # Warmup as % of total steps\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"  # Evaluate at end of each epoch\n",
    "\n",
    "    # Cross-validation\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8  # 80% for training from each fold\n",
    "    VAL_RATIO = 0.2  # 20% for validation from each fold\n",
    "\n",
    "    # Dynamic undersampling\n",
    "    DYNAMIC_UNDERSAMPLE = False  # Balance classes per epoch\n",
    "\n",
    "    # Model saving\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models\"\n",
    "    RESULTS_DIR = \"../results/roberta-fine-tune/original_and_augumented/\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training would be start on the device: {DEVICE}\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for fine-tuning\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"  # Base model\n",
    "    NUM_LABELS = 2  # Binary classification\n",
    "    MAX_LENGTH = 128  # Maximum sequence length\n",
    "    NUM_FROZEN_LAYERS = 3  # Number of initial layers to freeze (0 = only train classification head)\n",
    "\n",
    "    # Training configuration\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    WARMUP_RATIO = 0.1  # Warmup as % of total steps\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"  # Evaluate at end of each epoch\n",
    "\n",
    "    # Cross-validation\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8  # 80% for training from each fold\n",
    "    VAL_RATIO = 0.2  # 20% for validation from each fold\n",
    "\n",
    "    # Dynamic undersampling\n",
    "    DYNAMIC_UNDERSAMPLE = False  # Balance classes per epoch\n",
    "\n",
    "    # Model saving\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models\"\n",
    "    RESULTS_DIR = \"../results/roberta-fine-tune/augumented/\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training would be start on the device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4bd487-c125-4c65-a2af-4e1c3cf124a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:37:22,828 - INFO - ================================================================================\n",
      "2025-12-03 13:37:22,829 - INFO - Starting Fine-tuning Pipeline\n",
      "2025-12-03 13:37:22,829 - INFO - ================================================================================\n",
      "2025-12-03 13:37:22,836 - INFO - Fold 0: Train=3824, Val=956\n",
      "2025-12-03 13:37:22,837 - INFO -   Train label dist: {0: 3276, 1: 548}\n",
      "2025-12-03 13:37:22,837 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 13:37:22,841 - INFO - Fold 1: Train=3824, Val=957\n",
      "2025-12-03 13:37:22,841 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 13:37:22,842 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 13:37:22,846 - INFO - Fold 2: Train=3824, Val=957\n",
      "2025-12-03 13:37:22,846 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 13:37:22,846 - INFO -   Train lang dist: {'es': 1352, 'en': 1256, 'it': 1216}\n",
      "2025-12-03 13:37:22,850 - INFO - Fold 3: Train=3824, Val=957\n",
      "2025-12-03 13:37:22,851 - INFO -   Train label dist: {0: 3278, 1: 546}\n",
      "2025-12-03 13:37:22,851 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 13:37:22,855 - INFO - Fold 4: Train=3824, Val=957\n",
      "2025-12-03 13:37:22,856 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 13:37:22,856 - INFO -   Train lang dist: {'es': 1351, 'en': 1256, 'it': 1217}\n",
      "2025-12-03 13:37:22,857 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:37:22,857 - INFO - Fold 1/5\n",
      "2025-12-03 13:37:22,857 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0:\n",
      "  Train: 548 positive samples\n",
      "  Val:   137 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 13:37:26,256 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 13:37:26,256 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 13:37:26,257 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 13:37:26,258 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-12-03 13:37:26,258 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 13:37:26,258 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-12-03 13:37:26,259 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.51it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 108.23it/s]\n",
      "2025-12-03 13:37:40,850 - INFO - Train Loss: 0.1340\n",
      "2025-12-03 13:37:40,850 - INFO - Val Loss: 0.1165\n",
      "2025-12-03 13:37:40,850 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 13:37:40,850 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:37:40,850 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 13:37:40,850 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:37:41,515 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt (F1: 0.4614, Fold: 0, Epoch: 0)\n",
      "2025-12-03 13:37:41,515 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.71it/s, loss=0.135]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.65it/s]\n",
      "2025-12-03 13:37:55,987 - INFO - Train Loss: 0.1349\n",
      "2025-12-03 13:37:55,988 - INFO - Val Loss: 0.1219\n",
      "2025-12-03 13:37:55,988 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 13:37:55,988 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:37:55,988 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 13:37:55,988 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:37:55,988 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.08it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 107.92it/s]\n",
      "2025-12-03 13:38:10,361 - INFO - Train Loss: 0.1255\n",
      "2025-12-03 13:38:10,362 - INFO - Val Loss: 0.1158\n",
      "2025-12-03 13:38:10,362 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 13:38:10,362 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:38:10,362 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 13:38:10,363 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:38:10,363 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.36it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 106.01it/s]\n",
      "2025-12-03 13:38:25,023 - INFO - Train Loss: 0.1227\n",
      "2025-12-03 13:38:25,024 - INFO - Val Loss: 0.1066\n",
      "2025-12-03 13:38:25,024 - INFO - Overall - Precision: 0.9288, Recall: 0.5036, F1: 0.4689\n",
      "2025-12-03 13:38:25,024 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:38:25,024 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 13:38:25,024 - INFO - it - Precision: 0.9424, Recall: 0.5139, F1: 0.4965\n",
      "2025-12-03 13:38:25,686 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_3_f1_0.4689.pt (F1: 0.4689, Fold: 0, Epoch: 3)\n",
      "2025-12-03 13:38:25,686 - INFO - \n",
      "Epoch 5/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.55it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 107.08it/s]\n",
      "2025-12-03 13:38:40,268 - INFO - Train Loss: 0.1144\n",
      "2025-12-03 13:38:40,268 - INFO - Val Loss: 0.1019\n",
      "2025-12-03 13:38:40,268 - INFO - Overall - Precision: 0.7003, Recall: 0.5219, F1: 0.5081\n",
      "2025-12-03 13:38:40,268 - INFO - en - Precision: 0.7546, Recall: 0.5498, F1: 0.5501\n",
      "2025-12-03 13:38:40,268 - INFO - es - Precision: 0.4301, Recall: 0.4983, F1: 0.4617\n",
      "2025-12-03 13:38:40,268 - INFO - it - Precision: 0.6087, Recall: 0.5102, F1: 0.4932\n",
      "2025-12-03 13:38:40,329 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt\n",
      "2025-12-03 13:38:40,996 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_4_f1_0.5081.pt (F1: 0.5081, Fold: 0, Epoch: 4)\n",
      "2025-12-03 13:38:40,997 - INFO - \n",
      "Epoch 6/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.88it/s, loss=0.11] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.38it/s]\n",
      "2025-12-03 13:38:55,416 - INFO - Train Loss: 0.1094\n",
      "2025-12-03 13:38:55,417 - INFO - Val Loss: 0.1085\n",
      "2025-12-03 13:38:55,417 - INFO - Overall - Precision: 0.7185, Recall: 0.5541, F1: 0.5637\n",
      "2025-12-03 13:38:55,417 - INFO - en - Precision: 0.7726, Recall: 0.6088, F1: 0.6357\n",
      "2025-12-03 13:38:55,417 - INFO - es - Precision: 0.5987, Recall: 0.5144, F1: 0.4983\n",
      "2025-12-03 13:38:55,417 - INFO - it - Precision: 0.6433, Recall: 0.5222, F1: 0.5163\n",
      "2025-12-03 13:38:55,485 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_3_f1_0.4689.pt\n",
      "2025-12-03 13:38:56,155 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_5_f1_0.5637.pt (F1: 0.5637, Fold: 0, Epoch: 5)\n",
      "2025-12-03 13:38:56,155 - INFO - \n",
      "Epoch 7/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.46it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.25it/s]\n",
      "2025-12-03 13:39:10,356 - INFO - Train Loss: 0.1049\n",
      "2025-12-03 13:39:10,357 - INFO - Val Loss: 0.1049\n",
      "2025-12-03 13:39:10,357 - INFO - Overall - Precision: 0.7557, Recall: 0.6052, F1: 0.6342\n",
      "2025-12-03 13:39:10,357 - INFO - en - Precision: 0.7909, Recall: 0.6605, F1: 0.6953\n",
      "2025-12-03 13:39:10,357 - INFO - es - Precision: 0.7673, Recall: 0.6067, F1: 0.6374\n",
      "2025-12-03 13:39:10,357 - INFO - it - Precision: 0.5858, Recall: 0.5185, F1: 0.5121\n",
      "2025-12-03 13:39:10,425 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_4_f1_0.5081.pt\n",
      "2025-12-03 13:39:11,090 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_6_f1_0.6342.pt (F1: 0.6342, Fold: 0, Epoch: 6)\n",
      "2025-12-03 13:39:11,090 - INFO - \n",
      "Epoch 8/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.07it/s, loss=0.101]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 107.12it/s]\n",
      "2025-12-03 13:39:25,473 - INFO - Train Loss: 0.1003\n",
      "2025-12-03 13:39:25,473 - INFO - Val Loss: 0.1130\n",
      "2025-12-03 13:39:25,474 - INFO - Overall - Precision: 0.7794, Recall: 0.6107, F1: 0.6427\n",
      "2025-12-03 13:39:25,474 - INFO - en - Precision: 0.8111, Recall: 0.6717, F1: 0.7094\n",
      "2025-12-03 13:39:25,474 - INFO - es - Precision: 0.7877, Recall: 0.6084, F1: 0.6411\n",
      "2025-12-03 13:39:25,474 - INFO - it - Precision: 0.6098, Recall: 0.5203, F1: 0.5142\n",
      "2025-12-03 13:39:25,541 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_5_f1_0.5637.pt\n",
      "2025-12-03 13:39:26,205 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_7_f1_0.6427.pt (F1: 0.6427, Fold: 0, Epoch: 7)\n",
      "2025-12-03 13:39:26,206 - INFO - \n",
      "Epoch 9/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.38it/s, loss=0.1]   \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.45it/s]\n",
      "2025-12-03 13:39:40,435 - INFO - Train Loss: 0.1000\n",
      "2025-12-03 13:39:40,435 - INFO - Val Loss: 0.1010\n",
      "2025-12-03 13:39:40,435 - INFO - Overall - Precision: 0.7366, Recall: 0.6660, F1: 0.6913\n",
      "2025-12-03 13:39:40,436 - INFO - en - Precision: 0.7628, Recall: 0.7266, F1: 0.7422\n",
      "2025-12-03 13:39:40,436 - INFO - es - Precision: 0.7633, Recall: 0.6725, F1: 0.7034\n",
      "2025-12-03 13:39:40,436 - INFO - it - Precision: 0.6244, Recall: 0.5629, F1: 0.5764\n",
      "2025-12-03 13:39:40,503 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_6_f1_0.6342.pt\n",
      "2025-12-03 13:39:41,159 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.6913.pt (F1: 0.6913, Fold: 0, Epoch: 8)\n",
      "2025-12-03 13:39:41,160 - INFO - \n",
      "Epoch 10/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.18it/s, loss=0.087] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.83it/s]\n",
      "2025-12-03 13:39:55,447 - INFO - Train Loss: 0.0869\n",
      "2025-12-03 13:39:55,447 - INFO - Val Loss: 0.1124\n",
      "2025-12-03 13:39:55,447 - INFO - Overall - Precision: 0.7474, Recall: 0.6332, F1: 0.6640\n",
      "2025-12-03 13:39:55,448 - INFO - en - Precision: 0.7933, Recall: 0.6937, F1: 0.7265\n",
      "2025-12-03 13:39:55,448 - INFO - es - Precision: 0.7886, Recall: 0.6280, F1: 0.6641\n",
      "2025-12-03 13:39:55,448 - INFO - it - Precision: 0.6026, Recall: 0.5490, F1: 0.5585\n",
      "2025-12-03 13:39:55,448 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:39:55,448 - INFO - Fold 2/5\n",
      "2025-12-03 13:39:55,448 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 13:39:58,533 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 13:39:58,533 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 13:39:58,534 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 13:39:58,535 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 13:39:58,535 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 13:39:58,535 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 13:39:58,537 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.29it/s, loss=0.135]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.06it/s]\n",
      "2025-12-03 13:40:12,809 - INFO - Train Loss: 0.1345\n",
      "2025-12-03 13:40:12,809 - INFO - Val Loss: 0.1162\n",
      "2025-12-03 13:40:12,809 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:40:12,809 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:40:12,809 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:40:12,810 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:40:13,464 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 1, Epoch: 0)\n",
      "2025-12-03 13:40:13,464 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.40it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.78it/s]\n",
      "2025-12-03 13:40:27,692 - INFO - Train Loss: 0.1342\n",
      "2025-12-03 13:40:27,692 - INFO - Val Loss: 0.1189\n",
      "2025-12-03 13:40:27,692 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:40:27,693 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:40:27,693 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:40:27,693 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:40:27,693 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.21it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 109.48it/s]\n",
      "2025-12-03 13:40:42,002 - INFO - Train Loss: 0.1259\n",
      "2025-12-03 13:40:42,002 - INFO - Val Loss: 0.1161\n",
      "2025-12-03 13:40:42,003 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:40:42,003 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:40:42,003 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:40:42,003 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:40:42,003 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.04it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.12it/s]\n",
      "2025-12-03 13:40:56,367 - INFO - Train Loss: 0.1222\n",
      "2025-12-03 13:40:56,367 - INFO - Val Loss: 0.1028\n",
      "2025-12-03 13:40:56,368 - INFO - Overall - Precision: 0.4278, Recall: 0.4994, F1: 0.4608\n",
      "2025-12-03 13:40:56,368 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:40:56,368 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:40:56,368 - INFO - it - Precision: 0.4408, Recall: 0.4981, F1: 0.4677\n",
      "2025-12-03 13:40:56,368 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 13:40:56,369 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:40:56,369 - INFO - Fold 3/5\n",
      "2025-12-03 13:40:56,369 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 2:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 13:40:59,413 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 13:40:59,414 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 13:40:59,414 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 13:40:59,415 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 13:40:59,415 - INFO - Language weights: {'es': 0.9409476243674836, 'en': 1.0128671880134061, 'it': 1.0461851876191102}\n",
      "2025-12-03 13:40:59,416 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 13:40:59,417 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.17it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.90it/s]\n",
      "2025-12-03 13:41:13,725 - INFO - Train Loss: 0.1298\n",
      "2025-12-03 13:41:13,725 - INFO - Val Loss: 0.1165\n",
      "2025-12-03 13:41:13,725 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:41:13,725 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:41:13,725 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:41:13,725 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:41:14,394 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 2, Epoch: 0)\n",
      "2025-12-03 13:41:14,394 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.21it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.48it/s]\n",
      "2025-12-03 13:41:28,674 - INFO - Train Loss: 0.1340\n",
      "2025-12-03 13:41:28,674 - INFO - Val Loss: 0.1228\n",
      "2025-12-03 13:41:28,675 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:41:28,675 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:41:28,675 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:41:28,675 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:41:28,675 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.95it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 108.26it/s]\n",
      "2025-12-03 13:41:43,092 - INFO - Train Loss: 0.1272\n",
      "2025-12-03 13:41:43,092 - INFO - Val Loss: 0.1179\n",
      "2025-12-03 13:41:43,092 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:41:43,092 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:41:43,093 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:41:43,093 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:41:43,093 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.32it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.68it/s]\n",
      "2025-12-03 13:41:57,329 - INFO - Train Loss: 0.1215\n",
      "2025-12-03 13:41:57,329 - INFO - Val Loss: 0.1133\n",
      "2025-12-03 13:41:57,330 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:41:57,330 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:41:57,330 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:41:57,330 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:41:57,331 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 13:41:57,331 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:41:57,331 - INFO - Fold 4/5\n",
      "2025-12-03 13:41:57,331 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 3:\n",
      "  Train: 546 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 13:42:00,453 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 13:42:00,453 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 13:42:00,454 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 13:42:00,455 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-12-03 13:42:00,455 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 13:42:00,455 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-12-03 13:42:00,457 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.96it/s, loss=0.136]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.29it/s]\n",
      "2025-12-03 13:42:14,837 - INFO - Train Loss: 0.1355\n",
      "2025-12-03 13:42:14,837 - INFO - Val Loss: 0.1167\n",
      "2025-12-03 13:42:14,837 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:42:14,837 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 13:42:14,838 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:42:14,838 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 13:42:15,504 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 3, Epoch: 0)\n",
      "2025-12-03 13:42:15,504 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.16it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.30it/s]\n",
      "2025-12-03 13:42:29,814 - INFO - Train Loss: 0.1319\n",
      "2025-12-03 13:42:29,814 - INFO - Val Loss: 0.1185\n",
      "2025-12-03 13:42:29,814 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:42:29,815 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 13:42:29,815 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:42:29,815 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 13:42:29,815 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.04it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.63it/s]\n",
      "2025-12-03 13:42:44,147 - INFO - Train Loss: 0.1302\n",
      "2025-12-03 13:42:44,147 - INFO - Val Loss: 0.1159\n",
      "2025-12-03 13:42:44,148 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:42:44,148 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 13:42:44,148 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:42:44,148 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 13:42:44,148 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.06it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.15it/s]\n",
      "2025-12-03 13:42:58,507 - INFO - Train Loss: 0.1244\n",
      "2025-12-03 13:42:58,508 - INFO - Val Loss: 0.1068\n",
      "2025-12-03 13:42:58,508 - INFO - Overall - Precision: 0.4278, Recall: 0.4994, F1: 0.4608\n",
      "2025-12-03 13:42:58,508 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 13:42:58,508 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:42:58,508 - INFO - it - Precision: 0.4422, Recall: 0.4981, F1: 0.4685\n",
      "2025-12-03 13:42:58,508 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 13:42:58,508 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:42:58,509 - INFO - Fold 5/5\n",
      "2025-12-03 13:42:58,509 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 4:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 13:43:01,543 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 13:43:01,543 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 13:43:01,544 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 13:43:01,545 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 13:43:01,545 - INFO - Language weights: {'es': 0.9416953224870754, 'en': 1.0129222776114961, 'it': 1.045382399901429}\n",
      "2025-12-03 13:43:01,545 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 13:43:01,546 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.94it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.51it/s]\n",
      "2025-12-03 13:43:15,954 - INFO - Train Loss: 0.1318\n",
      "2025-12-03 13:43:15,954 - INFO - Val Loss: 0.1163\n",
      "2025-12-03 13:43:15,954 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:43:15,954 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:43:15,955 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:43:15,955 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:43:16,634 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 4, Epoch: 0)\n",
      "2025-12-03 13:43:16,635 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.23it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.25it/s]\n",
      "2025-12-03 13:43:30,900 - INFO - Train Loss: 0.1304\n",
      "2025-12-03 13:43:30,900 - INFO - Val Loss: 0.1207\n",
      "2025-12-03 13:43:30,900 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:43:30,900 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:43:30,900 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:43:30,900 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:43:30,900 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.14it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.53it/s]\n",
      "2025-12-03 13:43:45,215 - INFO - Train Loss: 0.1292\n",
      "2025-12-03 13:43:45,215 - INFO - Val Loss: 0.1110\n",
      "2025-12-03 13:43:45,216 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:43:45,216 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:43:45,216 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:43:45,216 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:43:45,216 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.05it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.22it/s]\n",
      "2025-12-03 13:43:59,558 - INFO - Train Loss: 0.1214\n",
      "2025-12-03 13:43:59,558 - INFO - Val Loss: 0.1029\n",
      "2025-12-03 13:43:59,559 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 13:43:59,559 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 13:43:59,559 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 13:43:59,560 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 13:43:59,560 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 13:43:59,568 - INFO - \n",
      "Results saved to: ../results/roberta-fine-tune/augumented/training_results.csv\n",
      "2025-12-03 13:43:59,568 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 13:43:59,569 - INFO - Best Models Saved:\n",
      "2025-12-03 13:43:59,569 - INFO - ================================================================================\n",
      "2025-12-03 13:43:59,569 - INFO - Fold 0, Epoch 8: F1=0.6913 -> ../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.6913.pt\n",
      "2025-12-03 13:43:59,569 - INFO - Fold 0, Epoch 7: F1=0.6427 -> ../fine_tuned_models/checkpoints/fold_0_epoch_7_f1_0.6427.pt\n"
     ]
    }
   ],
   "source": [
    "main(train_data, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141d468-18bd-4a32-878e-ea24c52d6ee7",
   "metadata": {},
   "source": [
    "### Visualizing Fine-Tune Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f968616-5ab7-4911-a664-6746ac34d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/roberta-fine-tune/augumented/training_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669256a3-d5ef-427e-97fa-5649a8d91ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_fold_augmented.svg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_plot = alt.Chart(df).mark_line(point=True, size=3).encode(\n",
    "    x=alt.X('epoch:Q', title='Epoch'),\n",
    "    y=alt.Y('overall_macro_f1:Q', title='Macro F1 Score', scale=alt.Scale(domain=[0.3, 0.8])),\n",
    "    color=alt.Color('fold:N', title='Fold'),\n",
    "    tooltip=['fold:N', 'epoch:Q', alt.Tooltip('overall_macro_f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Overall F1 Score by Fold')\n",
    "f1_plot.save(os.path.join(figures_root, 'f1_vs_fold_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_fold_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aeffa-3c2b-4e12-9d73-176d7f020dc9",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_fold_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc1abcc-a363-4160-b763-684a111b67cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/loss_augmented.svg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_data = df[['fold', 'epoch', 'train_loss', 'val_loss']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='loss_type', value_name='loss'\n",
    ")\n",
    "\n",
    "loss_plot = alt.Chart(loss_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y='loss:Q', color='loss_type:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'loss_type:N', alt.Tooltip('loss:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Training & Validation Loss')\n",
    "\n",
    "loss_plot.save(os.path.join(figures_root, 'loss_augmented.svg'))\n",
    "os.path.join(figures_root, 'loss_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149bf7-3ed2-45af-8954-1af12c737f15",
   "metadata": {},
   "source": [
    "![](../figures/loss_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db4cc8b-762d-46a5-ae47-122d777394a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_lang_augmented.svg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_data = df[['fold', 'epoch', 'en_macro_f1', 'es_macro_f1', 'it_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='language', value_name='f1'\n",
    ")\n",
    "lang_data['language'] = lang_data['language'].str.replace('_macro_f1', '').str.upper()\n",
    "\n",
    "lang_plot = alt.Chart(lang_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y=alt.Y('f1:Q', scale=alt.Scale(domain=[0.4, 0.95])), color='language:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'language:N', alt.Tooltip('f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='F1 by Language')\n",
    "\n",
    "lang_plot.save(os.path.join(figures_root, 'f1_vs_lang_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_lang_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e06673-7cd3-44fb-b8e4-af3a8e495fbd",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_lang_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b491471f-fb74-4ab7-bf4c-d9fbc0663cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/fold_vs_epoch_augmented.svg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = df[['fold', 'epoch', 'overall_macro_precision', 'overall_macro_recall', 'overall_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='metric', value_name='value'\n",
    ")\n",
    "metrics_data['metric'] = metrics_data['metric'].str.replace('overall_macro_', '').str.capitalize()\n",
    "metrics_data['fold_epoch'] = 'F' + metrics_data['fold'].astype(str) + ':E' + metrics_data['epoch'].astype(str)\n",
    "\n",
    "heatmap = alt.Chart(metrics_data).mark_rect().encode(\n",
    "    x='fold_epoch:O', y='metric:N',\n",
    "    color=alt.Color('value:Q', scale=alt.Scale(scheme='viridis')),\n",
    "    tooltip=['fold_epoch:N', 'metric:N', alt.Tooltip('value:Q', format='.4f')]\n",
    ").properties(width=700, height=150, title='Precision/Recall/F1 Heatmap')\n",
    "\n",
    "heatmap.save(os.path.join(figures_root, 'fold_vs_epoch_augmented.svg'))\n",
    "os.path.join(figures_root, 'fold_vs_epoch_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f99d57-d61d-4de7-974a-32f713900f1d",
   "metadata": {},
   "source": [
    "![](../figures/fold_vs_epoch_augmented.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afbe71-d966-4561-b119-9e1bed6f4e4b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef7c0fd-3153-43a2-8393-b9bfecb5f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n",
      "training files: ['train_en.csv', 'train_it.csv', 'train_es.csv']\n",
      "Total training samples: 2988\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 2560 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 428 (14.3%)\n",
      "  Total: 2988\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=938, Class 1=88, Total=1026\n",
      "  ES: Class 0=743, Class 1=133, Total=876\n",
      "  IT: Class 0=879, Class 1=207, Total=1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your training data\n",
    "from src.baseline.baseline import train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8848534-c659-40e8-9ab8-9724f7a56c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    CHECKPOINT_PATH = \"../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.6913.pt\"\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_LABELS = 2\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c7681f-96f1-44e4-9598-b2224ad1a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Dataset for inference\"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51abcda-364e-45b2-9cf6-c20c6fbfbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds: List[int], labels: List[int], languages: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculates and returns evaluation metrics for predictions and labels.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    macro_precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    metrics[\"overall\"] = {\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    unique_langs = set(languages)\n",
    "    for lang in unique_langs:\n",
    "        lang_mask = np.array([l == lang for l in languages])\n",
    "        lang_preds = np.array(preds)[lang_mask]\n",
    "        lang_labels = np.array(labels)[lang_mask]\n",
    "\n",
    "        if len(lang_labels) > 0:\n",
    "            lang_precision = precision_score(\n",
    "                lang_labels, lang_preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            lang_recall = recall_score(\n",
    "                lang_labels, lang_preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            lang_f1 = f1_score(lang_labels, lang_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "            metrics[lang] = {\n",
    "                \"macro_precision\": lang_precision,\n",
    "                \"macro_recall\": lang_recall,\n",
    "                \"macro_f1\": lang_f1,\n",
    "            }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f92fb3-5360-44ce-9dd6-f83a10d570d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(df: pd.DataFrame, config: InferenceConfig) -> Dict:\n",
    "    print(f\"Running inference on {len(df)} samples...\")\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        num_labels=config.NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint weights - HANDLE DIFFERENT CHECKPOINT FORMATS\n",
    "    checkpoint = torch.load(config.CHECKPOINT_PATH, map_location=config.DEVICE)\n",
    "    \n",
    "    # Try different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"model_state_dict\"]\n",
    "        elif \"state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            # Assume the entire checkpoint IS the state dict\n",
    "            state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Loaded checkpoint from: {config.CHECKPOINT_PATH}\")\n",
    "    \n",
    "    # Try to print checkpoint metadata if available\n",
    "    if isinstance(checkpoint, dict):\n",
    "        for key in [\"fold\", \"epoch\", \"f1_score\"]:\n",
    "            if key in checkpoint:\n",
    "                print(f\"  {key.capitalize()}: {checkpoint[key]}\")\n",
    "    \n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare dataset and dataloader\n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    languages = df[\"lang\"].tolist()\n",
    "    \n",
    "    dataset = TweetDataset(texts, tokenizer, config.MAX_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Run inference\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(config.DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_preds, labels, languages)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INFERENCE RESULTS ON TRAIN DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Macro Precision: {metrics['overall']['macro_precision']:.4f}\")\n",
    "    print(f\"  Macro Recall:    {metrics['overall']['macro_recall']:.4f}\")\n",
    "    print(f\"  Macro F1:        {metrics['overall']['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Language Metrics:\")\n",
    "    for lang in sorted([k for k in metrics.keys() if k != \"overall\"]):\n",
    "        print(f\"  {lang.upper()}:\")\n",
    "        print(f\"    Precision: {metrics[lang]['macro_precision']:.4f}\")\n",
    "        print(f\"    Recall:    {metrics[lang]['macro_recall']:.4f}\")\n",
    "        print(f\"    F1:        {metrics[lang]['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"predictions\": all_preds,\n",
    "        \"probabilities\": all_probs,\n",
    "        \"labels\": labels,\n",
    "        \"languages\": languages,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739db1b9-9320-4ca4-a271-c7f17c940ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 2988 samples...\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: ../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.6913.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 94/94 [00:02<00:00, 37.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE RESULTS ON TRAIN DATA\n",
      "================================================================================\n",
      "\n",
      "Overall Metrics:\n",
      "  Macro Precision: 0.6256\n",
      "  Macro Recall:    0.6417\n",
      "  Macro F1:        0.6326\n",
      "\n",
      "Per-Language Metrics:\n",
      "  EN:\n",
      "    Precision: 0.5329\n",
      "    Recall:    0.5842\n",
      "    F1:        0.5134\n",
      "  ES:\n",
      "    Precision: 0.6827\n",
      "    Recall:    0.6815\n",
      "    F1:        0.6821\n",
      "  IT:\n",
      "    Precision: 0.9160\n",
      "    Recall:    0.6819\n",
      "    F1:        0.7302\n"
     ]
    }
   ],
   "source": [
    "results = run_inference(train_df, InferenceConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189552dd-bd35-4b07-aded-4e5c141483c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
