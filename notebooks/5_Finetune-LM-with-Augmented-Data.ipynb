{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d25684-e586-487d-98b1-238b28f88468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 19:56:25.094077: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 19:56:25.121161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-03 19:56:25.760486: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n",
      "training files: ['train_en.csv', 'train_it.csv', 'train_es.csv']\n",
      "Total training samples: 2988\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 2560 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 428 (14.3%)\n",
      "  Total: 2988\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=938, Class 1=88, Total=1026\n",
      "  ES: Class 0=743, Class 1=133, Total=876\n",
      "  IT: Class 0=879, Class 1=207, Total=1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from src.baseline.baseline import train_df, figures_root\n",
    "from src.finetune.finetuner import main\n",
    "from src.baseline.utils import calculate_class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00741d54-c2c7-4631-8b57-db05296b47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbe48fa-4751-4a12-92bd-8337b71f51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5) (5976, 8)\n"
     ]
    }
   ],
   "source": [
    "original_data = train_df\n",
    "augmented_data = pd.read_csv(\"../data/augmented_multilingual_tweets.csv\")\n",
    "print(original_data.shape, augmented_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27231170-0d78-446d-aa0e-23dea9c8d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8964, 5)\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.concat([original_data, augmented_data[list(original_data.columns)]], ignore_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c6197b-8700-4659-9e27-e6ada9529d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd97a2a-12b3-4a25-a8c0-045f562b7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 5120 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 856 (14.3%)\n",
      "  Total: 5976\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=1622, Class 1=340, Total=1962\n",
      "  ES: Class 0=1817, Class 1=295, Total=2112\n",
      "  IT: Class 0=1681, Class 1=221, Total=1902\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_class_distribution(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a2e02-03da-4580-8e39-1c90af03de75",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f0123e-cffa-4119-be97-d6476b7a91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training would be start on the device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for fine-tuning\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model\"  # Base model\n",
    "    NUM_LABELS = 2  # Binary classification\n",
    "    MAX_LENGTH = 128  # Maximum sequence length\n",
    "    NUM_FROZEN_LAYERS = 3  # Number of initial layers to freeze (0 = only train classification head)\n",
    "\n",
    "    # Training configuration\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01 \n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    WARMUP_RATIO = 0.15  # Warmup as % of total steps\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"  # Evaluate at end of each epoch\n",
    "\n",
    "    # Cross-validation\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8  # 80% for training from each fold\n",
    "    VAL_RATIO = 0.2  # 20% for validation from each fold\n",
    "\n",
    "    # Dynamic undersampling\n",
    "    DYNAMIC_UNDERSAMPLE = False  # Balance classes per epoch\n",
    "\n",
    "    # Model saving\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models\"\n",
    "    RESULTS_DIR = \"../results/roberta-fine-tune/original_and_augumented_mlm/\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training would be start on the device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4bd487-c125-4c65-a2af-4e1c3cf124a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 19:56:36,630 - INFO - ================================================================================\n",
      "2025-12-03 19:56:36,631 - INFO - Starting Fine-tuning Pipeline\n",
      "2025-12-03 19:56:36,631 - INFO - ================================================================================\n",
      "2025-12-03 19:56:36,640 - INFO - Fold 0: Train=3824, Val=956\n",
      "2025-12-03 19:56:36,640 - INFO -   Train label dist: {0: 3276, 1: 548}\n",
      "2025-12-03 19:56:36,641 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 19:56:36,645 - INFO - Fold 1: Train=3824, Val=957\n",
      "2025-12-03 19:56:36,645 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 19:56:36,646 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 19:56:36,650 - INFO - Fold 2: Train=3824, Val=957\n",
      "2025-12-03 19:56:36,650 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 19:56:36,651 - INFO -   Train lang dist: {'es': 1352, 'en': 1256, 'it': 1216}\n",
      "2025-12-03 19:56:36,655 - INFO - Fold 3: Train=3824, Val=957\n",
      "2025-12-03 19:56:36,655 - INFO -   Train label dist: {0: 3278, 1: 546}\n",
      "2025-12-03 19:56:36,655 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-03 19:56:36,659 - INFO - Fold 4: Train=3824, Val=957\n",
      "2025-12-03 19:56:36,660 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-03 19:56:36,660 - INFO -   Train lang dist: {'es': 1351, 'en': 1256, 'it': 1217}\n",
      "2025-12-03 19:56:36,661 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 19:56:36,661 - INFO - Fold 1/5\n",
      "2025-12-03 19:56:36,661 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0:\n",
      "  Train: 548 positive samples\n",
      "  Val:   137 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 19:56:37,456 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 19:56:37,457 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 19:56:37,457 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 19:56:37,458 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-12-03 19:56:37,459 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 19:56:37,459 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-12-03 19:56:37,460 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.33it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.42it/s]\n",
      "2025-12-03 19:56:52,079 - INFO - Train Loss: 0.1319\n",
      "2025-12-03 19:56:52,079 - INFO - Val Loss: 0.1138\n",
      "2025-12-03 19:56:52,080 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 19:56:52,080 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:56:52,080 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 19:56:52,080 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:56:52,975 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt (F1: 0.4614, Fold: 0, Epoch: 0)\n",
      "2025-12-03 19:56:52,976 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.77it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.11it/s]\n",
      "2025-12-03 19:57:07,423 - INFO - Train Loss: 0.1325\n",
      "2025-12-03 19:57:07,423 - INFO - Val Loss: 0.1191\n",
      "2025-12-03 19:57:07,423 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 19:57:07,423 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:57:07,423 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 19:57:07,423 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:57:07,424 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.72it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.42it/s]\n",
      "2025-12-03 19:57:21,895 - INFO - Train Loss: 0.1263\n",
      "2025-12-03 19:57:21,895 - INFO - Val Loss: 0.1169\n",
      "2025-12-03 19:57:21,896 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 19:57:21,896 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:57:21,896 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 19:57:21,896 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:57:21,896 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.40it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.42it/s]\n",
      "2025-12-03 19:57:36,487 - INFO - Train Loss: 0.1254\n",
      "2025-12-03 19:57:36,487 - INFO - Val Loss: 0.1110\n",
      "2025-12-03 19:57:36,487 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-03 19:57:36,487 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:57:36,487 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-03 19:57:36,487 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:57:36,488 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 19:57:36,488 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 19:57:36,488 - INFO - Fold 2/5\n",
      "2025-12-03 19:57:36,488 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 19:57:37,192 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 19:57:37,193 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 19:57:37,193 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 19:57:37,194 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 19:57:37,195 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 19:57:37,195 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 19:57:37,197 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.66it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.39it/s]\n",
      "2025-12-03 19:57:51,716 - INFO - Train Loss: 0.1321\n",
      "2025-12-03 19:57:51,716 - INFO - Val Loss: 0.1139\n",
      "2025-12-03 19:57:51,716 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:57:51,716 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:57:51,716 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:57:51,716 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:57:52,669 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 1, Epoch: 0)\n",
      "2025-12-03 19:57:52,670 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.40it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.63it/s]\n",
      "2025-12-03 19:58:07,273 - INFO - Train Loss: 0.1314\n",
      "2025-12-03 19:58:07,273 - INFO - Val Loss: 0.1173\n",
      "2025-12-03 19:58:07,273 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:58:07,274 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:58:07,274 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:58:07,274 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:58:07,274 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.66it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.10it/s]\n",
      "2025-12-03 19:58:21,771 - INFO - Train Loss: 0.1266\n",
      "2025-12-03 19:58:21,771 - INFO - Val Loss: 0.1173\n",
      "2025-12-03 19:58:21,771 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:58:21,771 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:58:21,772 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:58:21,772 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:58:21,772 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.61it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.54it/s]\n",
      "2025-12-03 19:58:36,291 - INFO - Train Loss: 0.1241\n",
      "2025-12-03 19:58:36,292 - INFO - Val Loss: 0.1087\n",
      "2025-12-03 19:58:36,292 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:58:36,292 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:58:36,292 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:58:36,292 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:58:36,292 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 19:58:36,292 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 19:58:36,293 - INFO - Fold 3/5\n",
      "2025-12-03 19:58:36,293 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 2:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 19:58:36,967 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 19:58:36,968 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 19:58:36,968 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 19:58:36,970 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 19:58:36,970 - INFO - Language weights: {'es': 0.9409476243674836, 'en': 1.0128671880134061, 'it': 1.0461851876191102}\n",
      "2025-12-03 19:58:36,970 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 19:58:36,971 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.25it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.82it/s]\n",
      "2025-12-03 19:58:51,252 - INFO - Train Loss: 0.1280\n",
      "2025-12-03 19:58:51,252 - INFO - Val Loss: 0.1139\n",
      "2025-12-03 19:58:51,252 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:58:51,252 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:58:51,252 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:58:51,252 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:58:51,261 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt\n",
      "2025-12-03 19:58:51,923 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 2, Epoch: 0)\n",
      "2025-12-03 19:58:51,923 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.47it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 109.34it/s]\n",
      "2025-12-03 19:59:06,509 - INFO - Train Loss: 0.1319\n",
      "2025-12-03 19:59:06,509 - INFO - Val Loss: 0.1191\n",
      "2025-12-03 19:59:06,509 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:59:06,510 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:59:06,510 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:59:06,510 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:59:06,510 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.64it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.28it/s]\n",
      "2025-12-03 19:59:21,011 - INFO - Train Loss: 0.1266\n",
      "2025-12-03 19:59:21,011 - INFO - Val Loss: 0.1173\n",
      "2025-12-03 19:59:21,012 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:59:21,012 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:59:21,012 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:59:21,012 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:59:21,012 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.65it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.53it/s]\n",
      "2025-12-03 19:59:35,507 - INFO - Train Loss: 0.1238\n",
      "2025-12-03 19:59:35,507 - INFO - Val Loss: 0.1148\n",
      "2025-12-03 19:59:35,507 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:59:35,507 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 19:59:35,508 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:59:35,508 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 19:59:35,508 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 19:59:35,508 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 19:59:35,508 - INFO - Fold 4/5\n",
      "2025-12-03 19:59:35,508 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 3:\n",
      "  Train: 546 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 19:59:36,171 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 19:59:36,171 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 19:59:36,172 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 19:59:36,173 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-12-03 19:59:36,173 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-03 19:59:36,173 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-12-03 19:59:36,175 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:12<00:00, 36.86it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 114.01it/s]\n",
      "2025-12-03 19:59:50,214 - INFO - Train Loss: 0.1332\n",
      "2025-12-03 19:59:50,215 - INFO - Val Loss: 0.1138\n",
      "2025-12-03 19:59:50,215 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 19:59:50,215 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 19:59:50,215 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 19:59:50,215 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 19:59:50,222 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4611.pt\n",
      "2025-12-03 19:59:50,868 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 3, Epoch: 0)\n",
      "2025-12-03 19:59:50,869 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.74it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.95it/s]\n",
      "2025-12-03 20:00:04,946 - INFO - Train Loss: 0.1299\n",
      "2025-12-03 20:00:04,946 - INFO - Val Loss: 0.1168\n",
      "2025-12-03 20:00:04,946 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:00:04,946 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 20:00:04,946 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:00:04,946 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 20:00:04,946 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:12<00:00, 36.80it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.96it/s]\n",
      "2025-12-03 20:00:18,999 - INFO - Train Loss: 0.1306\n",
      "2025-12-03 20:00:18,999 - INFO - Val Loss: 0.1166\n",
      "2025-12-03 20:00:19,000 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:00:19,000 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 20:00:19,000 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:00:19,000 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 20:00:19,000 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.76it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 114.34it/s]\n",
      "2025-12-03 20:00:33,065 - INFO - Train Loss: 0.1256\n",
      "2025-12-03 20:00:33,065 - INFO - Val Loss: 0.1109\n",
      "2025-12-03 20:00:33,066 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:00:33,066 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-03 20:00:33,066 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:00:33,066 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-03 20:00:33,066 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 20:00:33,066 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 20:00:33,067 - INFO - Fold 5/5\n",
      "2025-12-03 20:00:33,067 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 4:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models/mlm_twitter_xlm_roberta_optuna/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-03 20:00:33,726 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-03 20:00:33,726 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-03 20:00:33,727 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-03 20:00:33,728 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-03 20:00:33,728 - INFO - Language weights: {'es': 0.9416953224870754, 'en': 1.0129222776114961, 'it': 1.045382399901429}\n",
      "2025-12-03 20:00:33,728 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-03 20:00:33,729 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:12<00:00, 36.82it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 114.09it/s]\n",
      "2025-12-03 20:00:47,787 - INFO - Train Loss: 0.1307\n",
      "2025-12-03 20:00:47,787 - INFO - Val Loss: 0.1138\n",
      "2025-12-03 20:00:47,788 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:00:47,788 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 20:00:47,788 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:00:47,788 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 20:00:47,795 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4611.pt\n",
      "2025-12-03 20:00:48,458 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 4, Epoch: 0)\n",
      "2025-12-03 20:00:48,458 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.76it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 114.07it/s]\n",
      "2025-12-03 20:01:02,527 - INFO - Train Loss: 0.1288\n",
      "2025-12-03 20:01:02,527 - INFO - Val Loss: 0.1182\n",
      "2025-12-03 20:01:02,528 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:01:02,528 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 20:01:02,528 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:01:02,528 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 20:01:02,528 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:12<00:00, 36.80it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.21it/s]\n",
      "2025-12-03 20:01:16,587 - INFO - Train Loss: 0.1292\n",
      "2025-12-03 20:01:16,587 - INFO - Val Loss: 0.1140\n",
      "2025-12-03 20:01:16,587 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:01:16,587 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 20:01:16,587 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:01:16,588 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 20:01:16,588 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:12<00:00, 36.80it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.72it/s]\n",
      "2025-12-03 20:01:30,643 - INFO - Train Loss: 0.1246\n",
      "2025-12-03 20:01:30,643 - INFO - Val Loss: 0.1087\n",
      "2025-12-03 20:01:30,644 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-03 20:01:30,644 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-03 20:01:30,644 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-03 20:01:30,644 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-03 20:01:30,644 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-03 20:01:30,652 - INFO - \n",
      "Results saved to: ../results/roberta-fine-tune/original_and_augumented_mlm/training_results.csv\n",
      "2025-12-03 20:01:30,652 - INFO - \n",
      "================================================================================\n",
      "2025-12-03 20:01:30,652 - INFO - Best Models Saved:\n",
      "2025-12-03 20:01:30,653 - INFO - ================================================================================\n",
      "2025-12-03 20:01:30,653 - INFO - Fold 0, Epoch 0: F1=0.4614 -> ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt\n",
      "2025-12-03 20:01:30,653 - INFO - Fold 1, Epoch 0: F1=0.4611 -> ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt\n"
     ]
    }
   ],
   "source": [
    "main(train_data, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141d468-18bd-4a32-878e-ea24c52d6ee7",
   "metadata": {},
   "source": [
    "### Visualizing Fine-Tune Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f968616-5ab7-4911-a664-6746ac34d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/roberta-fine-tune/original_and_augumented_mlm/training_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669256a3-d5ef-427e-97fa-5649a8d91ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_fold_augmented.svg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_plot = alt.Chart(df).mark_line(point=True, size=3).encode(\n",
    "    x=alt.X('epoch:Q', title='Epoch'),\n",
    "    y=alt.Y('overall_macro_f1:Q', title='Macro F1 Score', scale=alt.Scale(domain=[0.3, 0.8])),\n",
    "    color=alt.Color('fold:N', title='Fold'),\n",
    "    tooltip=['fold:N', 'epoch:Q', alt.Tooltip('overall_macro_f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Overall F1 Score by Fold')\n",
    "f1_plot.save(os.path.join(figures_root, 'f1_vs_fold_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_fold_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aeffa-3c2b-4e12-9d73-176d7f020dc9",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_fold_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc1abcc-a363-4160-b763-684a111b67cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/loss_augmented.svg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_data = df[['fold', 'epoch', 'train_loss', 'val_loss']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='loss_type', value_name='loss'\n",
    ")\n",
    "\n",
    "loss_plot = alt.Chart(loss_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y='loss:Q', color='loss_type:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'loss_type:N', alt.Tooltip('loss:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Training & Validation Loss')\n",
    "\n",
    "loss_plot.save(os.path.join(figures_root, 'loss_augmented.svg'))\n",
    "os.path.join(figures_root, 'loss_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149bf7-3ed2-45af-8954-1af12c737f15",
   "metadata": {},
   "source": [
    "![](../figures/loss_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db4cc8b-762d-46a5-ae47-122d777394a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_lang_augmented.svg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_data = df[['fold', 'epoch', 'en_macro_f1', 'es_macro_f1', 'it_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='language', value_name='f1'\n",
    ")\n",
    "lang_data['language'] = lang_data['language'].str.replace('_macro_f1', '').str.upper()\n",
    "\n",
    "lang_plot = alt.Chart(lang_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y=alt.Y('f1:Q', scale=alt.Scale(domain=[0.4, 0.95])), color='language:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'language:N', alt.Tooltip('f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='F1 by Language')\n",
    "\n",
    "lang_plot.save(os.path.join(figures_root, 'f1_vs_lang_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_lang_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e06673-7cd3-44fb-b8e4-af3a8e495fbd",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_lang_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b491471f-fb74-4ab7-bf4c-d9fbc0663cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/fold_vs_epoch_augmented.svg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = df[['fold', 'epoch', 'overall_macro_precision', 'overall_macro_recall', 'overall_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='metric', value_name='value'\n",
    ")\n",
    "metrics_data['metric'] = metrics_data['metric'].str.replace('overall_macro_', '').str.capitalize()\n",
    "metrics_data['fold_epoch'] = 'F' + metrics_data['fold'].astype(str) + ':E' + metrics_data['epoch'].astype(str)\n",
    "\n",
    "heatmap = alt.Chart(metrics_data).mark_rect().encode(\n",
    "    x='fold_epoch:O', y='metric:N',\n",
    "    color=alt.Color('value:Q', scale=alt.Scale(scheme='viridis')),\n",
    "    tooltip=['fold_epoch:N', 'metric:N', alt.Tooltip('value:Q', format='.4f')]\n",
    ").properties(width=700, height=150, title='Precision/Recall/F1 Heatmap')\n",
    "\n",
    "heatmap.save(os.path.join(figures_root, 'fold_vs_epoch_augmented.svg'))\n",
    "os.path.join(figures_root, 'fold_vs_epoch_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f99d57-d61d-4de7-974a-32f713900f1d",
   "metadata": {},
   "source": [
    "![](../figures/fold_vs_epoch_augmented.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afbe71-d966-4561-b119-9e1bed6f4e4b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef7c0fd-3153-43a2-8393-b9bfecb5f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your training data\n",
    "from src.baseline.baseline import train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8848534-c659-40e8-9ab8-9724f7a56c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    CHECKPOINT_PATH = \"../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt\"\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_LABELS = 2\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7681f-96f1-44e4-9598-b2224ad1a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"Dataset for inference\"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51abcda-364e-45b2-9cf6-c20c6fbfbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds: List[int], labels: List[int], languages: List[str]) -> Dict:\n",
    "    metrics = {}\n",
    "\n",
    "    macro_precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    metrics[\"overall\"] = {\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    unique_langs = set(languages)\n",
    "    for lang in unique_langs:\n",
    "        lang_mask = np.array([l == lang for l in languages])\n",
    "        lang_preds = np.array(preds)[lang_mask]\n",
    "        lang_labels = np.array(labels)[lang_mask]\n",
    "\n",
    "        if len(lang_labels) > 0:\n",
    "            lang_precision = precision_score(\n",
    "                lang_labels, lang_preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            lang_recall = recall_score(\n",
    "                lang_labels, lang_preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            lang_f1 = f1_score(lang_labels, lang_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "            metrics[lang] = {\n",
    "                \"macro_precision\": lang_precision,\n",
    "                \"macro_recall\": lang_recall,\n",
    "                \"macro_f1\": lang_f1,\n",
    "            }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f92fb3-5360-44ce-9dd6-f83a10d570d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(df: pd.DataFrame, config: InferenceConfig) -> Dict:\n",
    "    print(f\"Running inference on {len(df)} samples...\")\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        num_labels=config.NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(config.CHECKPOINT_PATH, map_location=config.DEVICE)\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"model_state_dict\"]\n",
    "        elif \"state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Loaded checkpoint from: {config.CHECKPOINT_PATH}\")\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        for key in [\"fold\", \"epoch\", \"f1_score\"]:\n",
    "            if key in checkpoint:\n",
    "                print(f\"  {key.capitalize()}: {checkpoint[key]}\")\n",
    "    \n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    languages = df[\"lang\"].tolist()\n",
    "    \n",
    "    dataset = TweetDataset(texts, tokenizer, config.MAX_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(config.DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "    \n",
    "    metrics = calculate_metrics(all_preds, labels, languages)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INFERENCE RESULTS ON TRAIN DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Macro Precision: {metrics['overall']['macro_precision']:.4f}\")\n",
    "    print(f\"  Macro Recall:    {metrics['overall']['macro_recall']:.4f}\")\n",
    "    print(f\"  Macro F1:        {metrics['overall']['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Language Metrics:\")\n",
    "    for lang in sorted([k for k in metrics.keys() if k != \"overall\"]):\n",
    "        print(f\"  {lang.upper()}:\")\n",
    "        print(f\"    Precision: {metrics[lang]['macro_precision']:.4f}\")\n",
    "        print(f\"    Recall:    {metrics[lang]['macro_recall']:.4f}\")\n",
    "        print(f\"    F1:        {metrics[lang]['macro_f1']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": all_preds,\n",
    "        \"probabilities\": all_probs,\n",
    "        \"labels\": labels,\n",
    "        \"languages\": languages,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739db1b9-9320-4ca4-a271-c7f17c940ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_inference(train_df, InferenceConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189552dd-bd35-4b07-aded-4e5c141483c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
