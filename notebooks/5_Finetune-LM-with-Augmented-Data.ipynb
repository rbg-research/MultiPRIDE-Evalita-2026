{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d25684-e586-487d-98b1-238b28f88468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 12:37:26.387266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 12:37:26.414447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 12:37:27.011735: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n",
      "training files: ['train_en.csv', 'train_it.csv', 'train_es.csv']\n",
      "Total training samples: 2988\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 2560 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 428 (14.3%)\n",
      "  Total: 2988\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=938, Class 1=88, Total=1026\n",
      "  ES: Class 0=743, Class 1=133, Total=876\n",
      "  IT: Class 0=879, Class 1=207, Total=1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from src.baseline.baseline import train_df, figures_root\n",
    "from src.finetune.finetuner import run_train, run_inference\n",
    "from src.baseline.utils import calculate_class_distribution, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00741d54-c2c7-4631-8b57-db05296b47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbe48fa-4751-4a12-92bd-8337b71f51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5) (5976, 8)\n"
     ]
    }
   ],
   "source": [
    "original_data = train_df\n",
    "augmented_data = pd.read_csv(\"../data/augmented_multilingual_tweets.csv\")\n",
    "print(original_data.shape, augmented_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27231170-0d78-446d-aa0e-23dea9c8d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8964, 5)\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.concat([original_data, augmented_data[list(original_data.columns)]], ignore_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c6197b-8700-4659-9e27-e6ada9529d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd97a2a-12b3-4a25-a8c0-045f562b7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 5120 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 856 (14.3%)\n",
      "  Total: 5976\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=1622, Class 1=340, Total=1962\n",
      "  ES: Class 0=1817, Class 1=295, Total=2112\n",
      "  IT: Class 0=1681, Class 1=221, Total=1902\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_class_distribution(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a2e02-03da-4580-8e39-1c90af03de75",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f0123e-cffa-4119-be97-d6476b7a91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training would be start on the device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for fine-tuning\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model\" #\"cardiffnlp/twitter-xlm-roberta-base\"  # Base model\n",
    "    NUM_LABELS = 2  # Binary classification\n",
    "    MAX_LENGTH = 128  # Maximum sequence length\n",
    "    NUM_FROZEN_LAYERS = 3  # Number of initial layers to freeze (0 = only train classification head)\n",
    "\n",
    "    # Training configuration\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    WARMUP_RATIO = 0.1  # Warmup as % of total steps\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"  # Evaluate at end of each epoch\n",
    "\n",
    "    # Cross-validation\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8  # 80% for training from each fold\n",
    "    VAL_RATIO = 0.2  # 20% for validation from each fold\n",
    "\n",
    "    # Dynamic undersampling\n",
    "    DYNAMIC_UNDERSAMPLE = False  # Balance classes per epoch\n",
    "\n",
    "    # Model saving\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models\"\n",
    "    RESULTS_DIR = \"../results/roberta-fine-tune/augumented/\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training would be start on the device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4bd487-c125-4c65-a2af-4e1c3cf124a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 12:37:27,861 - INFO - Fold 0: Train=3824, Val=956\n",
      "2025-12-04 12:37:27,861 - INFO -   Train label dist: {0: 3276, 1: 548}\n",
      "2025-12-04 12:37:27,862 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 12:37:27,866 - INFO - Fold 1: Train=3824, Val=957\n",
      "2025-12-04 12:37:27,866 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 12:37:27,867 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 12:37:27,870 - INFO - Fold 2: Train=3824, Val=957\n",
      "2025-12-04 12:37:27,871 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 12:37:27,871 - INFO -   Train lang dist: {'es': 1352, 'en': 1256, 'it': 1216}\n",
      "2025-12-04 12:37:27,875 - INFO - Fold 3: Train=3824, Val=957\n",
      "2025-12-04 12:37:27,875 - INFO -   Train label dist: {0: 3278, 1: 546}\n",
      "2025-12-04 12:37:27,876 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 12:37:27,880 - INFO - Fold 4: Train=3824, Val=957\n",
      "2025-12-04 12:37:27,880 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 12:37:27,881 - INFO -   Train lang dist: {'es': 1351, 'en': 1256, 'it': 1217}\n",
      "2025-12-04 12:37:27,881 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:37:27,881 - INFO - Fold 1/5\n",
      "2025-12-04 12:37:27,881 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0:\n",
      "  Train: 548 positive samples\n",
      "  Val:   137 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 12:37:28,690 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-04 12:37:28,691 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 12:37:28,691 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-04 12:37:28,692 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-12-04 12:37:28,692 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 12:37:28,692 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-12-04 12:37:28,693 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.12it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.18it/s]\n",
      "2025-12-04 12:37:43,404 - INFO - Train Loss: 0.1319\n",
      "2025-12-04 12:37:43,405 - INFO - Val Loss: 0.1138\n",
      "2025-12-04 12:37:43,405 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-04 12:37:43,405 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:37:43,405 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-04 12:37:43,405 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:37:44,064 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt (F1: 0.4614, Fold: 0, Epoch: 0)\n",
      "2025-12-04 12:37:44,064 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.22it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.58it/s]\n",
      "2025-12-04 12:37:58,360 - INFO - Train Loss: 0.1324\n",
      "2025-12-04 12:37:58,361 - INFO - Val Loss: 0.1198\n",
      "2025-12-04 12:37:58,361 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-04 12:37:58,361 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:37:58,361 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-04 12:37:58,361 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:37:58,361 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.45it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 106.59it/s]\n",
      "2025-12-04 12:38:12,613 - INFO - Train Loss: 0.1246\n",
      "2025-12-04 12:38:12,613 - INFO - Val Loss: 0.1144\n",
      "2025-12-04 12:38:12,613 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-04 12:38:12,614 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:38:12,614 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-04 12:38:12,614 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:38:12,614 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 35.57it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.55it/s]\n",
      "2025-12-04 12:38:27,148 - INFO - Train Loss: 0.1213\n",
      "2025-12-04 12:38:27,148 - INFO - Val Loss: 0.1058\n",
      "2025-12-04 12:38:27,148 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-04 12:38:27,148 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:38:27,148 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-04 12:38:27,148 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:38:27,148 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-04 12:38:27,149 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:38:27,149 - INFO - Fold 2/5\n",
      "2025-12-04 12:38:27,149 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 12:38:27,829 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-04 12:38:27,829 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 12:38:27,830 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-04 12:38:27,832 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 12:38:27,832 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 12:38:27,832 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 12:38:27,834 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.19it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.89it/s]\n",
      "2025-12-04 12:38:42,144 - INFO - Train Loss: 0.1321\n",
      "2025-12-04 12:38:42,144 - INFO - Val Loss: 0.1139\n",
      "2025-12-04 12:38:42,144 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:38:42,145 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:38:42,145 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:38:42,145 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:38:43,094 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 1, Epoch: 0)\n",
      "2025-12-04 12:38:43,094 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.09it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.62it/s]\n",
      "2025-12-04 12:38:57,407 - INFO - Train Loss: 0.1310\n",
      "2025-12-04 12:38:57,407 - INFO - Val Loss: 0.1171\n",
      "2025-12-04 12:38:57,407 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:38:57,407 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:38:57,408 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:38:57,408 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:38:57,408 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.03it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.20it/s]\n",
      "2025-12-04 12:39:11,767 - INFO - Train Loss: 0.1248\n",
      "2025-12-04 12:39:11,767 - INFO - Val Loss: 0.1152\n",
      "2025-12-04 12:39:11,767 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:39:11,767 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:39:11,767 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:39:11,767 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:39:11,768 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.41it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.73it/s]\n",
      "2025-12-04 12:39:25,982 - INFO - Train Loss: 0.1198\n",
      "2025-12-04 12:39:25,982 - INFO - Val Loss: 0.1026\n",
      "2025-12-04 12:39:25,982 - INFO - Overall - Precision: 0.6783, Recall: 0.5030, F1: 0.4682\n",
      "2025-12-04 12:39:25,983 - INFO - en - Precision: 0.9153, Recall: 0.5093, F1: 0.4719\n",
      "2025-12-04 12:39:25,983 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:39:25,983 - INFO - it - Precision: 0.4408, Recall: 0.4981, F1: 0.4677\n",
      "2025-12-04 12:39:26,052 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt\n",
      "2025-12-04 12:39:26,728 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_3_f1_0.4682.pt (F1: 0.4682, Fold: 1, Epoch: 3)\n",
      "2025-12-04 12:39:26,729 - INFO - \n",
      "Epoch 5/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.36it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.99it/s]\n",
      "2025-12-04 12:39:40,957 - INFO - Train Loss: 0.1110\n",
      "2025-12-04 12:39:40,957 - INFO - Val Loss: 0.1055\n",
      "2025-12-04 12:39:40,957 - INFO - Overall - Precision: 0.7621, Recall: 0.5066, F1: 0.4755\n",
      "2025-12-04 12:39:40,957 - INFO - en - Precision: 0.9167, Recall: 0.5185, F1: 0.4903\n",
      "2025-12-04 12:39:40,958 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:39:40,958 - INFO - it - Precision: 0.4408, Recall: 0.4981, F1: 0.4677\n",
      "2025-12-04 12:39:40,990 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4614.pt\n",
      "2025-12-04 12:39:41,730 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_4_f1_0.4755.pt (F1: 0.4755, Fold: 1, Epoch: 4)\n",
      "2025-12-04 12:39:41,730 - INFO - \n",
      "Epoch 6/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.53it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.67it/s]\n",
      "2025-12-04 12:39:55,890 - INFO - Train Loss: 0.1028\n",
      "2025-12-04 12:39:55,891 - INFO - Val Loss: 0.1069\n",
      "2025-12-04 12:39:55,891 - INFO - Overall - Precision: 0.7401, Recall: 0.5742, F1: 0.5933\n",
      "2025-12-04 12:39:55,891 - INFO - en - Precision: 0.7760, Recall: 0.6420, F1: 0.6742\n",
      "2025-12-04 12:39:55,891 - INFO - es - Precision: 0.7671, Recall: 0.5382, F1: 0.5371\n",
      "2025-12-04 12:39:55,891 - INFO - it - Precision: 0.5858, Recall: 0.5185, F1: 0.5121\n",
      "2025-12-04 12:39:55,938 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_3_f1_0.4682.pt\n",
      "2025-12-04 12:39:56,625 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_5_f1_0.5933.pt (F1: 0.5933, Fold: 1, Epoch: 5)\n",
      "2025-12-04 12:39:56,626 - INFO - \n",
      "Epoch 7/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.42it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.94it/s]\n",
      "2025-12-04 12:40:10,823 - INFO - Train Loss: 0.1052\n",
      "2025-12-04 12:40:10,823 - INFO - Val Loss: 0.1090\n",
      "2025-12-04 12:40:10,823 - INFO - Overall - Precision: 0.7970, Recall: 0.6146, F1: 0.6487\n",
      "2025-12-04 12:40:10,824 - INFO - en - Precision: 0.8173, Recall: 0.6810, F1: 0.7195\n",
      "2025-12-04 12:40:10,824 - INFO - es - Precision: 0.8495, Recall: 0.5903, F1: 0.6193\n",
      "2025-12-04 12:40:10,824 - INFO - it - Precision: 0.6682, Recall: 0.5463, F1: 0.5561\n",
      "2025-12-04 12:40:10,883 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_4_f1_0.4755.pt\n",
      "2025-12-04 12:40:11,635 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_6_f1_0.6487.pt (F1: 0.6487, Fold: 1, Epoch: 6)\n",
      "2025-12-04 12:40:11,635 - INFO - \n",
      "Epoch 8/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.36it/s, loss=0.0953]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.92it/s]\n",
      "2025-12-04 12:40:25,876 - INFO - Train Loss: 0.0951\n",
      "2025-12-04 12:40:25,877 - INFO - Val Loss: 0.0964\n",
      "2025-12-04 12:40:25,877 - INFO - Overall - Precision: 0.7585, Recall: 0.6749, F1: 0.7040\n",
      "2025-12-04 12:40:25,877 - INFO - en - Precision: 0.7882, Recall: 0.7250, F1: 0.7498\n",
      "2025-12-04 12:40:25,877 - INFO - es - Precision: 0.7873, Recall: 0.7189, F1: 0.7459\n",
      "2025-12-04 12:40:25,877 - INFO - it - Precision: 0.5991, Recall: 0.5388, F1: 0.5451\n",
      "2025-12-04 12:40:25,940 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_5_f1_0.5933.pt\n",
      "2025-12-04 12:40:26,613 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_7_f1_0.7040.pt (F1: 0.7040, Fold: 1, Epoch: 7)\n",
      "2025-12-04 12:40:26,614 - INFO - \n",
      "Epoch 9/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.32it/s, loss=0.0898]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.11it/s]\n",
      "2025-12-04 12:40:40,858 - INFO - Train Loss: 0.0897\n",
      "2025-12-04 12:40:40,858 - INFO - Val Loss: 0.1205\n",
      "2025-12-04 12:40:40,858 - INFO - Overall - Precision: 0.8122, Recall: 0.6394, F1: 0.6788\n",
      "2025-12-04 12:40:40,858 - INFO - en - Precision: 0.8103, Recall: 0.6883, F1: 0.7251\n",
      "2025-12-04 12:40:40,859 - INFO - es - Precision: 0.8944, Recall: 0.6632, F1: 0.7146\n",
      "2025-12-04 12:40:40,859 - INFO - it - Precision: 0.6319, Recall: 0.5324, F1: 0.5346\n",
      "2025-12-04 12:40:40,859 - INFO - \n",
      "Epoch 10/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.42it/s, loss=0.0849]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.66it/s]\n",
      "2025-12-04 12:40:55,070 - INFO - Train Loss: 0.0847\n",
      "2025-12-04 12:40:55,071 - INFO - Val Loss: 0.1248\n",
      "2025-12-04 12:40:55,071 - INFO - Overall - Precision: 0.8101, Recall: 0.6726, F1: 0.7126\n",
      "2025-12-04 12:40:55,071 - INFO - en - Precision: 0.8324, Recall: 0.7254, F1: 0.7621\n",
      "2025-12-04 12:40:55,071 - INFO - es - Precision: 0.8553, Recall: 0.6997, F1: 0.7466\n",
      "2025-12-04 12:40:55,072 - INFO - it - Precision: 0.6554, Recall: 0.5564, F1: 0.5704\n",
      "2025-12-04 12:40:55,133 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_6_f1_0.6487.pt\n",
      "2025-12-04 12:40:55,808 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_9_f1_0.7126.pt (F1: 0.7126, Fold: 1, Epoch: 9)\n",
      "2025-12-04 12:40:55,809 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:40:55,809 - INFO - Fold 3/5\n",
      "2025-12-04 12:40:55,809 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 2:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 12:40:56,503 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-04 12:40:56,504 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 12:40:56,504 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-04 12:40:56,505 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 12:40:56,506 - INFO - Language weights: {'es': 0.9409476243674836, 'en': 1.0128671880134061, 'it': 1.0461851876191102}\n",
      "2025-12-04 12:40:56,506 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 12:40:56,507 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.53it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.17it/s]\n",
      "2025-12-04 12:41:10,683 - INFO - Train Loss: 0.1280\n",
      "2025-12-04 12:41:10,684 - INFO - Val Loss: 0.1139\n",
      "2025-12-04 12:41:10,684 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:41:10,684 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:41:10,684 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:41:10,684 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:41:10,691 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt\n",
      "2025-12-04 12:41:11,373 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 2, Epoch: 0)\n",
      "2025-12-04 12:41:11,373 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.39it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.15it/s]\n",
      "2025-12-04 12:41:25,582 - INFO - Train Loss: 0.1317\n",
      "2025-12-04 12:41:25,582 - INFO - Val Loss: 0.1201\n",
      "2025-12-04 12:41:25,582 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:41:25,582 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:41:25,583 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:41:25,583 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:41:25,583 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.56it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.53it/s]\n",
      "2025-12-04 12:41:39,745 - INFO - Train Loss: 0.1250\n",
      "2025-12-04 12:41:39,745 - INFO - Val Loss: 0.1155\n",
      "2025-12-04 12:41:39,746 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:41:39,746 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:41:39,746 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:41:39,746 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:41:39,746 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.44it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.19it/s]\n",
      "2025-12-04 12:41:53,943 - INFO - Train Loss: 0.1202\n",
      "2025-12-04 12:41:53,944 - INFO - Val Loss: 0.1109\n",
      "2025-12-04 12:41:53,944 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:41:53,944 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:41:53,944 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:41:53,944 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:41:53,944 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-04 12:41:53,944 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:41:53,945 - INFO - Fold 4/5\n",
      "2025-12-04 12:41:53,945 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 3:\n",
      "  Train: 546 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 12:41:54,647 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-04 12:41:54,647 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 12:41:54,648 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-04 12:41:54,649 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-12-04 12:41:54,649 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 12:41:54,649 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-12-04 12:41:54,651 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.54it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.87it/s]\n",
      "2025-12-04 12:42:08,827 - INFO - Train Loss: 0.1332\n",
      "2025-12-04 12:42:08,827 - INFO - Val Loss: 0.1138\n",
      "2025-12-04 12:42:08,827 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:42:08,827 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-04 12:42:08,827 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:42:08,827 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-04 12:42:08,834 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4611.pt\n",
      "2025-12-04 12:42:09,527 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 3, Epoch: 0)\n",
      "2025-12-04 12:42:09,528 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.48it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 110.56it/s]\n",
      "2025-12-04 12:42:23,727 - INFO - Train Loss: 0.1297\n",
      "2025-12-04 12:42:23,727 - INFO - Val Loss: 0.1166\n",
      "2025-12-04 12:42:23,728 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:42:23,728 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-04 12:42:23,728 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:42:23,728 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-04 12:42:23,728 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.54it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 111.39it/s]\n",
      "2025-12-04 12:42:37,899 - INFO - Train Loss: 0.1290\n",
      "2025-12-04 12:42:37,899 - INFO - Val Loss: 0.1142\n",
      "2025-12-04 12:42:37,899 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:42:37,899 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-04 12:42:37,900 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:42:37,900 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-04 12:42:37,900 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.50it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.58it/s]\n",
      "2025-12-04 12:42:52,072 - INFO - Train Loss: 0.1226\n",
      "2025-12-04 12:42:52,072 - INFO - Val Loss: 0.1061\n",
      "2025-12-04 12:42:52,072 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:42:52,072 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-04 12:42:52,072 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:42:52,072 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-04 12:42:52,073 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-04 12:42:52,073 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:42:52,073 - INFO - Fold 5/5\n",
      "2025-12-04 12:42:52,073 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 4:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 12:42:52,745 - INFO - Froze: Embeddings + First 3 Encoder Layers\n",
      "2025-12-04 12:42:52,746 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 12:42:52,746 - INFO - Trainable parameters: 64,382,978 / 278,045,186 (23.16%)\n",
      "2025-12-04 12:42:52,747 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 12:42:52,748 - INFO - Language weights: {'es': 0.9416953224870754, 'en': 1.0129222776114961, 'it': 1.045382399901429}\n",
      "2025-12-04 12:42:52,748 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 12:42:52,749 - INFO - \n",
      "Epoch 1/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.69it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.66it/s]\n",
      "2025-12-04 12:43:06,863 - INFO - Train Loss: 0.1307\n",
      "2025-12-04 12:43:06,863 - INFO - Val Loss: 0.1138\n",
      "2025-12-04 12:43:06,864 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:43:06,864 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:43:06,864 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:43:06,864 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:43:06,871 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4611.pt\n",
      "2025-12-04 12:43:07,570 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 4, Epoch: 0)\n",
      "2025-12-04 12:43:07,570 - INFO - \n",
      "Epoch 2/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.55it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.98it/s]\n",
      "2025-12-04 12:43:21,713 - INFO - Train Loss: 0.1286\n",
      "2025-12-04 12:43:21,714 - INFO - Val Loss: 0.1187\n",
      "2025-12-04 12:43:21,714 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:43:21,714 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:43:21,714 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:43:21,715 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:43:21,715 - INFO - \n",
      "Epoch 3/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.58it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 113.67it/s]\n",
      "2025-12-04 12:43:35,850 - INFO - Train Loss: 0.1276\n",
      "2025-12-04 12:43:35,850 - INFO - Val Loss: 0.1103\n",
      "2025-12-04 12:43:35,850 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:43:35,850 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:43:35,851 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:43:35,851 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:43:35,851 - INFO - \n",
      "Epoch 4/10\n",
      "Training: 100%|██████████| 478/478 [00:13<00:00, 36.51it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 120/120 [00:01<00:00, 112.17it/s]\n",
      "2025-12-04 12:43:50,024 - INFO - Train Loss: 0.1210\n",
      "2025-12-04 12:43:50,024 - INFO - Val Loss: 0.1027\n",
      "2025-12-04 12:43:50,024 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 12:43:50,024 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 12:43:50,024 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 12:43:50,025 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 12:43:50,025 - INFO - Early stopping triggered at epoch 3 (patience: 3)\n",
      "2025-12-04 12:43:50,033 - INFO - \n",
      "Results saved to: ../results/roberta-fine-tune/augumented/training_results.csv\n",
      "2025-12-04 12:43:50,033 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 12:43:50,033 - INFO - Best Models Saved:\n",
      "2025-12-04 12:43:50,034 - INFO - ================================================================================\n",
      "2025-12-04 12:43:50,034 - INFO - Fold 1, Epoch 9: F1=0.7126 -> ../fine_tuned_models/checkpoints/fold_1_epoch_9_f1_0.7126.pt\n",
      "2025-12-04 12:43:50,034 - INFO - Fold 1, Epoch 7: F1=0.7040 -> ../fine_tuned_models/checkpoints/fold_1_epoch_7_f1_0.7040.pt\n"
     ]
    }
   ],
   "source": [
    "best_checkpoint_path = run_train(train_data, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141d468-18bd-4a32-878e-ea24c52d6ee7",
   "metadata": {},
   "source": [
    "### Visualizing Fine-Tune Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f968616-5ab7-4911-a664-6746ac34d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(Config.RESULTS_DIR, \"training_results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669256a3-d5ef-427e-97fa-5649a8d91ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_fold_augmented.svg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_plot = alt.Chart(df).mark_line(point=True, size=3).encode(\n",
    "    x=alt.X('epoch:Q', title='Epoch'),\n",
    "    y=alt.Y('overall_macro_f1:Q', title='Macro F1 Score', scale=alt.Scale(domain=[0.3, 0.8])),\n",
    "    color=alt.Color('fold:N', title='Fold'),\n",
    "    tooltip=['fold:N', 'epoch:Q', alt.Tooltip('overall_macro_f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Overall F1 Score by Fold')\n",
    "f1_plot.save(os.path.join(figures_root, 'f1_vs_fold_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_fold_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aeffa-3c2b-4e12-9d73-176d7f020dc9",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_fold_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc1abcc-a363-4160-b763-684a111b67cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/loss_augmented.svg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_data = df[['fold', 'epoch', 'train_loss', 'val_loss']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='loss_type', value_name='loss'\n",
    ")\n",
    "\n",
    "loss_plot = alt.Chart(loss_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y='loss:Q', color='loss_type:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'loss_type:N', alt.Tooltip('loss:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Training & Validation Loss')\n",
    "\n",
    "loss_plot.save(os.path.join(figures_root, 'loss_augmented.svg'))\n",
    "os.path.join(figures_root, 'loss_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149bf7-3ed2-45af-8954-1af12c737f15",
   "metadata": {},
   "source": [
    "![](../figures/loss_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db4cc8b-762d-46a5-ae47-122d777394a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/f1_vs_lang_augmented.svg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_data = df[['fold', 'epoch', 'en_macro_f1', 'es_macro_f1', 'it_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='language', value_name='f1'\n",
    ")\n",
    "lang_data['language'] = lang_data['language'].str.replace('_macro_f1', '').str.upper()\n",
    "\n",
    "lang_plot = alt.Chart(lang_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y=alt.Y('f1:Q', scale=alt.Scale(domain=[0.4, 0.95])), color='language:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'language:N', alt.Tooltip('f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='F1 by Language')\n",
    "\n",
    "lang_plot.save(os.path.join(figures_root, 'f1_vs_lang_augmented.svg'))\n",
    "os.path.join(figures_root, 'f1_vs_lang_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e06673-7cd3-44fb-b8e4-af3a8e495fbd",
   "metadata": {},
   "source": [
    "![](../figures/f1_vs_lang_augmented.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b491471f-fb74-4ab7-bf4c-d9fbc0663cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../figures/fold_vs_epoch_augmented.svg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = df[['fold', 'epoch', 'overall_macro_precision', 'overall_macro_recall', 'overall_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='metric', value_name='value'\n",
    ")\n",
    "metrics_data['metric'] = metrics_data['metric'].str.replace('overall_macro_', '').str.capitalize()\n",
    "metrics_data['fold_epoch'] = 'F' + metrics_data['fold'].astype(str) + ':E' + metrics_data['epoch'].astype(str)\n",
    "\n",
    "heatmap = alt.Chart(metrics_data).mark_rect().encode(\n",
    "    x='fold_epoch:O', y='metric:N',\n",
    "    color=alt.Color('value:Q', scale=alt.Scale(scheme='viridis')),\n",
    "    tooltip=['fold_epoch:N', 'metric:N', alt.Tooltip('value:Q', format='.4f')]\n",
    ").properties(width=700, height=150, title='Precision/Recall/F1 Heatmap')\n",
    "\n",
    "heatmap.save(os.path.join(figures_root, 'fold_vs_epoch_augmented.svg'))\n",
    "os.path.join(figures_root, 'fold_vs_epoch_augmented.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f99d57-d61d-4de7-974a-32f713900f1d",
   "metadata": {},
   "source": [
    "![](../figures/fold_vs_epoch_augmented.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afbe71-d966-4561-b119-9e1bed6f4e4b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8848534-c659-40e8-9ab8-9724f7a56c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    CHECKPOINT_PATH = best_checkpoint_path\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_LABELS = 2\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739db1b9-9320-4ca4-a271-c7f17c940ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 2988 samples...\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: ../fine_tuned_models/checkpoints/fold_1_epoch_7_f1_0.7040.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 94/94 [00:02<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE RESULTS ON TRAIN DATA\n",
      "================================================================================\n",
      "\n",
      "Overall Metrics:\n",
      "  Macro Precision: 0.6085\n",
      "  Macro Recall:    0.6135\n",
      "  Macro F1:        0.6109\n",
      "\n",
      "Per-Language Metrics:\n",
      "  EN:\n",
      "    Precision: 0.5305\n",
      "    Recall:    0.5780\n",
      "    F1:        0.5102\n",
      "  ES:\n",
      "    Precision: 0.7147\n",
      "    Recall:    0.6831\n",
      "    F1:        0.6966\n",
      "  IT:\n",
      "    Precision: 0.8901\n",
      "    Recall:    0.6257\n",
      "    F1:        0.6594\n"
     ]
    }
   ],
   "source": [
    "results = run_inference(train_df, InferenceConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189552dd-bd35-4b07-aded-4e5c141483c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
