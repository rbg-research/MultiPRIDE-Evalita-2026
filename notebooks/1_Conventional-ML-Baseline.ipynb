{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8e57ea-44b6-46e1-b3b3-82fb4b76d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from src.utils import set_all_seeds\n",
    "\n",
    "set_all_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66735838-2edf-4a99-9618-946f85d7e6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_en.csv', 'train_es.csv', 'train_it.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = \"../data/multipride_data/\"\n",
    "figures_root = \"../figures/\"\n",
    "os.makedirs(figures_root, exist_ok=True)\n",
    "\n",
    "train_files = [file for file in os.listdir(data_root) if (file.endswith(\".csv\") and (\"train\" in file))]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f6e78f-c62a-4725-b30f-3a86bd39d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 2988\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "for file in train_files:\n",
    "    temp_df = pd.read_csv(os.path.join(data_root, file))\n",
    "    if \"en\" in file:\n",
    "        temp_df[\"bio\"] = [None] * temp_df.shape[0]\n",
    "    train_df = pd.concat([train_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(f\"Total training samples: {train_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a480f9-8646-49ac-83e9-a3cc92399c0a",
   "metadata": {},
   "source": [
    "# Language Specific - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516667d8-2fcb-4d20-98c0-cfaf90f49982",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_dict = {\n",
    "    'en': stopwords.words('english'),\n",
    "    'es': stopwords.words('spanish'),\n",
    "    'it': stopwords.words('italian'),\n",
    "}\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0)\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"RandomForest (u)\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"LinearSVC (u)\": LinearSVC(class_weight=None, max_iter=1000),\n",
    "    \"LinearSVC (b)\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"LogisticRegression (u)\": LogisticRegression(max_iter=500),\n",
    "    \"LogisticRegression (b)\": LogisticRegression(class_weight='balanced', max_iter=500),\n",
    "    \"RandomForest (b)\": RandomForestClassifier(class_weight='balanced', n_estimators=200),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2998f271-1e3b-4bd5-ac14-1fbd1fdc9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best for EN: LogisticRegression (b) (Acc=0.913, Prec=0.280, Rec=0.033, F1=0.056)\n",
      "\n",
      " Best for ES: LogisticRegression (b) (Acc=0.860, Prec=0.570, Rec=0.308, F1=0.399)\n",
      "\n",
      " Best for IT: RandomForest (b) (Acc=0.928, Prec=0.906, Rec=0.701, F1=0.787)\n",
      "\n",
      "All Results Summary:\n",
      "    Language                   Model  Accuracy  Precision  Recall     F1\n",
      "4        en  LogisticRegression (b)     0.913      0.280   0.033  0.056\n",
      "2        en           LinearSVC (b)     0.915      0.133   0.022  0.038\n",
      "0        en        RandomForest (u)     0.914      0.000   0.000  0.000\n",
      "1        en           LinearSVC (u)     0.914      0.000   0.000  0.000\n",
      "3        en  LogisticRegression (u)     0.914      0.000   0.000  0.000\n",
      "5        en        RandomForest (b)     0.914      0.000   0.000  0.000\n",
      "10       es  LogisticRegression (b)     0.860      0.570   0.308  0.399\n",
      "8        es           LinearSVC (b)     0.871      0.816   0.210  0.327\n",
      "7        es           LinearSVC (u)     0.860      0.860   0.098  0.171\n",
      "11       es        RandomForest (b)     0.853      0.750   0.053  0.097\n",
      "6        es        RandomForest (u)     0.853      0.700   0.038  0.071\n",
      "9        es  LogisticRegression (u)     0.848      0.000   0.000  0.000\n",
      "17       it        RandomForest (b)     0.928      0.906   0.701  0.787\n",
      "16       it  LogisticRegression (b)     0.907      0.911   0.575  0.701\n",
      "12       it        RandomForest (u)     0.911      0.990   0.536  0.695\n",
      "14       it           LinearSVC (b)     0.902      0.980   0.498  0.659\n",
      "13       it           LinearSVC (u)     0.866      1.000   0.300  0.458\n",
      "15       it  LogisticRegression (u)     0.816      0.400   0.034  0.063\n",
      "\n",
      "Combined Average Performance Across Languages:\n",
      " Accuracy:  0.887\n",
      " Precision: 0.517\n",
      " Recall:    0.189\n",
      " F1-score:  0.251\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "df = train_df\n",
    "\n",
    "for lang in df['lang'].unique():\n",
    "    \n",
    "    lang_df = df[df['lang'] == lang]\n",
    "    texts = lang_df['text'].astype(str)\n",
    "    labels = lang_df['label'].astype(int)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=stop_words_dict.get(lang, None),\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=20000\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            cv_results = cross_validate(model, X, labels, cv=cv, scoring=scoring)\n",
    "            all_results.append({\n",
    "                'Language': lang,\n",
    "                'Model': name,\n",
    "                'Accuracy': np.mean(cv_results['test_accuracy']),\n",
    "                'Precision': np.mean(cv_results['test_precision']),\n",
    "                'Recall': np.mean(cv_results['test_recall']),\n",
    "                'F1': np.mean(cv_results['test_f1'])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"{name} failed for {lang}: {e}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values(['Language', 'F1'], ascending=[True, False])\n",
    "\n",
    "# Print best per language\n",
    "for lang in results_df['Language'].unique():\n",
    "    best = results_df[results_df['Language'] == lang].iloc[0]\n",
    "    print(f\"\\n Best for {lang.upper()}: {best['Model']} \"\n",
    "          f\"(Acc={best['Accuracy']:.3f}, Prec={best['Precision']:.3f}, \"\n",
    "          f\"Rec={best['Recall']:.3f}, F1={best['F1']:.3f})\")\n",
    "\n",
    "print(\"\\nAll Results Summary:\\n\", results_df.round(3))\n",
    "\n",
    "# --- Combined Average Score (across all languages) ---\n",
    "combined_avg = results_df[['Accuracy', 'Precision', 'Recall', 'F1']].mean()\n",
    "\n",
    "print(\"\\nCombined Average Performance Across Languages:\")\n",
    "print(f\" Accuracy:  {combined_avg['Accuracy']:.3f}\")\n",
    "print(f\" Precision: {combined_avg['Precision']:.3f}\")\n",
    "print(f\" Recall:    {combined_avg['Recall']:.3f}\")\n",
    "print(f\" F1-score:  {combined_avg['F1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f99bb-2b9d-4023-861c-42498471de46",
   "metadata": {},
   "source": [
    "# Multi-Lingual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c15ceb5-bc74-4ab0-90e0-4a132ed764a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df['text'].astype(str)\n",
    "labels = train_df['label'].astype(int)\n",
    "langs  = train_df['lang']\n",
    "\n",
    "# Merge language-specific stopwords\n",
    "stop_words_dict = {\n",
    "    'en': stopwords.words('english'),\n",
    "    'es': stopwords.words('spanish'),\n",
    "    'it': stopwords.words('italian'),\n",
    "}\n",
    "\n",
    "combined_stopwords = set()\n",
    "for lang in langs.unique():\n",
    "    combined_stopwords.update(stop_words_dict.get(lang, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37b6c85-3c25-47a8-bc77-4edb8f90c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Performance (Multilingual Unified Dataset):\n",
      "                    Model  Accuracy  Precision  Recall      F1\n",
      "0           LinearSVC (b)    0.8899     0.6659  0.4696  0.5497\n",
      "1  LogisticRegression (b)    0.8645     0.5253  0.5747  0.5478\n",
      "2        RandomForest (b)    0.8869     0.6887  0.4111  0.5068\n",
      "3        RandomForest (u)    0.8906     0.7247  0.3946  0.5056\n",
      "4           LinearSVC (u)    0.8942     0.8215  0.3341  0.4742\n",
      "5  LogisticRegression (u)    0.8638     0.8833  0.0561  0.1054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=list(combined_stopwords),\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=30000\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest (u)\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"LinearSVC (u)\": LinearSVC(class_weight=None, max_iter=1000),\n",
    "    \"LinearSVC (b)\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"LogisticRegression (u)\": LogisticRegression(max_iter=500),\n",
    "    \"LogisticRegression (b)\": LogisticRegression(class_weight='balanced', max_iter=500),\n",
    "    \"RandomForest (b)\": RandomForestClassifier(class_weight='balanced', n_estimators=200),\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([('tfidf', tfidf), ('clf', model)])\n",
    "    scores = cross_validate(pipeline, texts, labels, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Precision': np.mean(scores['test_precision']),\n",
    "        'Recall': np.mean(scores['test_recall']),\n",
    "        'F1': np.mean(scores['test_f1'])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='F1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"\\n Model Performance (Multilingual Unified Dataset):\")\n",
    "print(results_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29cb1b-30c7-4917-871d-393c36633d4c",
   "metadata": {},
   "source": [
    "### Observations (GenAI - Curated)\n",
    "\n",
    "* In the language-specific runs, performance differs sharply by language.\n",
    "    \n",
    "* For English, even though the Decision Tree yields the best accuracy (≈0.86), the extremely low precision and recall show that the classifier mostly predicts the dominant class (likely the non-reclamatory one). The imbalance in label distribution causes accuracy to be misleadingly high, while the model fails to capture reclamatory usage.\n",
    "\n",
    "  \n",
    "* For Spanish, results improve slightly while using KNN, DecisionTree, and even ensemble models (AdaBoost, GradientBoosting) show moderate precision–recall balance, suggesting that lexical patterns in Spanish are somewhat more distinctive for reclamatory contexts, though still not robustly separable.\n",
    "\n",
    "  \n",
    "* For Italian, performance is consistently high, with DecisionTree and ensemble models achieving strong F1 (>0.7). This indicates that Italian examples exhibit clearer lexical markers or less code-mixing noise, allowing the models to learn discriminative cues effectively.\n",
    "\n",
    "* When all languages are merged into a single multilingual dataset, overall performance stabilizes but slightly compresses across models. Linear SVM and Gradient Boosting achieve the best trade-off (F1 ≈ 0.55).  This shows that shared multilingual representations help capture some general structure of reclamatory intent, but language-specific nuances are diluted. Naive Bayes and Decision Tree drop substantially, implying they cannot generalize well to the mixed distribution of lexical and stylistic patterns.\n",
    "\n",
    "* Overall, these results imply that reclamatory intent is easier to detect when lexical cues are linguistically coherent (as in Italian), but harder when linguistic mixing or class imbalance dominates (as in English). Unified multilingual models yield moderate and balanced results, suggesting that cross-lingual lexical and stylistic overlap exists. It also suggests that more expressive models (transformers or multilingual embeddings) would likely be necessary to capture subtle sociolinguistic signals beyond surface words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9d11c-c368-4689-a795-e04727c18751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
