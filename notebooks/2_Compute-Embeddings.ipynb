{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4538b0cc-d8ac-4a57-94b5-bd5e0c466b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from src.embeddings import ModelRegistry\n",
    "from src.embeddings import EmbeddingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0af495d-409a-4823-a3eb-23450420afa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multilingual-e5-large',\n",
       " 'bge-m3',\n",
       " 'gte-multilingual-base',\n",
       " 'jina-embeddings-v3',\n",
       " 'snowflake-arctic-embed-l-v2.0',\n",
       " 'labse',\n",
       " 'use-multilingual',\n",
       " 'xlm-roberta-large']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "considered_models = list(ModelRegistry.list_models().keys())\n",
    "considered_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542c040b-6df2-4abb-9b09-3d63fc1f17e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_es.csv', 'train_it.csv', 'train_en.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = \"../data/multipride_data/\"\n",
    "figures_root = \"../figures/\"\n",
    "embeddings_root = \"../embeddings/\"\n",
    "os.makedirs(figures_root, exist_ok=True)\n",
    "os.makedirs(embeddings_root, exist_ok=True)\n",
    "\n",
    "train_files = [file for file in os.listdir(data_root) if (file.endswith(\".csv\") and (\"train\" in file))]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30166c1e-2fca-489e-96fc-9a6d5f60ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 2988\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame()\n",
    "\n",
    "for file in train_files:\n",
    "    temp_df = pd.read_csv(os.path.join(data_root, file))\n",
    "    if \"en\" in file:\n",
    "        temp_df[\"bio\"] = [None] * temp_df.shape[0]\n",
    "    train_df = pd.concat([train_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(f\"Total training samples: {train_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8ba15-71ab-4998-9893-a83f34144216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>bio</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es_1850</td>\n",
       "      <td>28 de Junio - Día Internacional del Orgullo LG...</td>\n",
       "      <td>Doblajes Para Videojuegos que nunca tuvieron D...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es_773</td>\n",
       "      <td>@USER no me gusta la Montero, por su apoyo a l...</td>\n",
       "      <td>Activista, sindicalista, madre y parte de la R...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es_1899</td>\n",
       "      <td>Es la semana del #GayPride y la dedicaré al #Q...</td>\n",
       "      <td>Pintor daltónico que habla de arte. Confundo e...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es_685</td>\n",
       "      <td>@USER @USER @USER A la carles vais los #TRANSF...</td>\n",
       "      <td>mujer Algemesí Valencia Telegram @USER\\n+34 62...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es_1717</td>\n",
       "      <td>Hoy a las 00:10 en TVE2, estreno del documenta...</td>\n",
       "      <td>Comunidad LGTBI+ sin ánimo de lucro. Reivindic...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  es_1850  28 de Junio - Día Internacional del Orgullo LG...   \n",
       "1   es_773  @USER no me gusta la Montero, por su apoyo a l...   \n",
       "2  es_1899  Es la semana del #GayPride y la dedicaré al #Q...   \n",
       "3   es_685  @USER @USER @USER A la carles vais los #TRANSF...   \n",
       "4  es_1717  Hoy a las 00:10 en TVE2, estreno del documenta...   \n",
       "\n",
       "                                                 bio  label lang  \n",
       "0  Doblajes Para Videojuegos que nunca tuvieron D...      0   es  \n",
       "1  Activista, sindicalista, madre y parte de la R...      0   es  \n",
       "2  Pintor daltónico que habla de arte. Confundo e...      0   es  \n",
       "3  mujer Algemesí Valencia Telegram @USER\\n+34 62...      0   es  \n",
       "4  Comunidad LGTBI+ sin ánimo de lucro. Reivindic...      0   es  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89785880-76ba-415c-aa64-9fea9005487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(train_df.text)\n",
    "ids = list(train_df.id)\n",
    "labels = [int(l) for l in list(train_df.label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd6b879-b849-4875-b339-542833057e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Multilingual E5 Large (intfloat/multilingual-e5-large)...\n",
      "Model: Multilingual E5 Large\n",
      "Device: cuda\n",
      "Embedding Dimension: 1024\n",
      "Max Sequence Length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dac106e8b7342078d8b8cb0e24aac96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'multilingual-e5-large' to 'bge-m3'...\n",
      "Model: BGE-M3\n",
      "Device: cuda\n",
      "Embedding Dimension: 1024\n",
      "Max Sequence Length: 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a311e64e63845a390c83fba12b177e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'bge-m3' to 'gte-multilingual-base'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GTE Multilingual Base\n",
      "Device: cuda\n",
      "Embedding Dimension: 768\n",
      "Max Sequence Length: 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ab2d5774924c66b61c5849aaf6dd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'gte-multilingual-base' to 'jina-embeddings-v3'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Jina Embeddings v3\n",
      "Device: cuda\n",
      "Embedding Dimension: 1024\n",
      "Max Sequence Length: 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e195c807a7184dc79b02b66651677b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'jina-embeddings-v3' to 'snowflake-arctic-embed-l-v2.0'...\n",
      "Model: Arctic Embed 2.0 Large\n",
      "Device: cuda\n",
      "Embedding Dimension: 1024\n",
      "Max Sequence Length: 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96efc2167cd345faa2a4b11d87feb75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'snowflake-arctic-embed-l-v2.0' to 'labse'...\n",
      "Model: LaBSE\n",
      "Device: cuda\n",
      "Embedding Dimension: 768\n",
      "Max Sequence Length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4e1c0bd93949ab84a53c775c7f47ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'labse' to 'use-multilingual'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/use-cmlm-multilingual were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Universal Sentence Encoder Multilingual\n",
      "Device: cuda\n",
      "Embedding Dimension: 768\n",
      "Max Sequence Length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c8e9cff5be4e288487c37c7f28d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name FacebookAI/xlm-roberta-large. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Switching model from 'use-multilingual' to 'xlm-roberta-large'...\n",
      "Model: XLM-RoBERTa model\n",
      "Device: cuda\n",
      "Embedding Dimension: 1024\n",
      "Max Sequence Length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3720cf4c529d43ad88bd66a9f9aacf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = None\n",
    "for i, considered_model in enumerate(considered_models):\n",
    "    gzip_path = os.path.join(embeddings_root, considered_model + \".json.gz\")\n",
    "\n",
    "    if not os.path.exists(gzip_path):\n",
    "        if pipeline is None:\n",
    "            pipeline = EmbeddingPipeline(model_key=considered_model)\n",
    "        else:\n",
    "            pipeline.switch_model(considered_model)\n",
    "            \n",
    "        text_embeddings = pipeline.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "        text_embeddings = text_embeddings.tolist()\n",
    "    \n",
    "        embeddings_dict = {}\n",
    "        for text_id, emb, label in zip(ids, text_embeddings, labels):\n",
    "            embeddings_dict[text_id] = {\"emb\": emb, \"label\": label}\n",
    "    \n",
    "        with gzip.open(gzip_path, 'wt', encoding='utf-8') as f:\n",
    "            json.dump(embeddings_dict, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f485ed-d57c-41e5-9b44-bf650fb36dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
