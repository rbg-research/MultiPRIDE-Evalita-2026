{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4c6172-46ad-4e4f-8957-9eca338ebceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.baseline.baseline import train_df\n",
    "from src.finetune.finetuner import main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50555366-5e60-4daa-a97c-7058a8315547",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093cade9-d5cf-461f-8c38-7d0114cd7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training would be start on the device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for fine-tuning\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"  # Base model\n",
    "    NUM_LABELS = 2  # Binary classification\n",
    "    MAX_LENGTH = 128  # Maximum sequence length\n",
    "    NUM_FROZEN_LAYERS = 6  # Number of initial layers to freeze (0 = only train classification head)\n",
    "\n",
    "    # Training configuration\n",
    "    LEARNING_RATE = 5e-6\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    WARMUP_RATIO = 0.05  # Warmup as % of total steps\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"  # Evaluate at end of each epoch\n",
    "\n",
    "    # Cross-validation\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8  # 80% for training from each fold\n",
    "    VAL_RATIO = 0.2  # 20% for validation from each fold\n",
    "\n",
    "    # Dynamic undersampling\n",
    "    DYNAMIC_UNDERSAMPLE = True  # Balance classes per epoch\n",
    "\n",
    "    # Model saving\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models\"\n",
    "    RESULTS_DIR = \"../results/roberta-fine-tune/\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training would be start on the device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb85403e-856b-4c4c-b82d-ee24fc3b5533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 07:50:52,344 - INFO - ================================================================================\n",
      "2025-11-12 07:50:52,347 - INFO - Starting Fine-tuning Pipeline\n",
      "2025-11-12 07:50:52,349 - INFO - ================================================================================\n",
      "2025-11-12 07:50:52,364 - INFO - Fold 0: Train=1912, Val=478\n",
      "2025-11-12 07:50:52,364 - INFO -   Train label dist: {0: 1638, 1: 274}\n",
      "2025-11-12 07:50:52,364 - INFO -   Train lang dist: {'it': 696, 'en': 656, 'es': 560}\n",
      "2025-11-12 07:50:52,370 - INFO - Fold 1: Train=1912, Val=478\n",
      "2025-11-12 07:50:52,370 - INFO -   Train label dist: {0: 1639, 1: 273}\n",
      "2025-11-12 07:50:52,371 - INFO -   Train lang dist: {'it': 696, 'en': 656, 'es': 560}\n",
      "2025-11-12 07:50:52,376 - INFO - Fold 2: Train=1912, Val=478\n",
      "2025-11-12 07:50:52,376 - INFO -   Train label dist: {0: 1638, 1: 274}\n",
      "2025-11-12 07:50:52,377 - INFO -   Train lang dist: {'it': 695, 'en': 657, 'es': 560}\n",
      "2025-11-12 07:50:52,387 - INFO - Fold 3: Train=1912, Val=479\n",
      "2025-11-12 07:50:52,388 - INFO -   Train label dist: {0: 1639, 1: 273}\n",
      "2025-11-12 07:50:52,390 - INFO -   Train lang dist: {'it': 695, 'en': 657, 'es': 560}\n",
      "2025-11-12 07:50:52,398 - INFO - Fold 4: Train=1912, Val=479\n",
      "2025-11-12 07:50:52,398 - INFO -   Train label dist: {0: 1640, 1: 272}\n",
      "2025-11-12 07:50:52,399 - INFO -   Train lang dist: {'it': 695, 'en': 657, 'es': 560}\n",
      "2025-11-12 07:50:52,400 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:50:52,400 - INFO - Fold 1/5\n",
      "2025-11-12 07:50:52,401 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 0:\n",
      "  Train: 274 positive samples\n",
      "  Val:   68 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-12 07:50:55,866 - INFO - Froze: Embeddings + First 6 Encoder Layers\n",
      "2025-11-12 07:50:55,866 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-11-12 07:50:55,867 - INFO - Trainable parameters: 43,119,362 / 278,045,186 (15.51%)\n",
      "2025-11-12 07:50:55,868 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-11-12 07:50:55,868 - INFO - Language weights: {'it': 0.9080362792659776, 'en': 0.9634043450748787, 'es': 1.1285593756591437}\n",
      "2025-11-12 07:50:55,868 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-11-12 07:50:55,869 - INFO - \n",
      "Epoch 1/10\n",
      "2025-11-12 07:50:55,870 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.92it/s, loss=0.334]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.51it/s]\n",
      "2025-11-12 07:50:59,170 - INFO - Train Loss: 0.3267\n",
      "2025-11-12 07:50:59,170 - INFO - Val Loss: 0.1151\n",
      "2025-11-12 07:50:59,170 - INFO - Overall - Precision: 0.4289, Recall: 0.5000, F1: 0.4617\n",
      "2025-11-12 07:50:59,170 - INFO - en - Precision: 0.4576, Recall: 0.5000, F1: 0.4778\n",
      "2025-11-12 07:50:59,170 - INFO - es - Precision: 0.4250, Recall: 0.5000, F1: 0.4595\n",
      "2025-11-12 07:50:59,170 - INFO - it - Precision: 0.4046, Recall: 0.5000, F1: 0.4473\n",
      "2025-11-12 07:50:59,817 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4617.pt (F1: 0.4617, Fold: 0, Epoch: 0)\n",
      "2025-11-12 07:50:59,817 - INFO - \n",
      "Epoch 2/10\n",
      "2025-11-12 07:50:59,818 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.42it/s, loss=0.318]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 159.96it/s]\n",
      "2025-11-12 07:51:02,966 - INFO - Train Loss: 0.3086\n",
      "2025-11-12 07:51:02,966 - INFO - Val Loss: 0.1100\n",
      "2025-11-12 07:51:02,966 - INFO - Overall - Precision: 0.4289, Recall: 0.5000, F1: 0.4617\n",
      "2025-11-12 07:51:02,966 - INFO - en - Precision: 0.4576, Recall: 0.5000, F1: 0.4778\n",
      "2025-11-12 07:51:02,967 - INFO - es - Precision: 0.4250, Recall: 0.5000, F1: 0.4595\n",
      "2025-11-12 07:51:02,967 - INFO - it - Precision: 0.4046, Recall: 0.5000, F1: 0.4473\n",
      "2025-11-12 07:51:02,967 - INFO - \n",
      "Epoch 3/10\n",
      "2025-11-12 07:51:02,968 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.74it/s, loss=0.259]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.53it/s]\n",
      "2025-11-12 07:51:06,095 - INFO - Train Loss: 0.2519\n",
      "2025-11-12 07:51:06,095 - INFO - Val Loss: 0.1252\n",
      "2025-11-12 07:51:06,095 - INFO - Overall - Precision: 0.5305, Recall: 0.5124, F1: 0.5057\n",
      "2025-11-12 07:51:06,095 - INFO - en - Precision: 0.4576, Recall: 0.5000, F1: 0.4778\n",
      "2025-11-12 07:51:06,095 - INFO - es - Precision: 0.6828, Recall: 0.5588, F1: 0.5696\n",
      "2025-11-12 07:51:06,096 - INFO - it - Precision: 0.4520, Recall: 0.4696, F1: 0.4568\n",
      "2025-11-12 07:51:06,737 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_2_f1_0.5057.pt (F1: 0.5057, Fold: 0, Epoch: 2)\n",
      "2025-11-12 07:51:06,737 - INFO - \n",
      "Epoch 4/10\n",
      "2025-11-12 07:51:06,738 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.42it/s, loss=0.244]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.18it/s]\n",
      "2025-11-12 07:51:09,880 - INFO - Train Loss: 0.2367\n",
      "2025-11-12 07:51:09,880 - INFO - Val Loss: 0.1335\n",
      "2025-11-12 07:51:09,880 - INFO - Overall - Precision: 0.5975, Recall: 0.6057, F1: 0.6012\n",
      "2025-11-12 07:51:09,881 - INFO - en - Precision: 0.9604, Recall: 0.5357, F1: 0.5460\n",
      "2025-11-12 07:51:09,881 - INFO - es - Precision: 0.6537, Recall: 0.6597, F1: 0.6566\n",
      "2025-11-12 07:51:09,881 - INFO - it - Precision: 0.5424, Recall: 0.5577, F1: 0.5399\n",
      "2025-11-12 07:51:09,950 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_0_f1_0.4617.pt\n",
      "2025-11-12 07:51:10,598 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_3_f1_0.6012.pt (F1: 0.6012, Fold: 0, Epoch: 3)\n",
      "2025-11-12 07:51:10,599 - INFO - \n",
      "Epoch 5/10\n",
      "2025-11-12 07:51:10,599 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.19it/s, loss=0.217]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.44it/s]\n",
      "2025-11-12 07:51:13,749 - INFO - Train Loss: 0.2107\n",
      "2025-11-12 07:51:13,750 - INFO - Val Loss: 0.1369\n",
      "2025-11-12 07:51:13,750 - INFO - Overall - Precision: 0.6024, Recall: 0.6524, F1: 0.6132\n",
      "2025-11-12 07:51:13,750 - INFO - en - Precision: 0.9604, Recall: 0.5357, F1: 0.5460\n",
      "2025-11-12 07:51:13,750 - INFO - es - Precision: 0.6146, Recall: 0.6653, F1: 0.6267\n",
      "2025-11-12 07:51:13,750 - INFO - it - Precision: 0.5807, Recall: 0.6298, F1: 0.5511\n",
      "2025-11-12 07:51:13,816 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_2_f1_0.5057.pt\n",
      "2025-11-12 07:51:14,465 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_4_f1_0.6132.pt (F1: 0.6132, Fold: 0, Epoch: 4)\n",
      "2025-11-12 07:51:14,465 - INFO - \n",
      "Epoch 6/10\n",
      "2025-11-12 07:51:14,466 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 56.54it/s, loss=0.202]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.07it/s]\n",
      "2025-11-12 07:51:17,642 - INFO - Train Loss: 0.1961\n",
      "2025-11-12 07:51:17,642 - INFO - Val Loss: 0.1525\n",
      "2025-11-12 07:51:17,642 - INFO - Overall - Precision: 0.6025, Recall: 0.6882, F1: 0.5992\n",
      "2025-11-12 07:51:17,643 - INFO - en - Precision: 0.6265, Recall: 0.5291, F1: 0.5349\n",
      "2025-11-12 07:51:17,643 - INFO - es - Precision: 0.6435, Recall: 0.7395, F1: 0.6557\n",
      "2025-11-12 07:51:17,643 - INFO - it - Precision: 0.5899, Recall: 0.6287, F1: 0.4637\n",
      "2025-11-12 07:51:17,643 - INFO - \n",
      "Epoch 7/10\n",
      "2025-11-12 07:51:17,644 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.93it/s, loss=0.197]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.12it/s]\n",
      "2025-11-12 07:51:20,765 - INFO - Train Loss: 0.1917\n",
      "2025-11-12 07:51:20,765 - INFO - Val Loss: 0.1503\n",
      "2025-11-12 07:51:20,765 - INFO - Overall - Precision: 0.6345, Recall: 0.7223, F1: 0.6485\n",
      "2025-11-12 07:51:20,765 - INFO - en - Precision: 0.5846, Recall: 0.5258, F1: 0.5299\n",
      "2025-11-12 07:51:20,766 - INFO - es - Precision: 0.6074, Recall: 0.7045, F1: 0.5856\n",
      "2025-11-12 07:51:20,766 - INFO - it - Precision: 0.6697, Recall: 0.7671, F1: 0.6650\n",
      "2025-11-12 07:51:20,834 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_3_f1_0.6012.pt\n",
      "2025-11-12 07:51:21,484 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_6_f1_0.6485.pt (F1: 0.6485, Fold: 0, Epoch: 6)\n",
      "2025-11-12 07:51:21,485 - INFO - \n",
      "Epoch 8/10\n",
      "2025-11-12 07:51:21,486 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.35it/s, loss=0.186]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 158.00it/s]\n",
      "2025-11-12 07:51:24,645 - INFO - Train Loss: 0.1805\n",
      "2025-11-12 07:51:24,646 - INFO - Val Loss: 0.1404\n",
      "2025-11-12 07:51:24,646 - INFO - Overall - Precision: 0.6675, Recall: 0.7664, F1: 0.6896\n",
      "2025-11-12 07:51:24,646 - INFO - en - Precision: 0.5726, Recall: 0.5482, F1: 0.5560\n",
      "2025-11-12 07:51:24,646 - INFO - es - Precision: 0.6250, Recall: 0.7367, F1: 0.6090\n",
      "2025-11-12 07:51:24,646 - INFO - it - Precision: 0.7367, Recall: 0.8474, F1: 0.7574\n",
      "2025-11-12 07:51:24,713 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_4_f1_0.6132.pt\n",
      "2025-11-12 07:51:25,359 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_7_f1_0.6896.pt (F1: 0.6896, Fold: 0, Epoch: 7)\n",
      "2025-11-12 07:51:25,359 - INFO - \n",
      "Epoch 9/10\n",
      "2025-11-12 07:51:25,360 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.82it/s, loss=0.191]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.80it/s]\n",
      "2025-11-12 07:51:28,478 - INFO - Train Loss: 0.1856\n",
      "2025-11-12 07:51:28,478 - INFO - Val Loss: 0.1021\n",
      "2025-11-12 07:51:28,479 - INFO - Overall - Precision: 0.7012, Recall: 0.7343, F1: 0.7153\n",
      "2025-11-12 07:51:28,479 - INFO - en - Precision: 0.4573, Recall: 0.4967, F1: 0.4762\n",
      "2025-11-12 07:51:28,479 - INFO - es - Precision: 0.6534, Recall: 0.7213, F1: 0.6719\n",
      "2025-11-12 07:51:28,479 - INFO - it - Precision: 0.7488, Recall: 0.8189, F1: 0.7724\n",
      "2025-11-12 07:51:28,548 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_6_f1_0.6485.pt\n",
      "2025-11-12 07:51:29,200 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.7153.pt (F1: 0.7153, Fold: 0, Epoch: 8)\n",
      "2025-11-12 07:51:29,201 - INFO - \n",
      "Epoch 10/10\n",
      "2025-11-12 07:51:29,202 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.26it/s, loss=0.182]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.05it/s]\n",
      "2025-11-12 07:51:32,346 - INFO - Train Loss: 0.1770\n",
      "2025-11-12 07:51:32,347 - INFO - Val Loss: 0.1271\n",
      "2025-11-12 07:51:32,347 - INFO - Overall - Precision: 0.6948, Recall: 0.7847, F1: 0.7216\n",
      "2025-11-12 07:51:32,347 - INFO - en - Precision: 0.6210, Recall: 0.6131, F1: 0.6168\n",
      "2025-11-12 07:51:32,347 - INFO - es - Precision: 0.6699, Recall: 0.7759, F1: 0.6893\n",
      "2025-11-12 07:51:32,347 - INFO - it - Precision: 0.7371, Recall: 0.8394, F1: 0.7597\n",
      "2025-11-12 07:51:32,414 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_7_f1_0.6896.pt\n",
      "2025-11-12 07:51:33,054 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_9_f1_0.7216.pt (F1: 0.7216, Fold: 0, Epoch: 9)\n",
      "2025-11-12 07:51:33,055 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:51:33,055 - INFO - Fold 2/5\n",
      "2025-11-12 07:51:33,055 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1:\n",
      "  Train: 273 positive samples\n",
      "  Val:   69 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-12 07:51:36,064 - INFO - Froze: Embeddings + First 6 Encoder Layers\n",
      "2025-11-12 07:51:36,064 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-11-12 07:51:36,065 - INFO - Trainable parameters: 43,119,362 / 278,045,186 (15.51%)\n",
      "2025-11-12 07:51:36,066 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-11-12 07:51:36,066 - INFO - Language weights: {'it': 0.9080362792659776, 'en': 0.9634043450748787, 'es': 1.1285593756591437}\n",
      "2025-11-12 07:51:36,066 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-11-12 07:51:36,067 - INFO - \n",
      "Epoch 1/10\n",
      "2025-11-12 07:51:36,068 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.41it/s, loss=0.351]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.15it/s]\n",
      "2025-11-12 07:51:39,218 - INFO - Train Loss: 0.3410\n",
      "2025-11-12 07:51:39,218 - INFO - Val Loss: 0.1158\n",
      "2025-11-12 07:51:39,219 - INFO - Overall - Precision: 0.4278, Recall: 0.5000, F1: 0.4611\n",
      "2025-11-12 07:51:39,219 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:51:39,219 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:51:39,219 - INFO - it - Precision: 0.4046, Recall: 0.5000, F1: 0.4473\n",
      "2025-11-12 07:51:39,866 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 1, Epoch: 0)\n",
      "2025-11-12 07:51:39,867 - INFO - \n",
      "Epoch 2/10\n",
      "2025-11-12 07:51:39,868 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.65it/s, loss=0.288]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.96it/s]\n",
      "2025-11-12 07:51:43,002 - INFO - Train Loss: 0.2798\n",
      "2025-11-12 07:51:43,002 - INFO - Val Loss: 0.1119\n",
      "2025-11-12 07:51:43,002 - INFO - Overall - Precision: 0.6786, Recall: 0.5060, F1: 0.4751\n",
      "2025-11-12 07:51:43,002 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:51:43,002 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:51:43,002 - INFO - it - Precision: 0.6564, Recall: 0.5116, F1: 0.4755\n",
      "2025-11-12 07:51:43,643 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_1_f1_0.4751.pt (F1: 0.4751, Fold: 1, Epoch: 1)\n",
      "2025-11-12 07:51:43,644 - INFO - \n",
      "Epoch 3/10\n",
      "2025-11-12 07:51:43,644 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.89it/s, loss=0.263]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.76it/s]\n",
      "2025-11-12 07:51:46,769 - INFO - Train Loss: 0.2554\n",
      "2025-11-12 07:51:46,769 - INFO - Val Loss: 0.1343\n",
      "2025-11-12 07:51:46,769 - INFO - Overall - Precision: 0.5689, Recall: 0.5359, F1: 0.5391\n",
      "2025-11-12 07:51:46,769 - INFO - en - Precision: 0.4565, Recall: 0.4900, F1: 0.4727\n",
      "2025-11-12 07:51:46,770 - INFO - es - Precision: 0.6823, Recall: 0.5741, F1: 0.5897\n",
      "2025-11-12 07:51:46,770 - INFO - it - Precision: 0.5209, Recall: 0.5150, F1: 0.5136\n",
      "2025-11-12 07:51:47,415 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_2_f1_0.5391.pt (F1: 0.5391, Fold: 1, Epoch: 2)\n",
      "2025-11-12 07:51:47,415 - INFO - \n",
      "Epoch 4/10\n",
      "2025-11-12 07:51:47,417 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.36it/s, loss=0.231]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.56it/s]\n",
      "2025-11-12 07:51:50,555 - INFO - Train Loss: 0.2245\n",
      "2025-11-12 07:51:50,555 - INFO - Val Loss: 0.1475\n",
      "2025-11-12 07:51:50,556 - INFO - Overall - Precision: 0.5654, Recall: 0.5882, F1: 0.5707\n",
      "2025-11-12 07:51:50,556 - INFO - en - Precision: 0.4562, Recall: 0.4867, F1: 0.4710\n",
      "2025-11-12 07:51:50,556 - INFO - es - Precision: 0.6093, Recall: 0.6736, F1: 0.6160\n",
      "2025-11-12 07:51:50,556 - INFO - it - Precision: 0.5201, Recall: 0.5282, F1: 0.5123\n",
      "2025-11-12 07:51:51,196 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_3_f1_0.5707.pt (F1: 0.5707, Fold: 1, Epoch: 3)\n",
      "2025-11-12 07:51:51,196 - INFO - \n",
      "Epoch 5/10\n",
      "2025-11-12 07:51:51,197 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.18it/s, loss=0.213]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.64it/s]\n",
      "2025-11-12 07:51:54,347 - INFO - Train Loss: 0.2068\n",
      "2025-11-12 07:51:54,347 - INFO - Val Loss: 0.1658\n",
      "2025-11-12 07:51:54,347 - INFO - Overall - Precision: 0.6034, Recall: 0.6961, F1: 0.5916\n",
      "2025-11-12 07:51:54,348 - INFO - en - Precision: 0.4562, Recall: 0.4867, F1: 0.4710\n",
      "2025-11-12 07:51:54,348 - INFO - es - Precision: 0.6310, Recall: 0.7487, F1: 0.5863\n",
      "2025-11-12 07:51:54,348 - INFO - it - Precision: 0.6069, Recall: 0.6644, F1: 0.5130\n",
      "2025-11-12 07:51:54,993 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_4_f1_0.5916.pt (F1: 0.5916, Fold: 1, Epoch: 4)\n",
      "2025-11-12 07:51:54,993 - INFO - \n",
      "Epoch 6/10\n",
      "2025-11-12 07:51:54,994 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.91it/s, loss=0.2]  \n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.04it/s]\n",
      "2025-11-12 07:51:58,117 - INFO - Train Loss: 0.1942\n",
      "2025-11-12 07:51:58,117 - INFO - Val Loss: 0.1444\n",
      "2025-11-12 07:51:58,117 - INFO - Overall - Precision: 0.6304, Recall: 0.7148, F1: 0.6429\n",
      "2025-11-12 07:51:58,117 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:51:58,117 - INFO - es - Precision: 0.6341, Recall: 0.7452, F1: 0.6219\n",
      "2025-11-12 07:51:58,117 - INFO - it - Precision: 0.6441, Recall: 0.7305, F1: 0.6268\n",
      "2025-11-12 07:51:58,756 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_5_f1_0.6429.pt (F1: 0.6429, Fold: 1, Epoch: 5)\n",
      "2025-11-12 07:51:58,757 - INFO - \n",
      "Epoch 7/10\n",
      "2025-11-12 07:51:58,757 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.41it/s, loss=0.185]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.42it/s]\n",
      "2025-11-12 07:52:01,902 - INFO - Train Loss: 0.1795\n",
      "2025-11-12 07:52:01,903 - INFO - Val Loss: 0.1521\n",
      "2025-11-12 07:52:01,903 - INFO - Overall - Precision: 0.6560, Recall: 0.7488, F1: 0.6751\n",
      "2025-11-12 07:52:01,903 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:52:01,904 - INFO - es - Precision: 0.6407, Recall: 0.7672, F1: 0.5909\n",
      "2025-11-12 07:52:01,904 - INFO - it - Precision: 0.7154, Recall: 0.8055, F1: 0.7353\n",
      "2025-11-12 07:52:02,570 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_6_f1_0.6751.pt (F1: 0.6751, Fold: 1, Epoch: 6)\n",
      "2025-11-12 07:52:02,571 - INFO - \n",
      "Epoch 8/10\n",
      "2025-11-12 07:52:02,572 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.65it/s, loss=0.171]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.22it/s]\n",
      "2025-11-12 07:52:05,704 - INFO - Train Loss: 0.1657\n",
      "2025-11-12 07:52:05,704 - INFO - Val Loss: 0.1832\n",
      "2025-11-12 07:52:05,705 - INFO - Overall - Precision: 0.6518, Recall: 0.7535, F1: 0.6682\n",
      "2025-11-12 07:52:05,705 - INFO - en - Precision: 0.4557, Recall: 0.4800, F1: 0.4675\n",
      "2025-11-12 07:52:05,705 - INFO - es - Precision: 0.6344, Recall: 0.7546, F1: 0.5736\n",
      "2025-11-12 07:52:05,705 - INFO - it - Precision: 0.7282, Recall: 0.8323, F1: 0.7483\n",
      "2025-11-12 07:52:05,705 - INFO - \n",
      "Epoch 9/10\n",
      "2025-11-12 07:52:05,706 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.38it/s, loss=0.176]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 158.63it/s]\n",
      "2025-11-12 07:52:08,862 - INFO - Train Loss: 0.1707\n",
      "2025-11-12 07:52:08,862 - INFO - Val Loss: 0.1631\n",
      "2025-11-12 07:52:08,862 - INFO - Overall - Precision: 0.6635, Recall: 0.7633, F1: 0.6839\n",
      "2025-11-12 07:52:08,862 - INFO - en - Precision: 0.4557, Recall: 0.4800, F1: 0.4675\n",
      "2025-11-12 07:52:08,862 - INFO - es - Precision: 0.6524, Recall: 0.7882, F1: 0.6201\n",
      "2025-11-12 07:52:08,862 - INFO - it - Precision: 0.7282, Recall: 0.8323, F1: 0.7483\n",
      "2025-11-12 07:52:09,494 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_8_f1_0.6839.pt (F1: 0.6839, Fold: 1, Epoch: 8)\n",
      "2025-11-12 07:52:09,495 - INFO - \n",
      "Epoch 10/10\n",
      "2025-11-12 07:52:09,496 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.51it/s, loss=0.18] \n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.94it/s]\n",
      "2025-11-12 07:52:12,636 - INFO - Train Loss: 0.1749\n",
      "2025-11-12 07:52:12,636 - INFO - Val Loss: 0.1643\n",
      "2025-11-12 07:52:12,636 - INFO - Overall - Precision: 0.6682, Recall: 0.7670, F1: 0.6899\n",
      "2025-11-12 07:52:12,636 - INFO - en - Precision: 0.4539, Recall: 0.4600, F1: 0.4570\n",
      "2025-11-12 07:52:12,636 - INFO - es - Precision: 0.6576, Recall: 0.7966, F1: 0.6319\n",
      "2025-11-12 07:52:12,636 - INFO - it - Precision: 0.7623, Recall: 0.8573, F1: 0.7895\n",
      "2025-11-12 07:52:13,274 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_1_epoch_9_f1_0.6899.pt (F1: 0.6899, Fold: 1, Epoch: 9)\n",
      "2025-11-12 07:52:13,274 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:52:13,274 - INFO - Fold 3/5\n",
      "2025-11-12 07:52:13,275 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 2:\n",
      "  Train: 274 positive samples\n",
      "  Val:   69 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-12 07:52:16,310 - INFO - Froze: Embeddings + First 6 Encoder Layers\n",
      "2025-11-12 07:52:16,310 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-11-12 07:52:16,311 - INFO - Trainable parameters: 43,119,362 / 278,045,186 (15.51%)\n",
      "2025-11-12 07:52:16,312 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-11-12 07:52:16,312 - INFO - Language weights: {'it': 0.9093912592122662, 'en': 0.961989231586796, 'es': 1.1286195092009375}\n",
      "2025-11-12 07:52:16,313 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-11-12 07:52:16,314 - INFO - \n",
      "Epoch 1/10\n",
      "2025-11-12 07:52:16,315 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.28it/s, loss=0.344]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.83it/s]\n",
      "2025-11-12 07:52:19,464 - INFO - Train Loss: 0.3337\n",
      "2025-11-12 07:52:19,464 - INFO - Val Loss: 0.1138\n",
      "2025-11-12 07:52:19,464 - INFO - Overall - Precision: 0.4278, Recall: 0.5000, F1: 0.4611\n",
      "2025-11-12 07:52:19,464 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:52:19,465 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:52:19,465 - INFO - it - Precision: 0.4046, Recall: 0.5000, F1: 0.4473\n",
      "2025-11-12 07:52:20,106 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 2, Epoch: 0)\n",
      "2025-11-12 07:52:20,106 - INFO - \n",
      "Epoch 2/10\n",
      "2025-11-12 07:52:20,107 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.34it/s, loss=0.325]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 157.85it/s]\n",
      "2025-11-12 07:52:23,268 - INFO - Train Loss: 0.3156\n",
      "2025-11-12 07:52:23,268 - INFO - Val Loss: 0.1096\n",
      "2025-11-12 07:52:23,268 - INFO - Overall - Precision: 0.4275, Recall: 0.4976, F1: 0.4599\n",
      "2025-11-12 07:52:23,268 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:52:23,268 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:52:23,269 - INFO - it - Precision: 0.4035, Recall: 0.4929, F1: 0.4437\n",
      "2025-11-12 07:52:23,269 - INFO - \n",
      "Epoch 3/10\n",
      "2025-11-12 07:52:23,270 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.46it/s, loss=0.251]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.52it/s]\n",
      "2025-11-12 07:52:26,408 - INFO - Train Loss: 0.2435\n",
      "2025-11-12 07:52:26,408 - INFO - Val Loss: 0.1312\n",
      "2025-11-12 07:52:26,409 - INFO - Overall - Precision: 0.5849, Recall: 0.5527, F1: 0.5599\n",
      "2025-11-12 07:52:26,409 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:52:26,409 - INFO - es - Precision: 0.6796, Recall: 0.5556, F1: 0.5638\n",
      "2025-11-12 07:52:26,409 - INFO - it - Precision: 0.5555, Recall: 0.5542, F1: 0.5548\n",
      "2025-11-12 07:52:27,050 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_2_f1_0.5599.pt (F1: 0.5599, Fold: 2, Epoch: 2)\n",
      "2025-11-12 07:52:27,050 - INFO - \n",
      "Epoch 4/10\n",
      "2025-11-12 07:52:27,051 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.78it/s, loss=0.244]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.02it/s]\n",
      "2025-11-12 07:52:30,174 - INFO - Train Loss: 0.2368\n",
      "2025-11-12 07:52:30,174 - INFO - Val Loss: 0.1477\n",
      "2025-11-12 07:52:30,175 - INFO - Overall - Precision: 0.5595, Recall: 0.5881, F1: 0.5625\n",
      "2025-11-12 07:52:30,175 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:52:30,175 - INFO - es - Precision: 0.5817, Recall: 0.5877, F1: 0.5844\n",
      "2025-11-12 07:52:30,175 - INFO - it - Precision: 0.5350, Recall: 0.5566, F1: 0.4809\n",
      "2025-11-12 07:52:30,817 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_3_f1_0.5625.pt (F1: 0.5625, Fold: 2, Epoch: 3)\n",
      "2025-11-12 07:52:30,818 - INFO - \n",
      "Epoch 5/10\n",
      "2025-11-12 07:52:30,819 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.42it/s, loss=0.226]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.33it/s]\n",
      "2025-11-12 07:52:33,955 - INFO - Train Loss: 0.2199\n",
      "2025-11-12 07:52:33,956 - INFO - Val Loss: 0.1436\n",
      "2025-11-12 07:52:33,956 - INFO - Overall - Precision: 0.6121, Recall: 0.6763, F1: 0.6231\n",
      "2025-11-12 07:52:33,956 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:52:33,956 - INFO - es - Precision: 0.5976, Recall: 0.6610, F1: 0.5987\n",
      "2025-11-12 07:52:33,956 - INFO - it - Precision: 0.6273, Recall: 0.7047, F1: 0.6039\n",
      "2025-11-12 07:52:34,599 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_4_f1_0.6231.pt (F1: 0.6231, Fold: 2, Epoch: 4)\n",
      "2025-11-12 07:52:34,600 - INFO - \n",
      "Epoch 6/10\n",
      "2025-11-12 07:52:34,601 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.48it/s, loss=0.198]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.67it/s]\n",
      "2025-11-12 07:52:37,743 - INFO - Train Loss: 0.1919\n",
      "2025-11-12 07:52:37,743 - INFO - Val Loss: 0.1569\n",
      "2025-11-12 07:52:37,743 - INFO - Overall - Precision: 0.6222, Recall: 0.7230, F1: 0.6226\n",
      "2025-11-12 07:52:37,743 - INFO - en - Precision: 0.4571, Recall: 0.4967, F1: 0.4760\n",
      "2025-11-12 07:52:37,744 - INFO - es - Precision: 0.6255, Recall: 0.7267, F1: 0.6159\n",
      "2025-11-12 07:52:37,744 - INFO - it - Precision: 0.6528, Recall: 0.7321, F1: 0.5511\n",
      "2025-11-12 07:52:37,744 - INFO - \n",
      "Epoch 7/10\n",
      "2025-11-12 07:52:37,745 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.66it/s, loss=0.185]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.47it/s]\n",
      "2025-11-12 07:52:40,880 - INFO - Train Loss: 0.1801\n",
      "2025-11-12 07:52:40,880 - INFO - Val Loss: 0.1558\n",
      "2025-11-12 07:52:40,880 - INFO - Overall - Precision: 0.6402, Recall: 0.7461, F1: 0.6498\n",
      "2025-11-12 07:52:40,880 - INFO - en - Precision: 0.4571, Recall: 0.4967, F1: 0.4760\n",
      "2025-11-12 07:52:40,880 - INFO - es - Precision: 0.6172, Recall: 0.7200, F1: 0.5874\n",
      "2025-11-12 07:52:40,880 - INFO - it - Precision: 0.6875, Recall: 0.8036, F1: 0.6505\n",
      "2025-11-12 07:52:41,518 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_6_f1_0.6498.pt (F1: 0.6498, Fold: 2, Epoch: 6)\n",
      "2025-11-12 07:52:41,518 - INFO - \n",
      "Epoch 8/10\n",
      "2025-11-12 07:52:41,519 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 56.93it/s, loss=0.197]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.13it/s]\n",
      "2025-11-12 07:52:44,678 - INFO - Train Loss: 0.1915\n",
      "2025-11-12 07:52:44,678 - INFO - Val Loss: 0.1199\n",
      "2025-11-12 07:52:44,678 - INFO - Overall - Precision: 0.7035, Recall: 0.7746, F1: 0.7281\n",
      "2025-11-12 07:52:44,678 - INFO - en - Precision: 0.4571, Recall: 0.4967, F1: 0.4760\n",
      "2025-11-12 07:52:44,678 - INFO - es - Precision: 0.6423, Recall: 0.7376, F1: 0.6512\n",
      "2025-11-12 07:52:44,678 - INFO - it - Precision: 0.7786, Recall: 0.8876, F1: 0.8081\n",
      "2025-11-12 07:52:44,685 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_8_f1_0.7153.pt\n",
      "2025-11-12 07:52:45,343 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_7_f1_0.7281.pt (F1: 0.7281, Fold: 2, Epoch: 7)\n",
      "2025-11-12 07:52:45,343 - INFO - \n",
      "Epoch 9/10\n",
      "2025-11-12 07:52:45,344 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.93it/s, loss=0.187]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.57it/s]\n",
      "2025-11-12 07:52:48,463 - INFO - Train Loss: 0.1819\n",
      "2025-11-12 07:52:48,463 - INFO - Val Loss: 0.1202\n",
      "2025-11-12 07:52:48,463 - INFO - Overall - Precision: 0.7108, Recall: 0.7782, F1: 0.7352\n",
      "2025-11-12 07:52:48,463 - INFO - en - Precision: 0.4571, Recall: 0.4967, F1: 0.4760\n",
      "2025-11-12 07:52:48,463 - INFO - es - Precision: 0.6344, Recall: 0.7292, F1: 0.6389\n",
      "2025-11-12 07:52:48,464 - INFO - it - Precision: 0.8083, Recall: 0.9054, F1: 0.8402\n",
      "2025-11-12 07:52:48,470 - INFO - Deleted checkpoint: ../fine_tuned_models/checkpoints/fold_0_epoch_9_f1_0.7216.pt\n",
      "2025-11-12 07:52:49,117 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_2_epoch_8_f1_0.7352.pt (F1: 0.7352, Fold: 2, Epoch: 8)\n",
      "2025-11-12 07:52:49,117 - INFO - \n",
      "Epoch 10/10\n",
      "2025-11-12 07:52:49,118 - INFO - Undersampling: Minority=274, Majority=548\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.56it/s, loss=0.171]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.37it/s]\n",
      "2025-11-12 07:52:52,255 - INFO - Train Loss: 0.1655\n",
      "2025-11-12 07:52:52,255 - INFO - Val Loss: 0.1316\n",
      "2025-11-12 07:52:52,255 - INFO - Overall - Precision: 0.6993, Recall: 0.7877, F1: 0.7262\n",
      "2025-11-12 07:52:52,256 - INFO - en - Precision: 0.5300, Recall: 0.5157, F1: 0.5167\n",
      "2025-11-12 07:52:52,256 - INFO - es - Precision: 0.6344, Recall: 0.7292, F1: 0.6389\n",
      "2025-11-12 07:52:52,256 - INFO - it - Precision: 0.7946, Recall: 0.9179, F1: 0.8260\n",
      "2025-11-12 07:52:52,256 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:52:52,256 - INFO - Fold 4/5\n",
      "2025-11-12 07:52:52,256 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 3:\n",
      "  Train: 273 positive samples\n",
      "  Val:   70 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-12 07:52:55,627 - INFO - Froze: Embeddings + First 6 Encoder Layers\n",
      "2025-11-12 07:52:55,627 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-11-12 07:52:55,628 - INFO - Trainable parameters: 43,119,362 / 278,045,186 (15.51%)\n",
      "2025-11-12 07:52:55,628 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-11-12 07:52:55,629 - INFO - Language weights: {'it': 0.9093912592122662, 'en': 0.961989231586796, 'es': 1.1286195092009375}\n",
      "2025-11-12 07:52:55,629 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-11-12 07:52:55,630 - INFO - \n",
      "Epoch 1/10\n",
      "2025-11-12 07:52:55,631 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.54it/s, loss=0.349]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.66it/s]\n",
      "2025-11-12 07:52:58,774 - INFO - Train Loss: 0.3388\n",
      "2025-11-12 07:52:58,774 - INFO - Val Loss: 0.1175\n",
      "2025-11-12 07:52:58,774 - INFO - Overall - Precision: 0.4269, Recall: 0.5000, F1: 0.4606\n",
      "2025-11-12 07:52:58,775 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:52:58,775 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:52:58,775 - INFO - it - Precision: 0.4023, Recall: 0.5000, F1: 0.4459\n",
      "2025-11-12 07:52:59,433 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_0_f1_0.4606.pt (F1: 0.4606, Fold: 3, Epoch: 0)\n",
      "2025-11-12 07:52:59,434 - INFO - \n",
      "Epoch 2/10\n",
      "2025-11-12 07:52:59,435 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.23it/s, loss=0.285]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.16it/s]\n",
      "2025-11-12 07:53:02,585 - INFO - Train Loss: 0.2765\n",
      "2025-11-12 07:53:02,585 - INFO - Val Loss: 0.1129\n",
      "2025-11-12 07:53:02,586 - INFO - Overall - Precision: 0.4269, Recall: 0.5000, F1: 0.4606\n",
      "2025-11-12 07:53:02,586 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:53:02,586 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:53:02,586 - INFO - it - Precision: 0.4023, Recall: 0.5000, F1: 0.4459\n",
      "2025-11-12 07:53:02,586 - INFO - \n",
      "Epoch 3/10\n",
      "2025-11-12 07:53:02,587 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.72it/s, loss=0.256]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.40it/s]\n",
      "2025-11-12 07:53:05,721 - INFO - Train Loss: 0.2489\n",
      "2025-11-12 07:53:05,721 - INFO - Val Loss: 0.1336\n",
      "2025-11-12 07:53:05,721 - INFO - Overall - Precision: 0.5401, Recall: 0.5252, F1: 0.5262\n",
      "2025-11-12 07:53:05,721 - INFO - en - Precision: 0.6263, Recall: 0.5290, F1: 0.5347\n",
      "2025-11-12 07:53:05,721 - INFO - es - Precision: 0.5926, Recall: 0.5286, F1: 0.5242\n",
      "2025-11-12 07:53:05,721 - INFO - it - Precision: 0.4952, Recall: 0.4954, F1: 0.4952\n",
      "2025-11-12 07:53:06,362 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_2_f1_0.5262.pt (F1: 0.5262, Fold: 3, Epoch: 2)\n",
      "2025-11-12 07:53:06,362 - INFO - \n",
      "Epoch 4/10\n",
      "2025-11-12 07:53:06,363 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.73it/s, loss=0.232]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.62it/s]\n",
      "2025-11-12 07:53:09,485 - INFO - Train Loss: 0.2255\n",
      "2025-11-12 07:53:09,486 - INFO - Val Loss: 0.1404\n",
      "2025-11-12 07:53:09,486 - INFO - Overall - Precision: 0.5558, Recall: 0.5646, F1: 0.5590\n",
      "2025-11-12 07:53:09,486 - INFO - en - Precision: 0.5591, Recall: 0.5224, F1: 0.5251\n",
      "2025-11-12 07:53:09,486 - INFO - es - Precision: 0.5785, Recall: 0.5632, F1: 0.5685\n",
      "2025-11-12 07:53:09,486 - INFO - it - Precision: 0.5236, Recall: 0.5345, F1: 0.5109\n",
      "2025-11-12 07:53:10,121 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_3_f1_0.5590.pt (F1: 0.5590, Fold: 3, Epoch: 3)\n",
      "2025-11-12 07:53:10,121 - INFO - \n",
      "Epoch 5/10\n",
      "2025-11-12 07:53:10,122 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.77it/s, loss=0.212]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.60it/s]\n",
      "2025-11-12 07:53:13,243 - INFO - Train Loss: 0.2054\n",
      "2025-11-12 07:53:13,243 - INFO - Val Loss: 0.1508\n",
      "2025-11-12 07:53:13,243 - INFO - Overall - Precision: 0.5908, Recall: 0.6492, F1: 0.5947\n",
      "2025-11-12 07:53:13,243 - INFO - en - Precision: 0.5591, Recall: 0.5224, F1: 0.5251\n",
      "2025-11-12 07:53:13,243 - INFO - es - Precision: 0.5924, Recall: 0.6467, F1: 0.5958\n",
      "2025-11-12 07:53:13,243 - INFO - it - Precision: 0.5831, Recall: 0.6319, F1: 0.5318\n",
      "2025-11-12 07:53:13,878 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_4_f1_0.5947.pt (F1: 0.5947, Fold: 3, Epoch: 4)\n",
      "2025-11-12 07:53:13,878 - INFO - \n",
      "Epoch 6/10\n",
      "2025-11-12 07:53:13,879 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.13it/s, loss=0.195]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.53it/s]\n",
      "2025-11-12 07:53:17,032 - INFO - Train Loss: 0.1892\n",
      "2025-11-12 07:53:17,032 - INFO - Val Loss: 0.1412\n",
      "2025-11-12 07:53:17,032 - INFO - Overall - Precision: 0.6188, Recall: 0.6865, F1: 0.6308\n",
      "2025-11-12 07:53:17,032 - INFO - en - Precision: 0.5844, Recall: 0.5257, F1: 0.5297\n",
      "2025-11-12 07:53:17,032 - INFO - es - Precision: 0.5939, Recall: 0.6568, F1: 0.5930\n",
      "2025-11-12 07:53:17,032 - INFO - it - Precision: 0.6366, Recall: 0.7149, F1: 0.6182\n",
      "2025-11-12 07:53:17,692 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_5_f1_0.6308.pt (F1: 0.6308, Fold: 3, Epoch: 5)\n",
      "2025-11-12 07:53:17,693 - INFO - \n",
      "Epoch 7/10\n",
      "2025-11-12 07:53:17,694 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.43it/s, loss=0.202]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.20it/s]\n",
      "2025-11-12 07:53:20,840 - INFO - Train Loss: 0.1957\n",
      "2025-11-12 07:53:20,840 - INFO - Val Loss: 0.1558\n",
      "2025-11-12 07:53:20,840 - INFO - Overall - Precision: 0.6327, Recall: 0.7312, F1: 0.6405\n",
      "2025-11-12 07:53:20,840 - INFO - en - Precision: 0.6046, Recall: 0.5548, F1: 0.5676\n",
      "2025-11-12 07:53:20,840 - INFO - es - Precision: 0.6255, Recall: 0.7267, F1: 0.6159\n",
      "2025-11-12 07:53:20,840 - INFO - it - Precision: 0.6523, Recall: 0.7416, F1: 0.6067\n",
      "2025-11-12 07:53:21,482 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_6_f1_0.6405.pt (F1: 0.6405, Fold: 3, Epoch: 6)\n",
      "2025-11-12 07:53:21,483 - INFO - \n",
      "Epoch 8/10\n",
      "2025-11-12 07:53:21,484 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.69it/s, loss=0.173]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 159.91it/s]\n",
      "2025-11-12 07:53:24,621 - INFO - Train Loss: 0.1677\n",
      "2025-11-12 07:53:24,621 - INFO - Val Loss: 0.1369\n",
      "2025-11-12 07:53:24,621 - INFO - Overall - Precision: 0.6785, Recall: 0.7563, F1: 0.7015\n",
      "2025-11-12 07:53:24,621 - INFO - en - Precision: 0.6623, Recall: 0.5614, F1: 0.5810\n",
      "2025-11-12 07:53:24,621 - INFO - es - Precision: 0.6255, Recall: 0.7267, F1: 0.6159\n",
      "2025-11-12 07:53:24,621 - INFO - it - Precision: 0.7446, Recall: 0.8261, F1: 0.7683\n",
      "2025-11-12 07:53:25,259 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_7_f1_0.7015.pt (F1: 0.7015, Fold: 3, Epoch: 7)\n",
      "2025-11-12 07:53:25,260 - INFO - \n",
      "Epoch 9/10\n",
      "2025-11-12 07:53:25,261 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.74it/s, loss=0.176]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.71it/s]\n",
      "2025-11-12 07:53:28,383 - INFO - Train Loss: 0.1707\n",
      "2025-11-12 07:53:28,383 - INFO - Val Loss: 0.1416\n",
      "2025-11-12 07:53:28,383 - INFO - Overall - Precision: 0.6694, Recall: 0.7548, F1: 0.6914\n",
      "2025-11-12 07:53:28,383 - INFO - en - Precision: 0.5422, Recall: 0.5190, F1: 0.5208\n",
      "2025-11-12 07:53:28,384 - INFO - es - Precision: 0.6194, Recall: 0.7183, F1: 0.6044\n",
      "2025-11-12 07:53:28,384 - INFO - it - Precision: 0.7461, Recall: 0.8447, F1: 0.7694\n",
      "2025-11-12 07:53:28,384 - INFO - \n",
      "Epoch 10/10\n",
      "2025-11-12 07:53:28,385 - INFO - Undersampling: Minority=273, Majority=546\n",
      "Training: 100%|██████████| 137/137 [00:02<00:00, 57.62it/s, loss=0.167]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 155.54it/s]\n",
      "2025-11-12 07:53:31,545 - INFO - Train Loss: 0.1624\n",
      "2025-11-12 07:53:31,546 - INFO - Val Loss: 0.1295\n",
      "2025-11-12 07:53:31,546 - INFO - Overall - Precision: 0.6825, Recall: 0.7587, F1: 0.7058\n",
      "2025-11-12 07:53:31,546 - INFO - en - Precision: 0.5422, Recall: 0.5190, F1: 0.5208\n",
      "2025-11-12 07:53:31,546 - INFO - es - Precision: 0.6320, Recall: 0.7351, F1: 0.6276\n",
      "2025-11-12 07:53:31,546 - INFO - it - Precision: 0.7640, Recall: 0.8479, F1: 0.7898\n",
      "2025-11-12 07:53:32,201 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_3_epoch_9_f1_0.7058.pt (F1: 0.7058, Fold: 3, Epoch: 9)\n",
      "2025-11-12 07:53:32,201 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:53:32,202 - INFO - Fold 5/5\n",
      "2025-11-12 07:53:32,202 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 4:\n",
      "  Train: 272 positive samples\n",
      "  Val:   70 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-12 07:53:35,307 - INFO - Froze: Embeddings + First 6 Encoder Layers\n",
      "2025-11-12 07:53:35,307 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-11-12 07:53:35,308 - INFO - Trainable parameters: 43,119,362 / 278,045,186 (15.51%)\n",
      "2025-11-12 07:53:35,309 - INFO - Label weights: {0: 0.5829268292682926, 1: 3.514705882352941}\n",
      "2025-11-12 07:53:35,309 - INFO - Language weights: {'it': 0.9093912592122662, 'en': 0.961989231586796, 'es': 1.1286195092009375}\n",
      "2025-11-12 07:53:35,309 - INFO - Pos weight (for BCE): 6.0294\n",
      "2025-11-12 07:53:35,310 - INFO - \n",
      "Epoch 1/10\n",
      "2025-11-12 07:53:35,311 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.00it/s, loss=0.352]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.49it/s]\n",
      "2025-11-12 07:53:38,460 - INFO - Train Loss: 0.3438\n",
      "2025-11-12 07:53:38,460 - INFO - Val Loss: 0.1180\n",
      "2025-11-12 07:53:38,460 - INFO - Overall - Precision: 0.4269, Recall: 0.5000, F1: 0.4606\n",
      "2025-11-12 07:53:38,460 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:53:38,460 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:53:38,461 - INFO - it - Precision: 0.4023, Recall: 0.5000, F1: 0.4459\n",
      "2025-11-12 07:53:39,112 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_0_f1_0.4606.pt (F1: 0.4606, Fold: 4, Epoch: 0)\n",
      "2025-11-12 07:53:39,112 - INFO - \n",
      "Epoch 2/10\n",
      "2025-11-12 07:53:39,113 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.22it/s, loss=0.311]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.31it/s]\n",
      "2025-11-12 07:53:42,246 - INFO - Train Loss: 0.3037\n",
      "2025-11-12 07:53:42,246 - INFO - Val Loss: 0.1135\n",
      "2025-11-12 07:53:42,246 - INFO - Overall - Precision: 0.4266, Recall: 0.4976, F1: 0.4594\n",
      "2025-11-12 07:53:42,246 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:53:42,246 - INFO - es - Precision: 0.4220, Recall: 0.5000, F1: 0.4577\n",
      "2025-11-12 07:53:42,246 - INFO - it - Precision: 0.4012, Recall: 0.4929, F1: 0.4423\n",
      "2025-11-12 07:53:42,246 - INFO - \n",
      "Epoch 3/10\n",
      "2025-11-12 07:53:42,247 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.62it/s, loss=0.259]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.08it/s]\n",
      "2025-11-12 07:53:45,369 - INFO - Train Loss: 0.2530\n",
      "2025-11-12 07:53:45,369 - INFO - Val Loss: 0.1281\n",
      "2025-11-12 07:53:45,369 - INFO - Overall - Precision: 0.6378, Recall: 0.5484, F1: 0.5551\n",
      "2025-11-12 07:53:45,369 - INFO - en - Precision: 0.4573, Recall: 0.5000, F1: 0.4777\n",
      "2025-11-12 07:53:45,369 - INFO - es - Precision: 0.7486, Recall: 0.6010, F1: 0.6270\n",
      "2025-11-12 07:53:45,370 - INFO - it - Precision: 0.5491, Recall: 0.5231, F1: 0.5167\n",
      "2025-11-12 07:53:46,009 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_2_f1_0.5551.pt (F1: 0.5551, Fold: 4, Epoch: 2)\n",
      "2025-11-12 07:53:46,010 - INFO - \n",
      "Epoch 4/10\n",
      "2025-11-12 07:53:46,011 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.53it/s, loss=0.217]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 158.31it/s]\n",
      "2025-11-12 07:53:49,145 - INFO - Train Loss: 0.2126\n",
      "2025-11-12 07:53:49,145 - INFO - Val Loss: 0.1664\n",
      "2025-11-12 07:53:49,145 - INFO - Overall - Precision: 0.5287, Recall: 0.5496, F1: 0.5148\n",
      "2025-11-12 07:53:49,146 - INFO - en - Precision: 0.4557, Recall: 0.4800, F1: 0.4675\n",
      "2025-11-12 07:53:49,146 - INFO - es - Precision: 0.5397, Recall: 0.5676, F1: 0.5249\n",
      "2025-11-12 07:53:49,146 - INFO - it - Precision: 0.4889, Recall: 0.4826, F1: 0.4185\n",
      "2025-11-12 07:53:49,146 - INFO - \n",
      "Epoch 5/10\n",
      "2025-11-12 07:53:49,147 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.75it/s, loss=0.215]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.83it/s]\n",
      "2025-11-12 07:53:52,254 - INFO - Train Loss: 0.2102\n",
      "2025-11-12 07:53:52,255 - INFO - Val Loss: 0.1680\n",
      "2025-11-12 07:53:52,255 - INFO - Overall - Precision: 0.5579, Recall: 0.6054, F1: 0.5431\n",
      "2025-11-12 07:53:52,255 - INFO - en - Precision: 0.4560, Recall: 0.4833, F1: 0.4693\n",
      "2025-11-12 07:53:52,255 - INFO - es - Precision: 0.5481, Recall: 0.5913, F1: 0.4941\n",
      "2025-11-12 07:53:52,255 - INFO - it - Precision: 0.5536, Recall: 0.5847, F1: 0.4933\n",
      "2025-11-12 07:53:52,255 - INFO - \n",
      "Epoch 6/10\n",
      "2025-11-12 07:53:52,256 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.77it/s, loss=0.219]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 162.05it/s]\n",
      "2025-11-12 07:53:55,362 - INFO - Train Loss: 0.2146\n",
      "2025-11-12 07:53:55,362 - INFO - Val Loss: 0.1420\n",
      "2025-11-12 07:53:55,363 - INFO - Overall - Precision: 0.6225, Recall: 0.6703, F1: 0.6362\n",
      "2025-11-12 07:53:55,363 - INFO - en - Precision: 0.4568, Recall: 0.4933, F1: 0.4744\n",
      "2025-11-12 07:53:55,363 - INFO - es - Precision: 0.6274, Recall: 0.6660, F1: 0.6399\n",
      "2025-11-12 07:53:55,363 - INFO - it - Precision: 0.6239, Recall: 0.6926, F1: 0.6106\n",
      "2025-11-12 07:53:56,000 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_5_f1_0.6362.pt (F1: 0.6362, Fold: 4, Epoch: 5)\n",
      "2025-11-12 07:53:56,000 - INFO - \n",
      "Epoch 7/10\n",
      "2025-11-12 07:53:56,001 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 56.34it/s, loss=0.186]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 160.83it/s]\n",
      "2025-11-12 07:53:59,173 - INFO - Train Loss: 0.1814\n",
      "2025-11-12 07:53:59,173 - INFO - Val Loss: 0.1770\n",
      "2025-11-12 07:53:59,173 - INFO - Overall - Precision: 0.6094, Recall: 0.6986, F1: 0.6064\n",
      "2025-11-12 07:53:59,173 - INFO - en - Precision: 0.4560, Recall: 0.4833, F1: 0.4693\n",
      "2025-11-12 07:53:59,173 - INFO - es - Precision: 0.5885, Recall: 0.6662, F1: 0.5537\n",
      "2025-11-12 07:53:59,173 - INFO - it - Precision: 0.6501, Recall: 0.7349, F1: 0.5850\n",
      "2025-11-12 07:53:59,173 - INFO - \n",
      "Epoch 8/10\n",
      "2025-11-12 07:53:59,174 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.82it/s, loss=0.167]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 161.24it/s]\n",
      "2025-11-12 07:54:02,282 - INFO - Train Loss: 0.1632\n",
      "2025-11-12 07:54:02,282 - INFO - Val Loss: 0.1718\n",
      "2025-11-12 07:54:02,283 - INFO - Overall - Precision: 0.6326, Recall: 0.7243, F1: 0.6432\n",
      "2025-11-12 07:54:02,283 - INFO - en - Precision: 0.4557, Recall: 0.4800, F1: 0.4675\n",
      "2025-11-12 07:54:02,283 - INFO - es - Precision: 0.6121, Recall: 0.7116, F1: 0.5761\n",
      "2025-11-12 07:54:02,283 - INFO - it - Precision: 0.6827, Recall: 0.7840, F1: 0.6778\n",
      "2025-11-12 07:54:02,917 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_7_f1_0.6432.pt (F1: 0.6432, Fold: 4, Epoch: 7)\n",
      "2025-11-12 07:54:02,917 - INFO - \n",
      "Epoch 9/10\n",
      "2025-11-12 07:54:02,918 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.61it/s, loss=0.171]\n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 157.45it/s]\n",
      "2025-11-12 07:54:06,052 - INFO - Train Loss: 0.1668\n",
      "2025-11-12 07:54:06,053 - INFO - Val Loss: 0.1854\n",
      "2025-11-12 07:54:06,053 - INFO - Overall - Precision: 0.6338, Recall: 0.7255, F1: 0.6450\n",
      "2025-11-12 07:54:06,053 - INFO - en - Precision: 0.4557, Recall: 0.4800, F1: 0.4675\n",
      "2025-11-12 07:54:06,053 - INFO - es - Precision: 0.5918, Recall: 0.6738, F1: 0.5262\n",
      "2025-11-12 07:54:06,053 - INFO - it - Precision: 0.7162, Recall: 0.8197, F1: 0.7301\n",
      "2025-11-12 07:54:06,691 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_8_f1_0.6450.pt (F1: 0.6450, Fold: 4, Epoch: 8)\n",
      "2025-11-12 07:54:06,691 - INFO - \n",
      "Epoch 10/10\n",
      "2025-11-12 07:54:06,693 - INFO - Undersampling: Minority=272, Majority=544\n",
      "Training: 100%|██████████| 136/136 [00:02<00:00, 57.60it/s, loss=0.17] \n",
      "Validating: 100%|██████████| 120/120 [00:00<00:00, 159.94it/s]\n",
      "2025-11-12 07:54:09,815 - INFO - Train Loss: 0.1658\n",
      "2025-11-12 07:54:09,816 - INFO - Val Loss: 0.1642\n",
      "2025-11-12 07:54:09,816 - INFO - Overall - Precision: 0.6403, Recall: 0.7316, F1: 0.6542\n",
      "2025-11-12 07:54:09,816 - INFO - en - Precision: 0.5154, Recall: 0.5271, F1: 0.5122\n",
      "2025-11-12 07:54:09,816 - INFO - es - Precision: 0.5957, Recall: 0.6728, F1: 0.5802\n",
      "2025-11-12 07:54:09,816 - INFO - it - Precision: 0.7461, Recall: 0.8447, F1: 0.7694\n",
      "2025-11-12 07:54:10,453 - INFO - Saved checkpoint: ../fine_tuned_models/checkpoints/fold_4_epoch_9_f1_0.6542.pt (F1: 0.6542, Fold: 4, Epoch: 9)\n",
      "2025-11-12 07:54:10,455 - INFO - \n",
      "Results saved to: ../results/roberta-fine-tune/training_results.csv\n",
      "2025-11-12 07:54:10,456 - INFO - \n",
      "================================================================================\n",
      "2025-11-12 07:54:10,456 - INFO - Best Models Saved:\n",
      "2025-11-12 07:54:10,456 - INFO - ================================================================================\n",
      "2025-11-12 07:54:10,456 - INFO - Fold 2, Epoch 8: F1=0.7352 -> ../fine_tuned_models/checkpoints/fold_2_epoch_8_f1_0.7352.pt\n",
      "2025-11-12 07:54:10,456 - INFO - Fold 2, Epoch 7: F1=0.7281 -> ../fine_tuned_models/checkpoints/fold_2_epoch_7_f1_0.7281.pt\n"
     ]
    }
   ],
   "source": [
    "main(train_df, Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441cdbe3-5836-4a17-901e-ed0a869d03d3",
   "metadata": {},
   "source": [
    "# Visualizing Fine-Tune Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e26ae0-edb4-45cb-8cad-3180deb6684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29fc7190-5d13-4a71-8ca8-58f1b300296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/roberta-fine-tune/training_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bb34b6-16a0-4c2f-90f6-18480f987d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-af90ec7c38fa43599186b7ac58f40afb.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-af90ec7c38fa43599186b7ac58f40afb.vega-embed details,\n",
       "  #altair-viz-af90ec7c38fa43599186b7ac58f40afb.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-af90ec7c38fa43599186b7ac58f40afb\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-af90ec7c38fa43599186b7ac58f40afb\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-af90ec7c38fa43599186b7ac58f40afb\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-6513e40d73c393fd13fdf742646c57f2\"}, \"mark\": {\"type\": \"line\", \"point\": true, \"size\": 3}, \"encoding\": {\"color\": {\"field\": \"fold\", \"title\": \"Fold\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"fold\", \"type\": \"nominal\"}, {\"field\": \"epoch\", \"type\": \"quantitative\"}, {\"field\": \"overall_macro_f1\", \"format\": \".4f\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"epoch\", \"title\": \"Epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"overall_macro_f1\", \"scale\": {\"domain\": [0.3, 0.8]}, \"title\": \"Macro F1 Score\", \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"Overall F1 Score by Fold\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-6513e40d73c393fd13fdf742646c57f2\": [{\"fold\": 0, \"epoch\": 0, \"train_loss\": 0.3267235863872253, \"val_loss\": 0.115067987019817, \"overall_macro_precision\": 0.4288702928870292, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4617117117117117, \"it_macro_precision\": 0.4046242774566474, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.4472843450479233, \"en_macro_precision\": 0.4575757575757576, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4778481012658228, \"es_macro_precision\": 0.425, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4594594594594595}, {\"fold\": 0, \"epoch\": 1, \"train_loss\": 0.3086312822317772, \"val_loss\": 0.1100468893845876, \"overall_macro_precision\": 0.4288702928870292, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4617117117117117, \"it_macro_precision\": 0.4046242774566474, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.4472843450479233, \"en_macro_precision\": 0.4575757575757576, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4778481012658228, \"es_macro_precision\": 0.425, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4594594594594595}, {\"fold\": 0, \"epoch\": 2, \"train_loss\": 0.251913471405741, \"val_loss\": 0.1252351569632689, \"overall_macro_precision\": 0.5304635761589404, \"overall_macro_recall\": 0.5123744619799139, \"overall_macro_f1\": 0.5056753759702961, \"it_macro_precision\": 0.4519822282980177, \"it_macro_recall\": 0.4695887445887445, \"it_macro_f1\": 0.4568288854003139, \"en_macro_precision\": 0.4575757575757576, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4778481012658228, \"es_macro_precision\": 0.6828358208955223, \"es_macro_recall\": 0.5588235294117647, \"es_macro_f1\": 0.5696091348265262}, {\"fold\": 0, \"epoch\": 3, \"train_loss\": 0.2367185081226112, \"val_loss\": 0.1334751188134153, \"overall_macro_precision\": 0.5975020678246484, \"overall_macro_recall\": 0.6057030129124821, \"overall_macro_f1\": 0.6011835642831953, \"it_macro_precision\": 0.5423553719008265, \"it_macro_recall\": 0.5576839826839827, \"it_macro_f1\": 0.5399143565472166, \"en_macro_precision\": 0.9603658536585368, \"en_macro_recall\": 0.5357142857142857, \"en_macro_f1\": 0.5460317460317461, \"es_macro_precision\": 0.653697996918336, \"es_macro_recall\": 0.6596638655462185, \"es_macro_f1\": 0.6565597095476401}, {\"fold\": 0, \"epoch\": 4, \"train_loss\": 0.2107491371677304, \"val_loss\": 0.1368804833541314, \"overall_macro_precision\": 0.6024195103142471, \"overall_macro_recall\": 0.6524390243902439, \"overall_macro_f1\": 0.6131697759604736, \"it_macro_precision\": 0.5807298680312416, \"it_macro_recall\": 0.6297619047619047, \"it_macro_f1\": 0.5511294261294262, \"en_macro_precision\": 0.9603658536585368, \"en_macro_recall\": 0.5357142857142857, \"en_macro_f1\": 0.5460317460317461, \"es_macro_precision\": 0.6145948945615982, \"es_macro_recall\": 0.6652661064425771, \"es_macro_f1\": 0.6266666666666667}, {\"fold\": 0, \"epoch\": 5, \"train_loss\": 0.1960989370615813, \"val_loss\": 0.1525283996636668, \"overall_macro_precision\": 0.6024964838255977, \"overall_macro_recall\": 0.6881994261119082, \"overall_macro_f1\": 0.5992334411306743, \"it_macro_precision\": 0.5899122807017544, \"it_macro_recall\": 0.6286796536796537, \"it_macro_f1\": 0.4636664054781453, \"en_macro_precision\": 0.6265432098765432, \"en_macro_recall\": 0.5290917691579944, \"en_macro_f1\": 0.5348618680699117, \"es_macro_precision\": 0.6434907695996164, \"es_macro_recall\": 0.7394957983193278, \"es_macro_f1\": 0.6556712962962963}, {\"fold\": 0, \"epoch\": 6, \"train_loss\": 0.1916557476740248, \"val_loss\": 0.1502745484933257, \"overall_macro_precision\": 0.6344802152030545, \"overall_macro_recall\": 0.7223457675753229, \"overall_macro_f1\": 0.648501720198545, \"it_macro_precision\": 0.6696919691969196, \"it_macro_recall\": 0.7670995670995671, \"it_macro_f1\": 0.6650069156293222, \"en_macro_precision\": 0.5846273291925466, \"en_macro_recall\": 0.5257805108798487, \"en_macro_f1\": 0.5299145299145299, \"es_macro_precision\": 0.6074432296047099, \"es_macro_recall\": 0.7044817927170868, \"es_macro_f1\": 0.5856162226840481}, {\"fold\": 0, \"epoch\": 7, \"train_loss\": 0.1804737198798761, \"val_loss\": 0.1403825618171443, \"overall_macro_precision\": 0.6674558080808081, \"overall_macro_recall\": 0.7663916786226685, \"overall_macro_f1\": 0.6895851936033769, \"it_macro_precision\": 0.7367256637168141, \"it_macro_recall\": 0.8474025974025974, \"it_macro_f1\": 0.7573632538569425, \"en_macro_precision\": 0.5726495726495726, \"en_macro_recall\": 0.5482497634815515, \"en_macro_f1\": 0.5560118963319642, \"es_macro_precision\": 0.6250264214753751, \"es_macro_recall\": 0.7366946778711485, \"es_macro_f1\": 0.609037826859609}, {\"fold\": 0, \"epoch\": 8, \"train_loss\": 0.1856320846390767, \"val_loss\": 0.1020865689963102, \"overall_macro_precision\": 0.7011887164326189, \"overall_macro_recall\": 0.7343256814921091, \"overall_macro_f1\": 0.7153349875930521, \"it_macro_precision\": 0.7488179669030732, \"it_macro_recall\": 0.8189393939393939, \"it_macro_f1\": 0.7723684210526316, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.4966887417218543, \"en_macro_f1\": 0.4761904761904761, \"es_macro_precision\": 0.6534406215316315, \"es_macro_recall\": 0.7212885154061625, \"es_macro_f1\": 0.6719191919191919}, {\"fold\": 0, \"epoch\": 9, \"train_loss\": 0.1770366205439569, \"val_loss\": 0.1271016285444299, \"overall_macro_precision\": 0.6948351621376144, \"overall_macro_recall\": 0.7846843615494978, \"overall_macro_f1\": 0.7216194646362244, \"it_macro_precision\": 0.7371445856019359, \"it_macro_recall\": 0.8393939393939394, \"it_macro_f1\": 0.7597222222222222, \"en_macro_precision\": 0.6209514170040487, \"en_macro_recall\": 0.6130558183538316, \"en_macro_f1\": 0.6167950128346168, \"es_macro_precision\": 0.669869425966987, \"es_macro_recall\": 0.7759103641456582, \"es_macro_f1\": 0.689257176679491}, {\"fold\": 1, \"epoch\": 0, \"train_loss\": 0.3409668991790853, \"val_loss\": 0.1158103788892428, \"overall_macro_precision\": 0.4278242677824267, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4611048478015783, \"it_macro_precision\": 0.4046242774566474, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.4472843450479233, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 1, \"epoch\": 1, \"train_loss\": 0.2798350875666976, \"val_loss\": 0.1119360426751275, \"overall_macro_precision\": 0.6785714285714286, \"overall_macro_recall\": 0.5060238829240636, \"overall_macro_f1\": 0.4751014561947959, \"it_macro_precision\": 0.6564327485380117, \"it_macro_recall\": 0.5115800865800866, \"it_macro_f1\": 0.4755167661920073, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 1, \"epoch\": 2, \"train_loss\": 0.2553854051188831, \"val_loss\": 0.1342812413970629, \"overall_macro_precision\": 0.5689479060265576, \"overall_macro_recall\": 0.5358775380036143, \"overall_macro_f1\": 0.5390549662487946, \"it_macro_precision\": 0.5209211318482841, \"it_macro_recall\": 0.5150432900432901, \"it_macro_f1\": 0.5135895032802249, \"en_macro_precision\": 0.4565217391304347, \"en_macro_recall\": 0.49, \"en_macro_f1\": 0.472668810289389, \"es_macro_precision\": 0.6823308270676691, \"es_macro_recall\": 0.5741023682200153, \"es_macro_f1\": 0.5896825396825397}, {\"fold\": 1, \"epoch\": 3, \"train_loss\": 0.2244763397713647, \"val_loss\": 0.1474877723803122, \"overall_macro_precision\": 0.5654069385718412, \"overall_macro_recall\": 0.5882498848375324, \"overall_macro_f1\": 0.5707229456668164, \"it_macro_precision\": 0.5201078582434515, \"it_macro_recall\": 0.5282467532467532, \"it_macro_f1\": 0.5123326286116984, \"en_macro_precision\": 0.45625, \"en_macro_recall\": 0.4866666666666667, \"en_macro_f1\": 0.4709677419354838, \"es_macro_precision\": 0.6093073593073592, \"es_macro_recall\": 0.6736058059587471, \"es_macro_f1\": 0.6159690366972477}, {\"fold\": 1, \"epoch\": 4, \"train_loss\": 0.2067945076383813, \"val_loss\": 0.1657989844679832, \"overall_macro_precision\": 0.6034266923263766, \"overall_macro_recall\": 0.6961482583891428, \"overall_macro_f1\": 0.5916484417714598, \"it_macro_precision\": 0.606941706561532, \"it_macro_recall\": 0.6643939393939394, \"it_macro_f1\": 0.5129809196121364, \"en_macro_precision\": 0.45625, \"en_macro_recall\": 0.4866666666666667, \"en_macro_f1\": 0.4709677419354838, \"es_macro_precision\": 0.6310386473429952, \"es_macro_recall\": 0.7486631016042781, \"es_macro_f1\": 0.5862723663770784}, {\"fold\": 1, \"epoch\": 5, \"train_loss\": 0.1941540304598582, \"val_loss\": 0.1443751791492104, \"overall_macro_precision\": 0.6303534571723426, \"overall_macro_recall\": 0.714840012756458, \"overall_macro_f1\": 0.6429476145626969, \"it_macro_precision\": 0.6440746753246753, \"it_macro_recall\": 0.7305194805194806, \"it_macro_f1\": 0.6268104776579353, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.6340852130325814, \"es_macro_recall\": 0.7452253628724217, \"es_macro_f1\": 0.6219367712165618}, {\"fold\": 1, \"epoch\": 6, \"train_loss\": 0.1794798510679363, \"val_loss\": 0.1520822898174325, \"overall_macro_precision\": 0.6559605517425202, \"overall_macro_recall\": 0.7488040820665463, \"overall_macro_f1\": 0.6751419205245063, \"it_macro_precision\": 0.7154304029304029, \"it_macro_recall\": 0.8055194805194805, \"it_macro_f1\": 0.7352774013028462, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.6407444668008049, \"es_macro_recall\": 0.767188693659282, \"es_macro_f1\": 0.5908858166922683}, {\"fold\": 1, \"epoch\": 7, \"train_loss\": 0.1656912503913588, \"val_loss\": 0.1831698179865876, \"overall_macro_precision\": 0.6518325162878547, \"overall_macro_recall\": 0.7535168845894902, \"overall_macro_f1\": 0.6681921020156314, \"it_macro_precision\": 0.7282188522152839, \"it_macro_recall\": 0.8322510822510822, \"it_macro_f1\": 0.7482882574460801, \"en_macro_precision\": 0.4556962025316455, \"en_macro_recall\": 0.48, \"en_macro_f1\": 0.4675324675324675, \"es_macro_precision\": 0.6344292053247277, \"es_macro_recall\": 0.7545836516424752, \"es_macro_f1\": 0.5735887096774194}, {\"fold\": 1, \"epoch\": 8, \"train_loss\": 0.1706827061495533, \"val_loss\": 0.1630900113843381, \"overall_macro_precision\": 0.6634621730426558, \"overall_macro_recall\": 0.7632968356897346, \"overall_macro_f1\": 0.6838624338624338, \"it_macro_precision\": 0.7282188522152839, \"it_macro_recall\": 0.8322510822510822, \"it_macro_f1\": 0.7482882574460801, \"en_macro_precision\": 0.4556962025316455, \"en_macro_recall\": 0.48, \"en_macro_f1\": 0.4675324675324675, \"es_macro_precision\": 0.6524242424242425, \"es_macro_recall\": 0.7881970970206265, \"es_macro_f1\": 0.6200796626054358}, {\"fold\": 1, \"epoch\": 9, \"train_loss\": 0.1748654276173371, \"val_loss\": 0.1643215182237327, \"overall_macro_precision\": 0.6681696428571429, \"overall_macro_recall\": 0.7669643173523263, \"overall_macro_f1\": 0.6899072383512904, \"it_macro_precision\": 0.7623172282263191, \"it_macro_recall\": 0.8572510822510823, \"it_macro_f1\": 0.7894523326572008, \"en_macro_precision\": 0.4539473684210526, \"en_macro_recall\": 0.46, \"en_macro_f1\": 0.4569536423841059, \"es_macro_precision\": 0.6575689935064934, \"es_macro_recall\": 0.7966004583651642, \"es_macro_f1\": 0.6319411485524442}, {\"fold\": 2, \"epoch\": 0, \"train_loss\": 0.3337063257834011, \"val_loss\": 0.1137847064218173, \"overall_macro_precision\": 0.4278242677824267, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4611048478015783, \"it_macro_precision\": 0.4046242774566474, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.4472843450479233, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 2, \"epoch\": 1, \"train_loss\": 0.3156460058759816, \"val_loss\": 0.109596178214997, \"overall_macro_precision\": 0.4275210084033613, \"overall_macro_recall\": 0.4975550122249388, \"overall_macro_f1\": 0.4598870056497175, \"it_macro_precision\": 0.4035087719298245, \"it_macro_recall\": 0.4928571428571429, \"it_macro_f1\": 0.4437299035369775, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 2, \"epoch\": 2, \"train_loss\": 0.2434504786982153, \"val_loss\": 0.13116221036762, \"overall_macro_precision\": 0.584931506849315, \"overall_macro_recall\": 0.5527266928882747, \"overall_macro_f1\": 0.559914647487625, \"it_macro_precision\": 0.5555186170212766, \"it_macro_recall\": 0.5542207792207792, \"it_macro_f1\": 0.5548316452231044, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.6796296296296296, \"es_macro_recall\": 0.5555767761650114, \"es_macro_f1\": 0.5638357705286839}, {\"fold\": 2, \"epoch\": 3, \"train_loss\": 0.2368379923222708, \"val_loss\": 0.1476562536011139, \"overall_macro_precision\": 0.5595400646784047, \"overall_macro_recall\": 0.588072711810354, \"overall_macro_f1\": 0.5625422392430728, \"it_macro_precision\": 0.5349785981808454, \"it_macro_recall\": 0.5566017316017315, \"it_macro_f1\": 0.4808987119437938, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.5817307692307693, \"es_macro_recall\": 0.5876623376623377, \"es_macro_f1\": 0.5843773028739867}, {\"fold\": 2, \"epoch\": 4, \"train_loss\": 0.2198796592598414, \"val_loss\": 0.1436182119573156, \"overall_macro_precision\": 0.6121482683982684, \"overall_macro_recall\": 0.676251727437015, \"overall_macro_f1\": 0.6231274638633377, \"it_macro_precision\": 0.6273229194721249, \"it_macro_recall\": 0.7046536796536796, \"it_macro_f1\": 0.603937728937729, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.5975694444444444, \"es_macro_recall\": 0.6610007639419404, \"es_macro_f1\": 0.5986810135369663}, {\"fold\": 2, \"epoch\": 5, \"train_loss\": 0.1918858456676893, \"val_loss\": 0.1569367301339904, \"overall_macro_precision\": 0.6221842473201802, \"overall_macro_recall\": 0.7229545374012261, \"overall_macro_f1\": 0.6226144046919428, \"it_macro_precision\": 0.6527777777777778, \"it_macro_recall\": 0.7321428571428572, \"it_macro_f1\": 0.5511157239231967, \"en_macro_precision\": 0.4570552147239264, \"en_macro_recall\": 0.4966666666666666, \"en_macro_f1\": 0.476038338658147, \"es_macro_precision\": 0.625475687103594, \"es_macro_recall\": 0.7266997708174179, \"es_macro_f1\": 0.615901171998733}, {\"fold\": 2, \"epoch\": 6, \"train_loss\": 0.1800779037068795, \"val_loss\": 0.1558277137887974, \"overall_macro_precision\": 0.6401557959315467, \"overall_macro_recall\": 0.7460933347507175, \"overall_macro_f1\": 0.6498168498168498, \"it_macro_precision\": 0.6875, \"it_macro_recall\": 0.8035714285714286, \"it_macro_f1\": 0.6505050505050505, \"en_macro_precision\": 0.4570552147239264, \"en_macro_recall\": 0.4966666666666666, \"en_macro_f1\": 0.476038338658147, \"es_macro_precision\": 0.6172161172161172, \"es_macro_recall\": 0.7200152788388083, \"es_macro_f1\": 0.5873992236488504}, {\"fold\": 2, \"epoch\": 7, \"train_loss\": 0.191548400619713, \"val_loss\": 0.1199327289747695, \"overall_macro_precision\": 0.7034955484938414, \"overall_macro_recall\": 0.7745650402182771, \"overall_macro_f1\": 0.7281245322556504, \"it_macro_precision\": 0.7786336756924992, \"it_macro_recall\": 0.8875541125541127, \"it_macro_f1\": 0.8080592908179115, \"en_macro_precision\": 0.4570552147239264, \"en_macro_recall\": 0.4966666666666666, \"en_macro_f1\": 0.476038338658147, \"es_macro_precision\": 0.6423340961098398, \"es_macro_recall\": 0.7375859434682964, \"es_macro_f1\": 0.6511819681143485}, {\"fold\": 2, \"epoch\": 8, \"train_loss\": 0.1819014149126562, \"val_loss\": 0.1202161794838806, \"overall_macro_precision\": 0.7108485499462943, \"overall_macro_recall\": 0.7782325218808689, \"overall_macro_f1\": 0.7351760357611772, \"it_macro_precision\": 0.8082620144832127, \"it_macro_recall\": 0.9054112554112554, \"it_macro_f1\": 0.8401699926090169, \"en_macro_precision\": 0.4570552147239264, \"en_macro_recall\": 0.4966666666666666, \"en_macro_f1\": 0.476038338658147, \"es_macro_precision\": 0.6344086021505376, \"es_macro_recall\": 0.7291825821237585, \"es_macro_f1\": 0.6389487870619946}, {\"fold\": 2, \"epoch\": 9, \"train_loss\": 0.1655476424897456, \"val_loss\": 0.1316004556603729, \"overall_macro_precision\": 0.699339666642119, \"overall_macro_recall\": 0.7877467134403459, \"overall_macro_f1\": 0.7262313860252005, \"it_macro_precision\": 0.7946428571428572, \"it_macro_recall\": 0.9178571428571428, \"it_macro_f1\": 0.8260394351418703, \"en_macro_precision\": 0.5300272975432211, \"en_macro_recall\": 0.5157142857142857, \"en_macro_f1\": 0.5166744222118815, \"es_macro_precision\": 0.6344086021505376, \"es_macro_recall\": 0.7291825821237585, \"es_macro_f1\": 0.6389487870619946}, {\"fold\": 3, \"epoch\": 0, \"train_loss\": 0.3388367239521803, \"val_loss\": 0.1175255377311259, \"overall_macro_precision\": 0.4269311064718162, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4605855855855856, \"it_macro_precision\": 0.4022988505747126, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.445859872611465, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 3, \"epoch\": 1, \"train_loss\": 0.2765473757401435, \"val_loss\": 0.1129226996563375, \"overall_macro_precision\": 0.4269311064718162, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4605855855855856, \"it_macro_precision\": 0.4022988505747126, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.445859872611465, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 3, \"epoch\": 2, \"train_loss\": 0.2488742131930198, \"val_loss\": 0.1335524233678976, \"overall_macro_precision\": 0.5401213943646286, \"overall_macro_recall\": 0.5251659098847363, \"overall_macro_f1\": 0.5261814352723444, \"it_macro_precision\": 0.4951584507042254, \"it_macro_recall\": 0.4953781512605042, \"it_macro_f1\": 0.495164410058027, \"en_macro_precision\": 0.6262939958592132, \"en_macro_recall\": 0.5290476190476191, \"en_macro_f1\": 0.5347077737847551, \"es_macro_precision\": 0.5925925925925926, \"es_macro_recall\": 0.5286478227654698, \"es_macro_f1\": 0.5241844769403824}, {\"fold\": 3, \"epoch\": 3, \"train_loss\": 0.2254904005268629, \"val_loss\": 0.1404408393427729, \"overall_macro_precision\": 0.5557564798071127, \"overall_macro_recall\": 0.5646175340551869, \"overall_macro_f1\": 0.5589746074820702, \"it_macro_precision\": 0.5236175115207373, \"it_macro_recall\": 0.534453781512605, \"it_macro_f1\": 0.5109126984126984, \"en_macro_precision\": 0.5591194968553459, \"en_macro_recall\": 0.5223809523809524, \"en_macro_f1\": 0.5251234883324817, \"es_macro_precision\": 0.5785104364326376, \"es_macro_recall\": 0.5632161955691367, \"es_macro_f1\": 0.5685343463121241}, {\"fold\": 3, \"epoch\": 4, \"train_loss\": 0.2054488635248076, \"val_loss\": 0.1508004310851295, \"overall_macro_precision\": 0.5907603383059203, \"overall_macro_recall\": 0.6491791826755151, \"overall_macro_f1\": 0.5946923076923076, \"it_macro_precision\": 0.5831457698927578, \"it_macro_recall\": 0.6319327731092437, \"it_macro_f1\": 0.5318385650224215, \"en_macro_precision\": 0.5591194968553459, \"en_macro_recall\": 0.5223809523809524, \"en_macro_f1\": 0.5251234883324817, \"es_macro_precision\": 0.5923520923520924, \"es_macro_recall\": 0.6466768525592055, \"es_macro_f1\": 0.595756880733945}, {\"fold\": 3, \"epoch\": 5, \"train_loss\": 0.1891628430109389, \"val_loss\": 0.1411580366703371, \"overall_macro_precision\": 0.6188457086894588, \"overall_macro_recall\": 0.6865001746419839, \"overall_macro_f1\": 0.6307549175970228, \"it_macro_precision\": 0.6366185897435898, \"it_macro_recall\": 0.7149159663865545, \"it_macro_f1\": 0.6181900726392252, \"en_macro_precision\": 0.584375, \"en_macro_recall\": 0.5257142857142857, \"en_macro_f1\": 0.5297491039426523, \"es_macro_precision\": 0.5939359267734554, \"es_macro_recall\": 0.6567990832696715, \"es_macro_f1\": 0.5930456294667399}, {\"fold\": 3, \"epoch\": 6, \"train_loss\": 0.1956996019848071, \"val_loss\": 0.1557727227918803, \"overall_macro_precision\": 0.6327238461846907, \"overall_macro_recall\": 0.7312259867272093, \"overall_macro_f1\": 0.6405417772490162, \"it_macro_precision\": 0.6522573811730439, \"it_macro_recall\": 0.7415966386554622, \"it_macro_f1\": 0.606744394618834, \"en_macro_precision\": 0.6046405823475887, \"en_macro_recall\": 0.5547619047619048, \"en_macro_f1\": 0.5675507988211571, \"es_macro_precision\": 0.625475687103594, \"es_macro_recall\": 0.7266997708174179, \"es_macro_f1\": 0.615901171998733}, {\"fold\": 3, \"epoch\": 7, \"train_loss\": 0.1676728614082519, \"val_loss\": 0.1368621625937521, \"overall_macro_precision\": 0.6784862787076683, \"overall_macro_recall\": 0.7562521830247991, \"overall_macro_f1\": 0.7015407273139231, \"it_macro_precision\": 0.744640605296343, \"it_macro_recall\": 0.8260504201680672, \"it_macro_f1\": 0.7683294869518906, \"en_macro_precision\": 0.6622641509433962, \"en_macro_recall\": 0.5614285714285714, \"en_macro_f1\": 0.5809913132345427, \"es_macro_precision\": 0.625475687103594, \"es_macro_recall\": 0.7266997708174179, \"es_macro_f1\": 0.615901171998733}, {\"fold\": 3, \"epoch\": 8, \"train_loss\": 0.1707056705316487, \"val_loss\": 0.1415501981197545, \"overall_macro_precision\": 0.6693593314763231, \"overall_macro_recall\": 0.7548375829549423, \"overall_macro_f1\": 0.6914336622807018, \"it_macro_precision\": 0.746063877642825, \"it_macro_recall\": 0.8447478991596639, \"it_macro_f1\": 0.769359045623637, \"en_macro_precision\": 0.5421940928270043, \"en_macro_recall\": 0.5190476190476191, \"en_macro_f1\": 0.5207792207792208, \"es_macro_precision\": 0.6193609022556391, \"es_macro_recall\": 0.7182964094728801, \"es_macro_f1\": 0.6043524349940762}, {\"fold\": 3, \"epoch\": 9, \"train_loss\": 0.1623781222347034, \"val_loss\": 0.1295013526609788, \"overall_macro_precision\": 0.6824710519832471, \"overall_macro_recall\": 0.7586971707998602, \"overall_macro_f1\": 0.7058411882319338, \"it_macro_precision\": 0.7639885222381635, \"it_macro_recall\": 0.8478991596638655, \"it_macro_f1\": 0.7898456721091478, \"en_macro_precision\": 0.5421940928270043, \"en_macro_recall\": 0.5190476190476191, \"en_macro_f1\": 0.5207792207792208, \"es_macro_precision\": 0.6319682675814752, \"es_macro_recall\": 0.7351031321619557, \"es_macro_f1\": 0.6276328502415459}, {\"fold\": 4, \"epoch\": 0, \"train_loss\": 0.3437919545480433, \"val_loss\": 0.1179714624459544, \"overall_macro_precision\": 0.4269311064718162, \"overall_macro_recall\": 0.5, \"overall_macro_f1\": 0.4605855855855856, \"it_macro_precision\": 0.4022988505747126, \"it_macro_recall\": 0.5, \"it_macro_f1\": 0.445859872611465, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 4, \"epoch\": 1, \"train_loss\": 0.3037378879346172, \"val_loss\": 0.1134755939555664, \"overall_macro_precision\": 0.4266247379454926, \"overall_macro_recall\": 0.4975550122249388, \"overall_macro_f1\": 0.4593679458239277, \"it_macro_precision\": 0.4011627906976744, \"it_macro_recall\": 0.4928571428571429, \"it_macro_f1\": 0.4423076923076923, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.4219858156028369, \"es_macro_recall\": 0.5, \"es_macro_f1\": 0.4576923076923077}, {\"fold\": 4, \"epoch\": 2, \"train_loss\": 0.2530475135563927, \"val_loss\": 0.1280717698236306, \"overall_macro_precision\": 0.6378058484185399, \"overall_macro_recall\": 0.548393293747817, \"overall_macro_f1\": 0.5551009137463601, \"it_macro_precision\": 0.5491071428571428, \"it_macro_recall\": 0.523109243697479, \"it_macro_f1\": 0.5166666666666667, \"en_macro_precision\": 0.4573170731707317, \"en_macro_recall\": 0.5, \"en_macro_f1\": 0.4777070063694267, \"es_macro_precision\": 0.7485902255639098, \"es_macro_recall\": 0.6010313216195569, \"es_macro_f1\": 0.626984126984127}, {\"fold\": 4, \"epoch\": 3, \"train_loss\": 0.2126467683736015, \"val_loss\": 0.1664348777383565, \"overall_macro_precision\": 0.528690841544177, \"overall_macro_recall\": 0.5496332518337408, \"overall_macro_f1\": 0.5147565340717228, \"it_macro_precision\": 0.4888874012585353, \"it_macro_recall\": 0.482563025210084, \"it_macro_f1\": 0.4185105709360819, \"en_macro_precision\": 0.4556962025316455, \"en_macro_recall\": 0.48, \"en_macro_f1\": 0.4675324675324675, \"es_macro_precision\": 0.5396505376344086, \"es_macro_recall\": 0.5676088617265088, \"es_macro_f1\": 0.5249326145552561}, {\"fold\": 4, \"epoch\": 4, \"train_loss\": 0.2102051712846493, \"val_loss\": 0.1680440523351232, \"overall_macro_precision\": 0.5578938277291571, \"overall_macro_recall\": 0.6053615089067412, \"overall_macro_f1\": 0.5430806954710113, \"it_macro_precision\": 0.553590425531915, \"it_macro_recall\": 0.5846638655462185, \"it_macro_f1\": 0.4933238636363636, \"en_macro_precision\": 0.4559748427672956, \"en_macro_recall\": 0.4833333333333333, \"en_macro_f1\": 0.4692556634304207, \"es_macro_precision\": 0.548146655922643, \"es_macro_recall\": 0.5912910618792971, \"es_macro_f1\": 0.4940972222222222}, {\"fold\": 4, \"epoch\": 5, \"train_loss\": 0.2145724964108975, \"val_loss\": 0.1420100068673491, \"overall_macro_precision\": 0.6225253743342377, \"overall_macro_recall\": 0.6703457911281872, \"overall_macro_f1\": 0.6361683196249919, \"it_macro_precision\": 0.6239189189189189, \"it_macro_recall\": 0.6926470588235294, \"it_macro_f1\": 0.6106481481481482, \"en_macro_precision\": 0.4567901234567901, \"en_macro_recall\": 0.4933333333333333, \"en_macro_f1\": 0.4743589743589743, \"es_macro_precision\": 0.6274193548387097, \"es_macro_recall\": 0.6659663865546219, \"es_macro_f1\": 0.6398615802916701}, {\"fold\": 4, \"epoch\": 6, \"train_loss\": 0.1814197009867605, \"val_loss\": 0.1770160580053925, \"overall_macro_precision\": 0.6094345432849609, \"overall_macro_recall\": 0.6986028641285364, \"overall_macro_f1\": 0.6064369219212169, \"it_macro_precision\": 0.6501074113856069, \"it_macro_recall\": 0.734873949579832, \"it_macro_f1\": 0.585016835016835, \"en_macro_precision\": 0.4559748427672956, \"en_macro_recall\": 0.4833333333333333, \"en_macro_f1\": 0.4692556634304207, \"es_macro_precision\": 0.5885225885225885, \"es_macro_recall\": 0.6661573720397249, \"es_macro_f1\": 0.5537175276201851}, {\"fold\": 4, \"epoch\": 7, \"train_loss\": 0.1631882808938184, \"val_loss\": 0.1718464794568717, \"overall_macro_precision\": 0.6325831096427834, \"overall_macro_recall\": 0.7242752357666783, \"overall_macro_f1\": 0.643175060252277, \"it_macro_precision\": 0.6827027027027027, \"it_macro_recall\": 0.7840336134453781, \"it_macro_f1\": 0.6777777777777778, \"en_macro_precision\": 0.4556962025316455, \"en_macro_recall\": 0.48, \"en_macro_f1\": 0.4675324675324675, \"es_macro_precision\": 0.6121457489878542, \"es_macro_recall\": 0.7116119174942704, \"es_macro_f1\": 0.5761273209549072}, {\"fold\": 4, \"epoch\": 8, \"train_loss\": 0.1668089010807521, \"val_loss\": 0.1853849650360644, \"overall_macro_precision\": 0.6338308457711442, \"overall_macro_recall\": 0.7254977296542089, \"overall_macro_f1\": 0.6449979901517435, \"it_macro_precision\": 0.7161931818181818, \"it_macro_recall\": 0.8197478991596638, \"it_macro_f1\": 0.7301224489795919, \"en_macro_precision\": 0.4556962025316455, \"en_macro_recall\": 0.48, \"en_macro_f1\": 0.4675324675324675, \"es_macro_precision\": 0.5917708753529649, \"es_macro_recall\": 0.6737967914438503, \"es_macro_f1\": 0.5262096774193548}, {\"fold\": 4, \"epoch\": 9, \"train_loss\": 0.1658437774407074, \"val_loss\": 0.164192140311934, \"overall_macro_precision\": 0.6403089293271266, \"overall_macro_recall\": 0.7316101990918618, \"overall_macro_f1\": 0.6542311598878249, \"it_macro_precision\": 0.746063877642825, \"it_macro_recall\": 0.8447478991596639, \"it_macro_f1\": 0.769359045623637, \"en_macro_precision\": 0.5154095701540957, \"en_macro_recall\": 0.5271428571428571, \"en_macro_f1\": 0.5121951219512195, \"es_macro_precision\": 0.5956659619450317, \"es_macro_recall\": 0.6728418640183346, \"es_macro_f1\": 0.5801710484637315}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1_plot = alt.Chart(df).mark_line(point=True, size=3).encode(\n",
    "    x=alt.X('epoch:Q', title='Epoch'),\n",
    "    y=alt.Y('overall_macro_f1:Q', title='Macro F1 Score', scale=alt.Scale(domain=[0.3, 0.8])),\n",
    "    color=alt.Color('fold:N', title='Fold'),\n",
    "    tooltip=['fold:N', 'epoch:Q', alt.Tooltip('overall_macro_f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Overall F1 Score by Fold')\n",
    "\n",
    "display(f1_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e229da-2e44-4497-abb2-085059f53f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-16cdcccf559e4077ac42e42ce51ef2a4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-16cdcccf559e4077ac42e42ce51ef2a4.vega-embed details,\n",
       "  #altair-viz-16cdcccf559e4077ac42e42ce51ef2a4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-16cdcccf559e4077ac42e42ce51ef2a4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-16cdcccf559e4077ac42e42ce51ef2a4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-16cdcccf559e4077ac42e42ce51ef2a4\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-f2fcc8935ee1f21df090f2c06be0e3a1\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"loss_type\", \"type\": \"nominal\"}, \"strokeDash\": {\"field\": \"fold\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"fold\", \"type\": \"nominal\"}, {\"field\": \"epoch\", \"type\": \"quantitative\"}, {\"field\": \"loss_type\", \"type\": \"nominal\"}, {\"field\": \"loss\", \"format\": \".4f\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"loss\", \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"Training & Validation Loss\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-f2fcc8935ee1f21df090f2c06be0e3a1\": [{\"fold\": 0, \"epoch\": 0, \"loss_type\": \"train_loss\", \"loss\": 0.3267235863872253}, {\"fold\": 0, \"epoch\": 1, \"loss_type\": \"train_loss\", \"loss\": 0.3086312822317772}, {\"fold\": 0, \"epoch\": 2, \"loss_type\": \"train_loss\", \"loss\": 0.251913471405741}, {\"fold\": 0, \"epoch\": 3, \"loss_type\": \"train_loss\", \"loss\": 0.2367185081226112}, {\"fold\": 0, \"epoch\": 4, \"loss_type\": \"train_loss\", \"loss\": 0.2107491371677304}, {\"fold\": 0, \"epoch\": 5, \"loss_type\": \"train_loss\", \"loss\": 0.1960989370615813}, {\"fold\": 0, \"epoch\": 6, \"loss_type\": \"train_loss\", \"loss\": 0.1916557476740248}, {\"fold\": 0, \"epoch\": 7, \"loss_type\": \"train_loss\", \"loss\": 0.1804737198798761}, {\"fold\": 0, \"epoch\": 8, \"loss_type\": \"train_loss\", \"loss\": 0.1856320846390767}, {\"fold\": 0, \"epoch\": 9, \"loss_type\": \"train_loss\", \"loss\": 0.1770366205439569}, {\"fold\": 1, \"epoch\": 0, \"loss_type\": \"train_loss\", \"loss\": 0.3409668991790853}, {\"fold\": 1, \"epoch\": 1, \"loss_type\": \"train_loss\", \"loss\": 0.2798350875666976}, {\"fold\": 1, \"epoch\": 2, \"loss_type\": \"train_loss\", \"loss\": 0.2553854051188831}, {\"fold\": 1, \"epoch\": 3, \"loss_type\": \"train_loss\", \"loss\": 0.2244763397713647}, {\"fold\": 1, \"epoch\": 4, \"loss_type\": \"train_loss\", \"loss\": 0.2067945076383813}, {\"fold\": 1, \"epoch\": 5, \"loss_type\": \"train_loss\", \"loss\": 0.1941540304598582}, {\"fold\": 1, \"epoch\": 6, \"loss_type\": \"train_loss\", \"loss\": 0.1794798510679363}, {\"fold\": 1, \"epoch\": 7, \"loss_type\": \"train_loss\", \"loss\": 0.1656912503913588}, {\"fold\": 1, \"epoch\": 8, \"loss_type\": \"train_loss\", \"loss\": 0.1706827061495533}, {\"fold\": 1, \"epoch\": 9, \"loss_type\": \"train_loss\", \"loss\": 0.1748654276173371}, {\"fold\": 2, \"epoch\": 0, \"loss_type\": \"train_loss\", \"loss\": 0.3337063257834011}, {\"fold\": 2, \"epoch\": 1, \"loss_type\": \"train_loss\", \"loss\": 0.3156460058759816}, {\"fold\": 2, \"epoch\": 2, \"loss_type\": \"train_loss\", \"loss\": 0.2434504786982153}, {\"fold\": 2, \"epoch\": 3, \"loss_type\": \"train_loss\", \"loss\": 0.2368379923222708}, {\"fold\": 2, \"epoch\": 4, \"loss_type\": \"train_loss\", \"loss\": 0.2198796592598414}, {\"fold\": 2, \"epoch\": 5, \"loss_type\": \"train_loss\", \"loss\": 0.1918858456676893}, {\"fold\": 2, \"epoch\": 6, \"loss_type\": \"train_loss\", \"loss\": 0.1800779037068795}, {\"fold\": 2, \"epoch\": 7, \"loss_type\": \"train_loss\", \"loss\": 0.191548400619713}, {\"fold\": 2, \"epoch\": 8, \"loss_type\": \"train_loss\", \"loss\": 0.1819014149126562}, {\"fold\": 2, \"epoch\": 9, \"loss_type\": \"train_loss\", \"loss\": 0.1655476424897456}, {\"fold\": 3, \"epoch\": 0, \"loss_type\": \"train_loss\", \"loss\": 0.3388367239521803}, {\"fold\": 3, \"epoch\": 1, \"loss_type\": \"train_loss\", \"loss\": 0.2765473757401435}, {\"fold\": 3, \"epoch\": 2, \"loss_type\": \"train_loss\", \"loss\": 0.2488742131930198}, {\"fold\": 3, \"epoch\": 3, \"loss_type\": \"train_loss\", \"loss\": 0.2254904005268629}, {\"fold\": 3, \"epoch\": 4, \"loss_type\": \"train_loss\", \"loss\": 0.2054488635248076}, {\"fold\": 3, \"epoch\": 5, \"loss_type\": \"train_loss\", \"loss\": 0.1891628430109389}, {\"fold\": 3, \"epoch\": 6, \"loss_type\": \"train_loss\", \"loss\": 0.1956996019848071}, {\"fold\": 3, \"epoch\": 7, \"loss_type\": \"train_loss\", \"loss\": 0.1676728614082519}, {\"fold\": 3, \"epoch\": 8, \"loss_type\": \"train_loss\", \"loss\": 0.1707056705316487}, {\"fold\": 3, \"epoch\": 9, \"loss_type\": \"train_loss\", \"loss\": 0.1623781222347034}, {\"fold\": 4, \"epoch\": 0, \"loss_type\": \"train_loss\", \"loss\": 0.3437919545480433}, {\"fold\": 4, \"epoch\": 1, \"loss_type\": \"train_loss\", \"loss\": 0.3037378879346172}, {\"fold\": 4, \"epoch\": 2, \"loss_type\": \"train_loss\", \"loss\": 0.2530475135563927}, {\"fold\": 4, \"epoch\": 3, \"loss_type\": \"train_loss\", \"loss\": 0.2126467683736015}, {\"fold\": 4, \"epoch\": 4, \"loss_type\": \"train_loss\", \"loss\": 0.2102051712846493}, {\"fold\": 4, \"epoch\": 5, \"loss_type\": \"train_loss\", \"loss\": 0.2145724964108975}, {\"fold\": 4, \"epoch\": 6, \"loss_type\": \"train_loss\", \"loss\": 0.1814197009867605}, {\"fold\": 4, \"epoch\": 7, \"loss_type\": \"train_loss\", \"loss\": 0.1631882808938184}, {\"fold\": 4, \"epoch\": 8, \"loss_type\": \"train_loss\", \"loss\": 0.1668089010807521}, {\"fold\": 4, \"epoch\": 9, \"loss_type\": \"train_loss\", \"loss\": 0.1658437774407074}, {\"fold\": 0, \"epoch\": 0, \"loss_type\": \"val_loss\", \"loss\": 0.115067987019817}, {\"fold\": 0, \"epoch\": 1, \"loss_type\": \"val_loss\", \"loss\": 0.1100468893845876}, {\"fold\": 0, \"epoch\": 2, \"loss_type\": \"val_loss\", \"loss\": 0.1252351569632689}, {\"fold\": 0, \"epoch\": 3, \"loss_type\": \"val_loss\", \"loss\": 0.1334751188134153}, {\"fold\": 0, \"epoch\": 4, \"loss_type\": \"val_loss\", \"loss\": 0.1368804833541314}, {\"fold\": 0, \"epoch\": 5, \"loss_type\": \"val_loss\", \"loss\": 0.1525283996636668}, {\"fold\": 0, \"epoch\": 6, \"loss_type\": \"val_loss\", \"loss\": 0.1502745484933257}, {\"fold\": 0, \"epoch\": 7, \"loss_type\": \"val_loss\", \"loss\": 0.1403825618171443}, {\"fold\": 0, \"epoch\": 8, \"loss_type\": \"val_loss\", \"loss\": 0.1020865689963102}, {\"fold\": 0, \"epoch\": 9, \"loss_type\": \"val_loss\", \"loss\": 0.1271016285444299}, {\"fold\": 1, \"epoch\": 0, \"loss_type\": \"val_loss\", \"loss\": 0.1158103788892428}, {\"fold\": 1, \"epoch\": 1, \"loss_type\": \"val_loss\", \"loss\": 0.1119360426751275}, {\"fold\": 1, \"epoch\": 2, \"loss_type\": \"val_loss\", \"loss\": 0.1342812413970629}, {\"fold\": 1, \"epoch\": 3, \"loss_type\": \"val_loss\", \"loss\": 0.1474877723803122}, {\"fold\": 1, \"epoch\": 4, \"loss_type\": \"val_loss\", \"loss\": 0.1657989844679832}, {\"fold\": 1, \"epoch\": 5, \"loss_type\": \"val_loss\", \"loss\": 0.1443751791492104}, {\"fold\": 1, \"epoch\": 6, \"loss_type\": \"val_loss\", \"loss\": 0.1520822898174325}, {\"fold\": 1, \"epoch\": 7, \"loss_type\": \"val_loss\", \"loss\": 0.1831698179865876}, {\"fold\": 1, \"epoch\": 8, \"loss_type\": \"val_loss\", \"loss\": 0.1630900113843381}, {\"fold\": 1, \"epoch\": 9, \"loss_type\": \"val_loss\", \"loss\": 0.1643215182237327}, {\"fold\": 2, \"epoch\": 0, \"loss_type\": \"val_loss\", \"loss\": 0.1137847064218173}, {\"fold\": 2, \"epoch\": 1, \"loss_type\": \"val_loss\", \"loss\": 0.109596178214997}, {\"fold\": 2, \"epoch\": 2, \"loss_type\": \"val_loss\", \"loss\": 0.13116221036762}, {\"fold\": 2, \"epoch\": 3, \"loss_type\": \"val_loss\", \"loss\": 0.1476562536011139}, {\"fold\": 2, \"epoch\": 4, \"loss_type\": \"val_loss\", \"loss\": 0.1436182119573156}, {\"fold\": 2, \"epoch\": 5, \"loss_type\": \"val_loss\", \"loss\": 0.1569367301339904}, {\"fold\": 2, \"epoch\": 6, \"loss_type\": \"val_loss\", \"loss\": 0.1558277137887974}, {\"fold\": 2, \"epoch\": 7, \"loss_type\": \"val_loss\", \"loss\": 0.1199327289747695}, {\"fold\": 2, \"epoch\": 8, \"loss_type\": \"val_loss\", \"loss\": 0.1202161794838806}, {\"fold\": 2, \"epoch\": 9, \"loss_type\": \"val_loss\", \"loss\": 0.1316004556603729}, {\"fold\": 3, \"epoch\": 0, \"loss_type\": \"val_loss\", \"loss\": 0.1175255377311259}, {\"fold\": 3, \"epoch\": 1, \"loss_type\": \"val_loss\", \"loss\": 0.1129226996563375}, {\"fold\": 3, \"epoch\": 2, \"loss_type\": \"val_loss\", \"loss\": 0.1335524233678976}, {\"fold\": 3, \"epoch\": 3, \"loss_type\": \"val_loss\", \"loss\": 0.1404408393427729}, {\"fold\": 3, \"epoch\": 4, \"loss_type\": \"val_loss\", \"loss\": 0.1508004310851295}, {\"fold\": 3, \"epoch\": 5, \"loss_type\": \"val_loss\", \"loss\": 0.1411580366703371}, {\"fold\": 3, \"epoch\": 6, \"loss_type\": \"val_loss\", \"loss\": 0.1557727227918803}, {\"fold\": 3, \"epoch\": 7, \"loss_type\": \"val_loss\", \"loss\": 0.1368621625937521}, {\"fold\": 3, \"epoch\": 8, \"loss_type\": \"val_loss\", \"loss\": 0.1415501981197545}, {\"fold\": 3, \"epoch\": 9, \"loss_type\": \"val_loss\", \"loss\": 0.1295013526609788}, {\"fold\": 4, \"epoch\": 0, \"loss_type\": \"val_loss\", \"loss\": 0.1179714624459544}, {\"fold\": 4, \"epoch\": 1, \"loss_type\": \"val_loss\", \"loss\": 0.1134755939555664}, {\"fold\": 4, \"epoch\": 2, \"loss_type\": \"val_loss\", \"loss\": 0.1280717698236306}, {\"fold\": 4, \"epoch\": 3, \"loss_type\": \"val_loss\", \"loss\": 0.1664348777383565}, {\"fold\": 4, \"epoch\": 4, \"loss_type\": \"val_loss\", \"loss\": 0.1680440523351232}, {\"fold\": 4, \"epoch\": 5, \"loss_type\": \"val_loss\", \"loss\": 0.1420100068673491}, {\"fold\": 4, \"epoch\": 6, \"loss_type\": \"val_loss\", \"loss\": 0.1770160580053925}, {\"fold\": 4, \"epoch\": 7, \"loss_type\": \"val_loss\", \"loss\": 0.1718464794568717}, {\"fold\": 4, \"epoch\": 8, \"loss_type\": \"val_loss\", \"loss\": 0.1853849650360644}, {\"fold\": 4, \"epoch\": 9, \"loss_type\": \"val_loss\", \"loss\": 0.164192140311934}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_data = df[['fold', 'epoch', 'train_loss', 'val_loss']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='loss_type', value_name='loss'\n",
    ")\n",
    "\n",
    "loss_plot = alt.Chart(loss_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y='loss:Q', color='loss_type:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'loss_type:N', alt.Tooltip('loss:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='Training & Validation Loss')\n",
    "\n",
    "display(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f2d4cc-f8fe-4499-99bb-a60bbdf49d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-a9432f2e389d453594f1cd1d8a0c611c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-a9432f2e389d453594f1cd1d8a0c611c.vega-embed details,\n",
       "  #altair-viz-a9432f2e389d453594f1cd1d8a0c611c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-a9432f2e389d453594f1cd1d8a0c611c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-a9432f2e389d453594f1cd1d8a0c611c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-a9432f2e389d453594f1cd1d8a0c611c\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b6ddb2ce83a5fef1819b487f34696733\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"language\", \"type\": \"nominal\"}, \"strokeDash\": {\"field\": \"fold\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"fold\", \"type\": \"nominal\"}, {\"field\": \"epoch\", \"type\": \"quantitative\"}, {\"field\": \"language\", \"type\": \"nominal\"}, {\"field\": \"f1\", \"format\": \".4f\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"f1\", \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"F1 by Language\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-b6ddb2ce83a5fef1819b487f34696733\": [{\"fold\": 0, \"epoch\": 0, \"language\": \"EN\", \"f1\": 0.4778481012658228}, {\"fold\": 0, \"epoch\": 1, \"language\": \"EN\", \"f1\": 0.4778481012658228}, {\"fold\": 0, \"epoch\": 2, \"language\": \"EN\", \"f1\": 0.4778481012658228}, {\"fold\": 0, \"epoch\": 3, \"language\": \"EN\", \"f1\": 0.5460317460317461}, {\"fold\": 0, \"epoch\": 4, \"language\": \"EN\", \"f1\": 0.5460317460317461}, {\"fold\": 0, \"epoch\": 5, \"language\": \"EN\", \"f1\": 0.5348618680699117}, {\"fold\": 0, \"epoch\": 6, \"language\": \"EN\", \"f1\": 0.5299145299145299}, {\"fold\": 0, \"epoch\": 7, \"language\": \"EN\", \"f1\": 0.5560118963319642}, {\"fold\": 0, \"epoch\": 8, \"language\": \"EN\", \"f1\": 0.4761904761904761}, {\"fold\": 0, \"epoch\": 9, \"language\": \"EN\", \"f1\": 0.6167950128346168}, {\"fold\": 1, \"epoch\": 0, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 1, \"epoch\": 1, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 1, \"epoch\": 2, \"language\": \"EN\", \"f1\": 0.472668810289389}, {\"fold\": 1, \"epoch\": 3, \"language\": \"EN\", \"f1\": 0.4709677419354838}, {\"fold\": 1, \"epoch\": 4, \"language\": \"EN\", \"f1\": 0.4709677419354838}, {\"fold\": 1, \"epoch\": 5, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 1, \"epoch\": 6, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 1, \"epoch\": 7, \"language\": \"EN\", \"f1\": 0.4675324675324675}, {\"fold\": 1, \"epoch\": 8, \"language\": \"EN\", \"f1\": 0.4675324675324675}, {\"fold\": 1, \"epoch\": 9, \"language\": \"EN\", \"f1\": 0.4569536423841059}, {\"fold\": 2, \"epoch\": 0, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 2, \"epoch\": 1, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 2, \"epoch\": 2, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 2, \"epoch\": 3, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 2, \"epoch\": 4, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 2, \"epoch\": 5, \"language\": \"EN\", \"f1\": 0.476038338658147}, {\"fold\": 2, \"epoch\": 6, \"language\": \"EN\", \"f1\": 0.476038338658147}, {\"fold\": 2, \"epoch\": 7, \"language\": \"EN\", \"f1\": 0.476038338658147}, {\"fold\": 2, \"epoch\": 8, \"language\": \"EN\", \"f1\": 0.476038338658147}, {\"fold\": 2, \"epoch\": 9, \"language\": \"EN\", \"f1\": 0.5166744222118815}, {\"fold\": 3, \"epoch\": 0, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 3, \"epoch\": 1, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 3, \"epoch\": 2, \"language\": \"EN\", \"f1\": 0.5347077737847551}, {\"fold\": 3, \"epoch\": 3, \"language\": \"EN\", \"f1\": 0.5251234883324817}, {\"fold\": 3, \"epoch\": 4, \"language\": \"EN\", \"f1\": 0.5251234883324817}, {\"fold\": 3, \"epoch\": 5, \"language\": \"EN\", \"f1\": 0.5297491039426523}, {\"fold\": 3, \"epoch\": 6, \"language\": \"EN\", \"f1\": 0.5675507988211571}, {\"fold\": 3, \"epoch\": 7, \"language\": \"EN\", \"f1\": 0.5809913132345427}, {\"fold\": 3, \"epoch\": 8, \"language\": \"EN\", \"f1\": 0.5207792207792208}, {\"fold\": 3, \"epoch\": 9, \"language\": \"EN\", \"f1\": 0.5207792207792208}, {\"fold\": 4, \"epoch\": 0, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 4, \"epoch\": 1, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 4, \"epoch\": 2, \"language\": \"EN\", \"f1\": 0.4777070063694267}, {\"fold\": 4, \"epoch\": 3, \"language\": \"EN\", \"f1\": 0.4675324675324675}, {\"fold\": 4, \"epoch\": 4, \"language\": \"EN\", \"f1\": 0.4692556634304207}, {\"fold\": 4, \"epoch\": 5, \"language\": \"EN\", \"f1\": 0.4743589743589743}, {\"fold\": 4, \"epoch\": 6, \"language\": \"EN\", \"f1\": 0.4692556634304207}, {\"fold\": 4, \"epoch\": 7, \"language\": \"EN\", \"f1\": 0.4675324675324675}, {\"fold\": 4, \"epoch\": 8, \"language\": \"EN\", \"f1\": 0.4675324675324675}, {\"fold\": 4, \"epoch\": 9, \"language\": \"EN\", \"f1\": 0.5121951219512195}, {\"fold\": 0, \"epoch\": 0, \"language\": \"ES\", \"f1\": 0.4594594594594595}, {\"fold\": 0, \"epoch\": 1, \"language\": \"ES\", \"f1\": 0.4594594594594595}, {\"fold\": 0, \"epoch\": 2, \"language\": \"ES\", \"f1\": 0.5696091348265262}, {\"fold\": 0, \"epoch\": 3, \"language\": \"ES\", \"f1\": 0.6565597095476401}, {\"fold\": 0, \"epoch\": 4, \"language\": \"ES\", \"f1\": 0.6266666666666667}, {\"fold\": 0, \"epoch\": 5, \"language\": \"ES\", \"f1\": 0.6556712962962963}, {\"fold\": 0, \"epoch\": 6, \"language\": \"ES\", \"f1\": 0.5856162226840481}, {\"fold\": 0, \"epoch\": 7, \"language\": \"ES\", \"f1\": 0.609037826859609}, {\"fold\": 0, \"epoch\": 8, \"language\": \"ES\", \"f1\": 0.6719191919191919}, {\"fold\": 0, \"epoch\": 9, \"language\": \"ES\", \"f1\": 0.689257176679491}, {\"fold\": 1, \"epoch\": 0, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 1, \"epoch\": 1, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 1, \"epoch\": 2, \"language\": \"ES\", \"f1\": 0.5896825396825397}, {\"fold\": 1, \"epoch\": 3, \"language\": \"ES\", \"f1\": 0.6159690366972477}, {\"fold\": 1, \"epoch\": 4, \"language\": \"ES\", \"f1\": 0.5862723663770784}, {\"fold\": 1, \"epoch\": 5, \"language\": \"ES\", \"f1\": 0.6219367712165618}, {\"fold\": 1, \"epoch\": 6, \"language\": \"ES\", \"f1\": 0.5908858166922683}, {\"fold\": 1, \"epoch\": 7, \"language\": \"ES\", \"f1\": 0.5735887096774194}, {\"fold\": 1, \"epoch\": 8, \"language\": \"ES\", \"f1\": 0.6200796626054358}, {\"fold\": 1, \"epoch\": 9, \"language\": \"ES\", \"f1\": 0.6319411485524442}, {\"fold\": 2, \"epoch\": 0, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 2, \"epoch\": 1, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 2, \"epoch\": 2, \"language\": \"ES\", \"f1\": 0.5638357705286839}, {\"fold\": 2, \"epoch\": 3, \"language\": \"ES\", \"f1\": 0.5843773028739867}, {\"fold\": 2, \"epoch\": 4, \"language\": \"ES\", \"f1\": 0.5986810135369663}, {\"fold\": 2, \"epoch\": 5, \"language\": \"ES\", \"f1\": 0.615901171998733}, {\"fold\": 2, \"epoch\": 6, \"language\": \"ES\", \"f1\": 0.5873992236488504}, {\"fold\": 2, \"epoch\": 7, \"language\": \"ES\", \"f1\": 0.6511819681143485}, {\"fold\": 2, \"epoch\": 8, \"language\": \"ES\", \"f1\": 0.6389487870619946}, {\"fold\": 2, \"epoch\": 9, \"language\": \"ES\", \"f1\": 0.6389487870619946}, {\"fold\": 3, \"epoch\": 0, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 3, \"epoch\": 1, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 3, \"epoch\": 2, \"language\": \"ES\", \"f1\": 0.5241844769403824}, {\"fold\": 3, \"epoch\": 3, \"language\": \"ES\", \"f1\": 0.5685343463121241}, {\"fold\": 3, \"epoch\": 4, \"language\": \"ES\", \"f1\": 0.595756880733945}, {\"fold\": 3, \"epoch\": 5, \"language\": \"ES\", \"f1\": 0.5930456294667399}, {\"fold\": 3, \"epoch\": 6, \"language\": \"ES\", \"f1\": 0.615901171998733}, {\"fold\": 3, \"epoch\": 7, \"language\": \"ES\", \"f1\": 0.615901171998733}, {\"fold\": 3, \"epoch\": 8, \"language\": \"ES\", \"f1\": 0.6043524349940762}, {\"fold\": 3, \"epoch\": 9, \"language\": \"ES\", \"f1\": 0.6276328502415459}, {\"fold\": 4, \"epoch\": 0, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 4, \"epoch\": 1, \"language\": \"ES\", \"f1\": 0.4576923076923077}, {\"fold\": 4, \"epoch\": 2, \"language\": \"ES\", \"f1\": 0.626984126984127}, {\"fold\": 4, \"epoch\": 3, \"language\": \"ES\", \"f1\": 0.5249326145552561}, {\"fold\": 4, \"epoch\": 4, \"language\": \"ES\", \"f1\": 0.4940972222222222}, {\"fold\": 4, \"epoch\": 5, \"language\": \"ES\", \"f1\": 0.6398615802916701}, {\"fold\": 4, \"epoch\": 6, \"language\": \"ES\", \"f1\": 0.5537175276201851}, {\"fold\": 4, \"epoch\": 7, \"language\": \"ES\", \"f1\": 0.5761273209549072}, {\"fold\": 4, \"epoch\": 8, \"language\": \"ES\", \"f1\": 0.5262096774193548}, {\"fold\": 4, \"epoch\": 9, \"language\": \"ES\", \"f1\": 0.5801710484637315}, {\"fold\": 0, \"epoch\": 0, \"language\": \"IT\", \"f1\": 0.4472843450479233}, {\"fold\": 0, \"epoch\": 1, \"language\": \"IT\", \"f1\": 0.4472843450479233}, {\"fold\": 0, \"epoch\": 2, \"language\": \"IT\", \"f1\": 0.4568288854003139}, {\"fold\": 0, \"epoch\": 3, \"language\": \"IT\", \"f1\": 0.5399143565472166}, {\"fold\": 0, \"epoch\": 4, \"language\": \"IT\", \"f1\": 0.5511294261294262}, {\"fold\": 0, \"epoch\": 5, \"language\": \"IT\", \"f1\": 0.4636664054781453}, {\"fold\": 0, \"epoch\": 6, \"language\": \"IT\", \"f1\": 0.6650069156293222}, {\"fold\": 0, \"epoch\": 7, \"language\": \"IT\", \"f1\": 0.7573632538569425}, {\"fold\": 0, \"epoch\": 8, \"language\": \"IT\", \"f1\": 0.7723684210526316}, {\"fold\": 0, \"epoch\": 9, \"language\": \"IT\", \"f1\": 0.7597222222222222}, {\"fold\": 1, \"epoch\": 0, \"language\": \"IT\", \"f1\": 0.4472843450479233}, {\"fold\": 1, \"epoch\": 1, \"language\": \"IT\", \"f1\": 0.4755167661920073}, {\"fold\": 1, \"epoch\": 2, \"language\": \"IT\", \"f1\": 0.5135895032802249}, {\"fold\": 1, \"epoch\": 3, \"language\": \"IT\", \"f1\": 0.5123326286116984}, {\"fold\": 1, \"epoch\": 4, \"language\": \"IT\", \"f1\": 0.5129809196121364}, {\"fold\": 1, \"epoch\": 5, \"language\": \"IT\", \"f1\": 0.6268104776579353}, {\"fold\": 1, \"epoch\": 6, \"language\": \"IT\", \"f1\": 0.7352774013028462}, {\"fold\": 1, \"epoch\": 7, \"language\": \"IT\", \"f1\": 0.7482882574460801}, {\"fold\": 1, \"epoch\": 8, \"language\": \"IT\", \"f1\": 0.7482882574460801}, {\"fold\": 1, \"epoch\": 9, \"language\": \"IT\", \"f1\": 0.7894523326572008}, {\"fold\": 2, \"epoch\": 0, \"language\": \"IT\", \"f1\": 0.4472843450479233}, {\"fold\": 2, \"epoch\": 1, \"language\": \"IT\", \"f1\": 0.4437299035369775}, {\"fold\": 2, \"epoch\": 2, \"language\": \"IT\", \"f1\": 0.5548316452231044}, {\"fold\": 2, \"epoch\": 3, \"language\": \"IT\", \"f1\": 0.4808987119437938}, {\"fold\": 2, \"epoch\": 4, \"language\": \"IT\", \"f1\": 0.603937728937729}, {\"fold\": 2, \"epoch\": 5, \"language\": \"IT\", \"f1\": 0.5511157239231967}, {\"fold\": 2, \"epoch\": 6, \"language\": \"IT\", \"f1\": 0.6505050505050505}, {\"fold\": 2, \"epoch\": 7, \"language\": \"IT\", \"f1\": 0.8080592908179115}, {\"fold\": 2, \"epoch\": 8, \"language\": \"IT\", \"f1\": 0.8401699926090169}, {\"fold\": 2, \"epoch\": 9, \"language\": \"IT\", \"f1\": 0.8260394351418703}, {\"fold\": 3, \"epoch\": 0, \"language\": \"IT\", \"f1\": 0.445859872611465}, {\"fold\": 3, \"epoch\": 1, \"language\": \"IT\", \"f1\": 0.445859872611465}, {\"fold\": 3, \"epoch\": 2, \"language\": \"IT\", \"f1\": 0.495164410058027}, {\"fold\": 3, \"epoch\": 3, \"language\": \"IT\", \"f1\": 0.5109126984126984}, {\"fold\": 3, \"epoch\": 4, \"language\": \"IT\", \"f1\": 0.5318385650224215}, {\"fold\": 3, \"epoch\": 5, \"language\": \"IT\", \"f1\": 0.6181900726392252}, {\"fold\": 3, \"epoch\": 6, \"language\": \"IT\", \"f1\": 0.606744394618834}, {\"fold\": 3, \"epoch\": 7, \"language\": \"IT\", \"f1\": 0.7683294869518906}, {\"fold\": 3, \"epoch\": 8, \"language\": \"IT\", \"f1\": 0.769359045623637}, {\"fold\": 3, \"epoch\": 9, \"language\": \"IT\", \"f1\": 0.7898456721091478}, {\"fold\": 4, \"epoch\": 0, \"language\": \"IT\", \"f1\": 0.445859872611465}, {\"fold\": 4, \"epoch\": 1, \"language\": \"IT\", \"f1\": 0.4423076923076923}, {\"fold\": 4, \"epoch\": 2, \"language\": \"IT\", \"f1\": 0.5166666666666667}, {\"fold\": 4, \"epoch\": 3, \"language\": \"IT\", \"f1\": 0.4185105709360819}, {\"fold\": 4, \"epoch\": 4, \"language\": \"IT\", \"f1\": 0.4933238636363636}, {\"fold\": 4, \"epoch\": 5, \"language\": \"IT\", \"f1\": 0.6106481481481482}, {\"fold\": 4, \"epoch\": 6, \"language\": \"IT\", \"f1\": 0.585016835016835}, {\"fold\": 4, \"epoch\": 7, \"language\": \"IT\", \"f1\": 0.6777777777777778}, {\"fold\": 4, \"epoch\": 8, \"language\": \"IT\", \"f1\": 0.7301224489795919}, {\"fold\": 4, \"epoch\": 9, \"language\": \"IT\", \"f1\": 0.769359045623637}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_data = df[['fold', 'epoch', 'en_macro_f1', 'es_macro_f1', 'it_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='language', value_name='f1'\n",
    ")\n",
    "lang_data['language'] = lang_data['language'].str.replace('_macro_f1', '').str.upper()\n",
    "\n",
    "lang_plot = alt.Chart(lang_data).mark_line(point=True).encode(\n",
    "    x='epoch:Q', y='f1:Q', color='language:N', strokeDash='fold:N',\n",
    "    tooltip=['fold:N', 'epoch:Q', 'language:N', alt.Tooltip('f1:Q', format='.4f')]\n",
    ").properties(width=600, height=300, title='F1 by Language')\n",
    "\n",
    "display(lang_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d63ee4b-3eb6-4e82-bd83-4b33ac27ad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-7362c2f0e32444689234c15025c4af26.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-7362c2f0e32444689234c15025c4af26.vega-embed details,\n",
       "  #altair-viz-7362c2f0e32444689234c15025c4af26.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-7362c2f0e32444689234c15025c4af26\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7362c2f0e32444689234c15025c4af26\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7362c2f0e32444689234c15025c4af26\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-32876e69730122fd7997ad4eee69713c\"}, \"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"value\", \"scale\": {\"scheme\": \"viridis\"}, \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"fold_epoch\", \"type\": \"nominal\"}, {\"field\": \"metric\", \"type\": \"nominal\"}, {\"field\": \"value\", \"format\": \".4f\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"fold_epoch\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"metric\", \"type\": \"nominal\"}}, \"height\": 150, \"title\": \"Precision/Recall/F1 Heatmap\", \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-32876e69730122fd7997ad4eee69713c\": [{\"fold\": 0, \"epoch\": 0, \"metric\": \"Precision\", \"value\": 0.4288702928870292, \"fold_epoch\": \"F0:E0\"}, {\"fold\": 0, \"epoch\": 1, \"metric\": \"Precision\", \"value\": 0.4288702928870292, \"fold_epoch\": \"F0:E1\"}, {\"fold\": 0, \"epoch\": 2, \"metric\": \"Precision\", \"value\": 0.5304635761589404, \"fold_epoch\": \"F0:E2\"}, {\"fold\": 0, \"epoch\": 3, \"metric\": \"Precision\", \"value\": 0.5975020678246484, \"fold_epoch\": \"F0:E3\"}, {\"fold\": 0, \"epoch\": 4, \"metric\": \"Precision\", \"value\": 0.6024195103142471, \"fold_epoch\": \"F0:E4\"}, {\"fold\": 0, \"epoch\": 5, \"metric\": \"Precision\", \"value\": 0.6024964838255977, \"fold_epoch\": \"F0:E5\"}, {\"fold\": 0, \"epoch\": 6, \"metric\": \"Precision\", \"value\": 0.6344802152030545, \"fold_epoch\": \"F0:E6\"}, {\"fold\": 0, \"epoch\": 7, \"metric\": \"Precision\", \"value\": 0.6674558080808081, \"fold_epoch\": \"F0:E7\"}, {\"fold\": 0, \"epoch\": 8, \"metric\": \"Precision\", \"value\": 0.7011887164326189, \"fold_epoch\": \"F0:E8\"}, {\"fold\": 0, \"epoch\": 9, \"metric\": \"Precision\", \"value\": 0.6948351621376144, \"fold_epoch\": \"F0:E9\"}, {\"fold\": 1, \"epoch\": 0, \"metric\": \"Precision\", \"value\": 0.4278242677824267, \"fold_epoch\": \"F1:E0\"}, {\"fold\": 1, \"epoch\": 1, \"metric\": \"Precision\", \"value\": 0.6785714285714286, \"fold_epoch\": \"F1:E1\"}, {\"fold\": 1, \"epoch\": 2, \"metric\": \"Precision\", \"value\": 0.5689479060265576, \"fold_epoch\": \"F1:E2\"}, {\"fold\": 1, \"epoch\": 3, \"metric\": \"Precision\", \"value\": 0.5654069385718412, \"fold_epoch\": \"F1:E3\"}, {\"fold\": 1, \"epoch\": 4, \"metric\": \"Precision\", \"value\": 0.6034266923263766, \"fold_epoch\": \"F1:E4\"}, {\"fold\": 1, \"epoch\": 5, \"metric\": \"Precision\", \"value\": 0.6303534571723426, \"fold_epoch\": \"F1:E5\"}, {\"fold\": 1, \"epoch\": 6, \"metric\": \"Precision\", \"value\": 0.6559605517425202, \"fold_epoch\": \"F1:E6\"}, {\"fold\": 1, \"epoch\": 7, \"metric\": \"Precision\", \"value\": 0.6518325162878547, \"fold_epoch\": \"F1:E7\"}, {\"fold\": 1, \"epoch\": 8, \"metric\": \"Precision\", \"value\": 0.6634621730426558, \"fold_epoch\": \"F1:E8\"}, {\"fold\": 1, \"epoch\": 9, \"metric\": \"Precision\", \"value\": 0.6681696428571429, \"fold_epoch\": \"F1:E9\"}, {\"fold\": 2, \"epoch\": 0, \"metric\": \"Precision\", \"value\": 0.4278242677824267, \"fold_epoch\": \"F2:E0\"}, {\"fold\": 2, \"epoch\": 1, \"metric\": \"Precision\", \"value\": 0.4275210084033613, \"fold_epoch\": \"F2:E1\"}, {\"fold\": 2, \"epoch\": 2, \"metric\": \"Precision\", \"value\": 0.584931506849315, \"fold_epoch\": \"F2:E2\"}, {\"fold\": 2, \"epoch\": 3, \"metric\": \"Precision\", \"value\": 0.5595400646784047, \"fold_epoch\": \"F2:E3\"}, {\"fold\": 2, \"epoch\": 4, \"metric\": \"Precision\", \"value\": 0.6121482683982684, \"fold_epoch\": \"F2:E4\"}, {\"fold\": 2, \"epoch\": 5, \"metric\": \"Precision\", \"value\": 0.6221842473201802, \"fold_epoch\": \"F2:E5\"}, {\"fold\": 2, \"epoch\": 6, \"metric\": \"Precision\", \"value\": 0.6401557959315467, \"fold_epoch\": \"F2:E6\"}, {\"fold\": 2, \"epoch\": 7, \"metric\": \"Precision\", \"value\": 0.7034955484938414, \"fold_epoch\": \"F2:E7\"}, {\"fold\": 2, \"epoch\": 8, \"metric\": \"Precision\", \"value\": 0.7108485499462943, \"fold_epoch\": \"F2:E8\"}, {\"fold\": 2, \"epoch\": 9, \"metric\": \"Precision\", \"value\": 0.699339666642119, \"fold_epoch\": \"F2:E9\"}, {\"fold\": 3, \"epoch\": 0, \"metric\": \"Precision\", \"value\": 0.4269311064718162, \"fold_epoch\": \"F3:E0\"}, {\"fold\": 3, \"epoch\": 1, \"metric\": \"Precision\", \"value\": 0.4269311064718162, \"fold_epoch\": \"F3:E1\"}, {\"fold\": 3, \"epoch\": 2, \"metric\": \"Precision\", \"value\": 0.5401213943646286, \"fold_epoch\": \"F3:E2\"}, {\"fold\": 3, \"epoch\": 3, \"metric\": \"Precision\", \"value\": 0.5557564798071127, \"fold_epoch\": \"F3:E3\"}, {\"fold\": 3, \"epoch\": 4, \"metric\": \"Precision\", \"value\": 0.5907603383059203, \"fold_epoch\": \"F3:E4\"}, {\"fold\": 3, \"epoch\": 5, \"metric\": \"Precision\", \"value\": 0.6188457086894588, \"fold_epoch\": \"F3:E5\"}, {\"fold\": 3, \"epoch\": 6, \"metric\": \"Precision\", \"value\": 0.6327238461846907, \"fold_epoch\": \"F3:E6\"}, {\"fold\": 3, \"epoch\": 7, \"metric\": \"Precision\", \"value\": 0.6784862787076683, \"fold_epoch\": \"F3:E7\"}, {\"fold\": 3, \"epoch\": 8, \"metric\": \"Precision\", \"value\": 0.6693593314763231, \"fold_epoch\": \"F3:E8\"}, {\"fold\": 3, \"epoch\": 9, \"metric\": \"Precision\", \"value\": 0.6824710519832471, \"fold_epoch\": \"F3:E9\"}, {\"fold\": 4, \"epoch\": 0, \"metric\": \"Precision\", \"value\": 0.4269311064718162, \"fold_epoch\": \"F4:E0\"}, {\"fold\": 4, \"epoch\": 1, \"metric\": \"Precision\", \"value\": 0.4266247379454926, \"fold_epoch\": \"F4:E1\"}, {\"fold\": 4, \"epoch\": 2, \"metric\": \"Precision\", \"value\": 0.6378058484185399, \"fold_epoch\": \"F4:E2\"}, {\"fold\": 4, \"epoch\": 3, \"metric\": \"Precision\", \"value\": 0.528690841544177, \"fold_epoch\": \"F4:E3\"}, {\"fold\": 4, \"epoch\": 4, \"metric\": \"Precision\", \"value\": 0.5578938277291571, \"fold_epoch\": \"F4:E4\"}, {\"fold\": 4, \"epoch\": 5, \"metric\": \"Precision\", \"value\": 0.6225253743342377, \"fold_epoch\": \"F4:E5\"}, {\"fold\": 4, \"epoch\": 6, \"metric\": \"Precision\", \"value\": 0.6094345432849609, \"fold_epoch\": \"F4:E6\"}, {\"fold\": 4, \"epoch\": 7, \"metric\": \"Precision\", \"value\": 0.6325831096427834, \"fold_epoch\": \"F4:E7\"}, {\"fold\": 4, \"epoch\": 8, \"metric\": \"Precision\", \"value\": 0.6338308457711442, \"fold_epoch\": \"F4:E8\"}, {\"fold\": 4, \"epoch\": 9, \"metric\": \"Precision\", \"value\": 0.6403089293271266, \"fold_epoch\": \"F4:E9\"}, {\"fold\": 0, \"epoch\": 0, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F0:E0\"}, {\"fold\": 0, \"epoch\": 1, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F0:E1\"}, {\"fold\": 0, \"epoch\": 2, \"metric\": \"Recall\", \"value\": 0.5123744619799139, \"fold_epoch\": \"F0:E2\"}, {\"fold\": 0, \"epoch\": 3, \"metric\": \"Recall\", \"value\": 0.6057030129124821, \"fold_epoch\": \"F0:E3\"}, {\"fold\": 0, \"epoch\": 4, \"metric\": \"Recall\", \"value\": 0.6524390243902439, \"fold_epoch\": \"F0:E4\"}, {\"fold\": 0, \"epoch\": 5, \"metric\": \"Recall\", \"value\": 0.6881994261119082, \"fold_epoch\": \"F0:E5\"}, {\"fold\": 0, \"epoch\": 6, \"metric\": \"Recall\", \"value\": 0.7223457675753229, \"fold_epoch\": \"F0:E6\"}, {\"fold\": 0, \"epoch\": 7, \"metric\": \"Recall\", \"value\": 0.7663916786226685, \"fold_epoch\": \"F0:E7\"}, {\"fold\": 0, \"epoch\": 8, \"metric\": \"Recall\", \"value\": 0.7343256814921091, \"fold_epoch\": \"F0:E8\"}, {\"fold\": 0, \"epoch\": 9, \"metric\": \"Recall\", \"value\": 0.7846843615494978, \"fold_epoch\": \"F0:E9\"}, {\"fold\": 1, \"epoch\": 0, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F1:E0\"}, {\"fold\": 1, \"epoch\": 1, \"metric\": \"Recall\", \"value\": 0.5060238829240636, \"fold_epoch\": \"F1:E1\"}, {\"fold\": 1, \"epoch\": 2, \"metric\": \"Recall\", \"value\": 0.5358775380036143, \"fold_epoch\": \"F1:E2\"}, {\"fold\": 1, \"epoch\": 3, \"metric\": \"Recall\", \"value\": 0.5882498848375324, \"fold_epoch\": \"F1:E3\"}, {\"fold\": 1, \"epoch\": 4, \"metric\": \"Recall\", \"value\": 0.6961482583891428, \"fold_epoch\": \"F1:E4\"}, {\"fold\": 1, \"epoch\": 5, \"metric\": \"Recall\", \"value\": 0.714840012756458, \"fold_epoch\": \"F1:E5\"}, {\"fold\": 1, \"epoch\": 6, \"metric\": \"Recall\", \"value\": 0.7488040820665463, \"fold_epoch\": \"F1:E6\"}, {\"fold\": 1, \"epoch\": 7, \"metric\": \"Recall\", \"value\": 0.7535168845894902, \"fold_epoch\": \"F1:E7\"}, {\"fold\": 1, \"epoch\": 8, \"metric\": \"Recall\", \"value\": 0.7632968356897346, \"fold_epoch\": \"F1:E8\"}, {\"fold\": 1, \"epoch\": 9, \"metric\": \"Recall\", \"value\": 0.7669643173523263, \"fold_epoch\": \"F1:E9\"}, {\"fold\": 2, \"epoch\": 0, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F2:E0\"}, {\"fold\": 2, \"epoch\": 1, \"metric\": \"Recall\", \"value\": 0.4975550122249388, \"fold_epoch\": \"F2:E1\"}, {\"fold\": 2, \"epoch\": 2, \"metric\": \"Recall\", \"value\": 0.5527266928882747, \"fold_epoch\": \"F2:E2\"}, {\"fold\": 2, \"epoch\": 3, \"metric\": \"Recall\", \"value\": 0.588072711810354, \"fold_epoch\": \"F2:E3\"}, {\"fold\": 2, \"epoch\": 4, \"metric\": \"Recall\", \"value\": 0.676251727437015, \"fold_epoch\": \"F2:E4\"}, {\"fold\": 2, \"epoch\": 5, \"metric\": \"Recall\", \"value\": 0.7229545374012261, \"fold_epoch\": \"F2:E5\"}, {\"fold\": 2, \"epoch\": 6, \"metric\": \"Recall\", \"value\": 0.7460933347507175, \"fold_epoch\": \"F2:E6\"}, {\"fold\": 2, \"epoch\": 7, \"metric\": \"Recall\", \"value\": 0.7745650402182771, \"fold_epoch\": \"F2:E7\"}, {\"fold\": 2, \"epoch\": 8, \"metric\": \"Recall\", \"value\": 0.7782325218808689, \"fold_epoch\": \"F2:E8\"}, {\"fold\": 2, \"epoch\": 9, \"metric\": \"Recall\", \"value\": 0.7877467134403459, \"fold_epoch\": \"F2:E9\"}, {\"fold\": 3, \"epoch\": 0, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F3:E0\"}, {\"fold\": 3, \"epoch\": 1, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F3:E1\"}, {\"fold\": 3, \"epoch\": 2, \"metric\": \"Recall\", \"value\": 0.5251659098847363, \"fold_epoch\": \"F3:E2\"}, {\"fold\": 3, \"epoch\": 3, \"metric\": \"Recall\", \"value\": 0.5646175340551869, \"fold_epoch\": \"F3:E3\"}, {\"fold\": 3, \"epoch\": 4, \"metric\": \"Recall\", \"value\": 0.6491791826755151, \"fold_epoch\": \"F3:E4\"}, {\"fold\": 3, \"epoch\": 5, \"metric\": \"Recall\", \"value\": 0.6865001746419839, \"fold_epoch\": \"F3:E5\"}, {\"fold\": 3, \"epoch\": 6, \"metric\": \"Recall\", \"value\": 0.7312259867272093, \"fold_epoch\": \"F3:E6\"}, {\"fold\": 3, \"epoch\": 7, \"metric\": \"Recall\", \"value\": 0.7562521830247991, \"fold_epoch\": \"F3:E7\"}, {\"fold\": 3, \"epoch\": 8, \"metric\": \"Recall\", \"value\": 0.7548375829549423, \"fold_epoch\": \"F3:E8\"}, {\"fold\": 3, \"epoch\": 9, \"metric\": \"Recall\", \"value\": 0.7586971707998602, \"fold_epoch\": \"F3:E9\"}, {\"fold\": 4, \"epoch\": 0, \"metric\": \"Recall\", \"value\": 0.5, \"fold_epoch\": \"F4:E0\"}, {\"fold\": 4, \"epoch\": 1, \"metric\": \"Recall\", \"value\": 0.4975550122249388, \"fold_epoch\": \"F4:E1\"}, {\"fold\": 4, \"epoch\": 2, \"metric\": \"Recall\", \"value\": 0.548393293747817, \"fold_epoch\": \"F4:E2\"}, {\"fold\": 4, \"epoch\": 3, \"metric\": \"Recall\", \"value\": 0.5496332518337408, \"fold_epoch\": \"F4:E3\"}, {\"fold\": 4, \"epoch\": 4, \"metric\": \"Recall\", \"value\": 0.6053615089067412, \"fold_epoch\": \"F4:E4\"}, {\"fold\": 4, \"epoch\": 5, \"metric\": \"Recall\", \"value\": 0.6703457911281872, \"fold_epoch\": \"F4:E5\"}, {\"fold\": 4, \"epoch\": 6, \"metric\": \"Recall\", \"value\": 0.6986028641285364, \"fold_epoch\": \"F4:E6\"}, {\"fold\": 4, \"epoch\": 7, \"metric\": \"Recall\", \"value\": 0.7242752357666783, \"fold_epoch\": \"F4:E7\"}, {\"fold\": 4, \"epoch\": 8, \"metric\": \"Recall\", \"value\": 0.7254977296542089, \"fold_epoch\": \"F4:E8\"}, {\"fold\": 4, \"epoch\": 9, \"metric\": \"Recall\", \"value\": 0.7316101990918618, \"fold_epoch\": \"F4:E9\"}, {\"fold\": 0, \"epoch\": 0, \"metric\": \"F1\", \"value\": 0.4617117117117117, \"fold_epoch\": \"F0:E0\"}, {\"fold\": 0, \"epoch\": 1, \"metric\": \"F1\", \"value\": 0.4617117117117117, \"fold_epoch\": \"F0:E1\"}, {\"fold\": 0, \"epoch\": 2, \"metric\": \"F1\", \"value\": 0.5056753759702961, \"fold_epoch\": \"F0:E2\"}, {\"fold\": 0, \"epoch\": 3, \"metric\": \"F1\", \"value\": 0.6011835642831953, \"fold_epoch\": \"F0:E3\"}, {\"fold\": 0, \"epoch\": 4, \"metric\": \"F1\", \"value\": 0.6131697759604736, \"fold_epoch\": \"F0:E4\"}, {\"fold\": 0, \"epoch\": 5, \"metric\": \"F1\", \"value\": 0.5992334411306743, \"fold_epoch\": \"F0:E5\"}, {\"fold\": 0, \"epoch\": 6, \"metric\": \"F1\", \"value\": 0.648501720198545, \"fold_epoch\": \"F0:E6\"}, {\"fold\": 0, \"epoch\": 7, \"metric\": \"F1\", \"value\": 0.6895851936033769, \"fold_epoch\": \"F0:E7\"}, {\"fold\": 0, \"epoch\": 8, \"metric\": \"F1\", \"value\": 0.7153349875930521, \"fold_epoch\": \"F0:E8\"}, {\"fold\": 0, \"epoch\": 9, \"metric\": \"F1\", \"value\": 0.7216194646362244, \"fold_epoch\": \"F0:E9\"}, {\"fold\": 1, \"epoch\": 0, \"metric\": \"F1\", \"value\": 0.4611048478015783, \"fold_epoch\": \"F1:E0\"}, {\"fold\": 1, \"epoch\": 1, \"metric\": \"F1\", \"value\": 0.4751014561947959, \"fold_epoch\": \"F1:E1\"}, {\"fold\": 1, \"epoch\": 2, \"metric\": \"F1\", \"value\": 0.5390549662487946, \"fold_epoch\": \"F1:E2\"}, {\"fold\": 1, \"epoch\": 3, \"metric\": \"F1\", \"value\": 0.5707229456668164, \"fold_epoch\": \"F1:E3\"}, {\"fold\": 1, \"epoch\": 4, \"metric\": \"F1\", \"value\": 0.5916484417714598, \"fold_epoch\": \"F1:E4\"}, {\"fold\": 1, \"epoch\": 5, \"metric\": \"F1\", \"value\": 0.6429476145626969, \"fold_epoch\": \"F1:E5\"}, {\"fold\": 1, \"epoch\": 6, \"metric\": \"F1\", \"value\": 0.6751419205245063, \"fold_epoch\": \"F1:E6\"}, {\"fold\": 1, \"epoch\": 7, \"metric\": \"F1\", \"value\": 0.6681921020156314, \"fold_epoch\": \"F1:E7\"}, {\"fold\": 1, \"epoch\": 8, \"metric\": \"F1\", \"value\": 0.6838624338624338, \"fold_epoch\": \"F1:E8\"}, {\"fold\": 1, \"epoch\": 9, \"metric\": \"F1\", \"value\": 0.6899072383512904, \"fold_epoch\": \"F1:E9\"}, {\"fold\": 2, \"epoch\": 0, \"metric\": \"F1\", \"value\": 0.4611048478015783, \"fold_epoch\": \"F2:E0\"}, {\"fold\": 2, \"epoch\": 1, \"metric\": \"F1\", \"value\": 0.4598870056497175, \"fold_epoch\": \"F2:E1\"}, {\"fold\": 2, \"epoch\": 2, \"metric\": \"F1\", \"value\": 0.559914647487625, \"fold_epoch\": \"F2:E2\"}, {\"fold\": 2, \"epoch\": 3, \"metric\": \"F1\", \"value\": 0.5625422392430728, \"fold_epoch\": \"F2:E3\"}, {\"fold\": 2, \"epoch\": 4, \"metric\": \"F1\", \"value\": 0.6231274638633377, \"fold_epoch\": \"F2:E4\"}, {\"fold\": 2, \"epoch\": 5, \"metric\": \"F1\", \"value\": 0.6226144046919428, \"fold_epoch\": \"F2:E5\"}, {\"fold\": 2, \"epoch\": 6, \"metric\": \"F1\", \"value\": 0.6498168498168498, \"fold_epoch\": \"F2:E6\"}, {\"fold\": 2, \"epoch\": 7, \"metric\": \"F1\", \"value\": 0.7281245322556504, \"fold_epoch\": \"F2:E7\"}, {\"fold\": 2, \"epoch\": 8, \"metric\": \"F1\", \"value\": 0.7351760357611772, \"fold_epoch\": \"F2:E8\"}, {\"fold\": 2, \"epoch\": 9, \"metric\": \"F1\", \"value\": 0.7262313860252005, \"fold_epoch\": \"F2:E9\"}, {\"fold\": 3, \"epoch\": 0, \"metric\": \"F1\", \"value\": 0.4605855855855856, \"fold_epoch\": \"F3:E0\"}, {\"fold\": 3, \"epoch\": 1, \"metric\": \"F1\", \"value\": 0.4605855855855856, \"fold_epoch\": \"F3:E1\"}, {\"fold\": 3, \"epoch\": 2, \"metric\": \"F1\", \"value\": 0.5261814352723444, \"fold_epoch\": \"F3:E2\"}, {\"fold\": 3, \"epoch\": 3, \"metric\": \"F1\", \"value\": 0.5589746074820702, \"fold_epoch\": \"F3:E3\"}, {\"fold\": 3, \"epoch\": 4, \"metric\": \"F1\", \"value\": 0.5946923076923076, \"fold_epoch\": \"F3:E4\"}, {\"fold\": 3, \"epoch\": 5, \"metric\": \"F1\", \"value\": 0.6307549175970228, \"fold_epoch\": \"F3:E5\"}, {\"fold\": 3, \"epoch\": 6, \"metric\": \"F1\", \"value\": 0.6405417772490162, \"fold_epoch\": \"F3:E6\"}, {\"fold\": 3, \"epoch\": 7, \"metric\": \"F1\", \"value\": 0.7015407273139231, \"fold_epoch\": \"F3:E7\"}, {\"fold\": 3, \"epoch\": 8, \"metric\": \"F1\", \"value\": 0.6914336622807018, \"fold_epoch\": \"F3:E8\"}, {\"fold\": 3, \"epoch\": 9, \"metric\": \"F1\", \"value\": 0.7058411882319338, \"fold_epoch\": \"F3:E9\"}, {\"fold\": 4, \"epoch\": 0, \"metric\": \"F1\", \"value\": 0.4605855855855856, \"fold_epoch\": \"F4:E0\"}, {\"fold\": 4, \"epoch\": 1, \"metric\": \"F1\", \"value\": 0.4593679458239277, \"fold_epoch\": \"F4:E1\"}, {\"fold\": 4, \"epoch\": 2, \"metric\": \"F1\", \"value\": 0.5551009137463601, \"fold_epoch\": \"F4:E2\"}, {\"fold\": 4, \"epoch\": 3, \"metric\": \"F1\", \"value\": 0.5147565340717228, \"fold_epoch\": \"F4:E3\"}, {\"fold\": 4, \"epoch\": 4, \"metric\": \"F1\", \"value\": 0.5430806954710113, \"fold_epoch\": \"F4:E4\"}, {\"fold\": 4, \"epoch\": 5, \"metric\": \"F1\", \"value\": 0.6361683196249919, \"fold_epoch\": \"F4:E5\"}, {\"fold\": 4, \"epoch\": 6, \"metric\": \"F1\", \"value\": 0.6064369219212169, \"fold_epoch\": \"F4:E6\"}, {\"fold\": 4, \"epoch\": 7, \"metric\": \"F1\", \"value\": 0.643175060252277, \"fold_epoch\": \"F4:E7\"}, {\"fold\": 4, \"epoch\": 8, \"metric\": \"F1\", \"value\": 0.6449979901517435, \"fold_epoch\": \"F4:E8\"}, {\"fold\": 4, \"epoch\": 9, \"metric\": \"F1\", \"value\": 0.6542311598878249, \"fold_epoch\": \"F4:E9\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_data = df[['fold', 'epoch', 'overall_macro_precision', 'overall_macro_recall', 'overall_macro_f1']].melt(\n",
    "    id_vars=['fold', 'epoch'], var_name='metric', value_name='value'\n",
    ")\n",
    "metrics_data['metric'] = metrics_data['metric'].str.replace('overall_macro_', '').str.capitalize()\n",
    "metrics_data['fold_epoch'] = 'F' + metrics_data['fold'].astype(str) + ':E' + metrics_data['epoch'].astype(str)\n",
    "\n",
    "heatmap = alt.Chart(metrics_data).mark_rect().encode(\n",
    "    x='fold_epoch:O', y='metric:N',\n",
    "    color=alt.Color('value:Q', scale=alt.Scale(scheme='viridis')),\n",
    "    tooltip=['fold_epoch:N', 'metric:N', alt.Tooltip('value:Q', format='.4f')]\n",
    ").properties(width=700, height=150, title='Precision/Recall/F1 Heatmap')\n",
    "\n",
    "display(heatmap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
