{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a591c694-b0d9-466a-9a71-ea963bca43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 19:43:50.589620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 19:43:50.620227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 19:43:51.216850: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All random seeds set to 42\n",
      "training files: ['train_en.csv', 'train_it.csv', 'train_es.csv']\n",
      "Total training samples: 2988\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Overall:\n",
      "  Class 0 (NOT_RECLAMATORY): 2560 (85.7%)\n",
      "  Class 1 (RECLAMATORY): 428 (14.3%)\n",
      "  Total: 2988\n",
      "\n",
      "Per Language:\n",
      "  EN: Class 0=938, Class 1=88, Total=1026\n",
      "  ES: Class 0=743, Class 1=133, Total=876\n",
      "  IT: Class 0=879, Class 1=207, Total=1086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "XLMRobertaTokenizer,\n",
    "XLMRobertaForSequenceClassification,\n",
    "get_linear_schedule_with_warmup,\n",
    "AutoModelForSequenceClassification,\n",
    "AutoTokenizer\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import altair as alt\n",
    "\n",
    "from src.baseline.baseline import train_df, SEED\n",
    "from src.finetune.dataloader import MultilingualDataset, StratifiedMultilingualSplitter, DynamicUndersamplingSampler\n",
    "from src.finetune.train import train_epoch, validate\n",
    "from src.finetune.utils import calculate_metrics, calculate_weights\n",
    "from src.finetune.finetuner import run_train, run_inference\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3486e5-f6b9-4941-8a6e-bfdbad4ac706",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ff4b28-ff7e-42c0-9c22-e5db9500d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_1 = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "RUN_2 = \"../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model\"  # resultant model of 6_Finetune-LM-with-Optuna-for-MLM.ipynb\n",
    "\n",
    "RUN = RUN_2\n",
    "\n",
    "class BaseConfig:\n",
    "    MODEL_NAME = RUN  \n",
    "    NUM_LABELS = 2\n",
    "    MAX_LENGTH = 128\n",
    "    NUM_FROZEN_LAYERS = 10      \n",
    "    LEARNING_RATE = 5e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 10\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    WARMUP_RATIO = 0.15\n",
    "    PATIENCE = 3\n",
    "    EVAL_STRATEGY = \"epoch\"\n",
    "    N_SPLITS = 5\n",
    "    TRAIN_RATIO = 0.8\n",
    "    VAL_RATIO = 0.2\n",
    "    DYNAMIC_UNDERSAMPLE = False\n",
    "    MAX_MODELS_TO_SAVE = 2\n",
    "    OUTPUT_DIR = \"../fine_tuned_models/mlm\"\n",
    "    RESULTS_DIR = \"../results/fine_tuned_mlm/\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee6d3e-7f94-475f-acb9-1b2aeedc588a",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31805474-c8fe-4f46-9254-16e731f13391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(Config, num_frozen_layers: int = None):\n",
    "    if num_frozen_layers is None:\n",
    "        num_frozen_layers = Config.NUM_FROZEN_LAYERS\n",
    "    \n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        Config.MODEL_NAME, num_labels=Config.NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    model = model.to(Config.DEVICE)\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(model.classifier.dense.weight)\n",
    "    torch.nn.init.zeros_(model.classifier.dense.bias)\n",
    "    torch.nn.init.xavier_uniform_(model.classifier.out_proj.weight)\n",
    "    torch.nn.init.zeros_(model.classifier.out_proj.bias)\n",
    "    \n",
    "    for param in model.roberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_total_layers = len(model.roberta.encoder.layer)\n",
    "    for idx in range(min(num_frozen_layers, num_total_layers)):\n",
    "        for param in model.roberta.encoder.layer[idx].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if num_frozen_layers >= num_total_layers and model.roberta.pooler is not None:\n",
    "        for param in model.roberta.pooler.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    print(f\"Froze: Embeddings + First {min(num_frozen_layers, num_total_layers)} Encoder Layers\")\n",
    "    print(\"Trainable: Classification Head + Remaining Encoder Layers\")\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(\n",
    "        f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032c152-4cd0-4c25-87c6-f36e5010ccc6",
   "metadata": {},
   "source": [
    "# Model Storage Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d306da42-c538-4ed6-b9db-1c5751ef65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpointManager:\n",
    "    def init(self, max_models: int = 2, output_dir: str = \"./models\"):\n",
    "        self.max_models = max_models\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.best_models = []\n",
    "\n",
    "    def save_checkpoint(self, model, score: float, epoch: int, fold: int) -> bool:\n",
    "        checkpoint_name = f\"fold_{fold}_epoch_{epoch}_f1_{score:.4f}.pt\"\n",
    "        checkpoint_path = self.output_dir / checkpoint_name\n",
    "    \n",
    "        self.best_models.append((score, checkpoint_path, epoch, fold))\n",
    "        self.best_models.sort(reverse=True, key=lambda x: x)\n",
    "    \n",
    "        if len(self.best_models) > self.max_models:\n",
    "            worst_score, worst_path, worst_epoch, worst_fold = self.best_models.pop()\n",
    "            if worst_path.exists():\n",
    "                worst_path.unlink()\n",
    "                logger.info(f\"Deleted checkpoint: {worst_path}\")\n",
    "    \n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        logger.info(\n",
    "            f\"Saved checkpoint: {checkpoint_path} (F1: {score:.4f}, Fold: {fold}, Epoch: {epoch})\"\n",
    "        )\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    def get_best_models(self) -> List[Tuple]:\n",
    "        return [(score, path, epoch, fold) for score, path, epoch, fold in self.best_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afff1c-8355-446f-9c9f-4122e48f2dbb",
   "metadata": {},
   "source": [
    "# Single Fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb01670-6131-41d5-b76e-f24782a5281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_fold(train_df: pd.DataFrame, val_df: pd.DataFrame, Config, fold_id: int = 0, trial_id: int = 0):\n",
    "    try:\n",
    "        model, tokenizer = setup_model(Config, num_frozen_layers=Config.NUM_FROZEN_LAYERS)\n",
    "    \n",
    "        label_weights, language_weights, pos_weight = calculate_weights(train_df)\n",
    "    \n",
    "        train_dataset = MultilingualDataset(\n",
    "            texts=train_df[\"text\"].tolist(),\n",
    "            labels=train_df[\"label\"].tolist(),\n",
    "            languages=train_df[\"lang\"].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=Config.MAX_LENGTH,\n",
    "        )\n",
    "    \n",
    "        val_dataset = MultilingualDataset(\n",
    "            texts=val_df[\"text\"].tolist(),\n",
    "            labels=val_df[\"label\"].tolist(),\n",
    "            languages=val_df[\"lang\"].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=Config.MAX_LENGTH,\n",
    "        )\n",
    "    \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=Config.WEIGHT_DECAY,\n",
    "        )\n",
    "    \n",
    "        balanced_train_size = 3 * len(train_df[train_df[\"label\"] == 1])\n",
    "        total_steps = (\n",
    "            balanced_train_size\n",
    "            // (Config.BATCH_SIZE * Config.GRADIENT_ACCUMULATION_STEPS)\n",
    "            * Config.NUM_EPOCHS\n",
    "        )\n",
    "        warmup_steps = int(Config.WARMUP_RATIO * total_steps)\n",
    "    \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "    \n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        fold_results = []\n",
    "    \n",
    "        for epoch in range(Config.NUM_EPOCHS):\n",
    "            \n",
    "            if Config.DYNAMIC_UNDERSAMPLE:\n",
    "                undersampler = DynamicUndersamplingSampler(\n",
    "                    train_df, minority_class=1, seed=SEED\n",
    "                )\n",
    "                balanced_indices = undersampler.get_balanced_indices(epoch)\n",
    "                train_subset_df = train_df.iloc[balanced_indices].reset_index(drop=True)\n",
    "            else:\n",
    "                train_subset_df = train_df\n",
    "    \n",
    "            train_dataset_epoch = MultilingualDataset(\n",
    "                texts=train_subset_df[\"text\"].tolist(),\n",
    "                labels=train_subset_df[\"label\"].tolist(),\n",
    "                languages=train_subset_df[\"lang\"].tolist(),\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "            )\n",
    "    \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset_epoch,\n",
    "                batch_size=Config.BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "            )\n",
    "    \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=Config.BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "            )\n",
    "    \n",
    "            train_loss = train_epoch(\n",
    "                model,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                label_weights,\n",
    "                language_weights,\n",
    "                pos_weight,\n",
    "                Config\n",
    "            )\n",
    "    \n",
    "            val_loss, val_preds, val_labels, val_languages = validate(\n",
    "                model, val_loader, Config\n",
    "            )\n",
    "    \n",
    "            val_metrics = calculate_metrics(val_preds, val_labels, val_languages)\n",
    "            current_f1 = val_metrics[\"overall\"][\"macro_f1\"]\n",
    "            \n",
    "            print(f\"Trial {trial_id}, Fold {fold_id}, Epoch {epoch+1}: F1={current_f1:.4f}\")\n",
    "    \n",
    "            if current_f1 > best_val_f1:\n",
    "                best_val_f1 = current_f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= Config.PATIENCE:\n",
    "                    logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "            fold_results.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": current_f1\n",
    "            })\n",
    "    \n",
    "        return best_val_f1, fold_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {fold_id}: {str(e)}\")\n",
    "        return 0.0, []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef37b79-e480-4720-bf27-be7db6754e0b",
   "metadata": {},
   "source": [
    "# Optuna Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dda42a-a547-470f-b92f-e72511ce238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial, train_df: pd.DataFrame, base_config: BaseConfig):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 12)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Trial {trial.number}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Learning Rate: {learning_rate:.2e}\")\n",
    "    print(f\"Weight Decay: {weight_decay:.4f}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Num Epochs: {num_epochs}\")\n",
    "    \n",
    "    class TrialConfig(base_config.__class__):\n",
    "        LEARNING_RATE = learning_rate\n",
    "        WEIGHT_DECAY = weight_decay\n",
    "        BATCH_SIZE = batch_size\n",
    "        NUM_EPOCHS = num_epochs\n",
    "        OUTPUT_DIR = f\"{base_config.OUTPUT_DIR}/trial_{trial.number}\"\n",
    "    \n",
    "    trial_config = TrialConfig()\n",
    "    \n",
    "    fold_scores = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "        \n",
    "        fold_train = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        fold_val = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n  Fold {fold_idx+1}/5: Train={len(fold_train)}, Val={len(fold_val)}\")\n",
    "        \n",
    "        try:\n",
    "            fold_f1, fold_results = train_single_fold(\n",
    "                fold_train, \n",
    "                fold_val, \n",
    "                trial_config, \n",
    "                fold_id=fold_idx,\n",
    "                trial_id=trial.number\n",
    "            )\n",
    "            \n",
    "            fold_scores.append(fold_f1)\n",
    "            print(f\"  Fold {fold_idx+1} F1: {fold_f1:.4f}\")\n",
    "            \n",
    "            avg_so_far = np.mean(fold_scores)\n",
    "            trial.report(avg_so_far, fold_idx)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                print(f\"  Trial pruned at fold {fold_idx}\")\n",
    "                raise optuna.TrialPruned()\n",
    "                \n",
    "        except optuna.TrialPruned:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fold {fold_idx} failed: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    avg_f1 = np.mean(fold_scores)\n",
    "    print(f\"\\nTrial {trial.number} - Average F1: {avg_f1:.4f}\")\n",
    "    print(f\"Fold scores: {[f'{f:.4f}' for f in fold_scores]}\\n\")\n",
    "    \n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26c9e7-e0ea-4757-95b1-c6a4c67a733f",
   "metadata": {},
   "source": [
    "# Visulize Optuna Parameter Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6723649-d8b4-4676-8e6a-8f169984fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optuna_results(study, output_dir: str = \"../figures\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data\n",
    "    trial_data = []\n",
    "    for trial in study.trials:\n",
    "        if trial.value is not None:\n",
    "            trial_data.append({\n",
    "                \"trial\": trial.number,\n",
    "                \"f1\": trial.value,\n",
    "                \"lr\": trial.params.get(\"learning_rate\", 0),\n",
    "                \"batch_size\": trial.params.get(\"batch_size\", 0),\n",
    "                \"epochs\": trial.params.get(\"num_epochs\", 0),\n",
    "                \"weight_decay\": trial.params.get(\"weight_decay\", 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(trial_data)\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # Plot 1: Optimization History\n",
    "    opt_history = alt.Chart(df).mark_line(point=True).encode(\n",
    "        x=alt.X(\"trial:Q\", title=\"Trial Number\"),\n",
    "        y=alt.Y(\"f1:Q\", title=\"F1 Score\"),\n",
    "        tooltip=[\"trial:Q\", alt.Tooltip(\"f1:Q\", format=\".4f\")]\n",
    "    ).properties(\n",
    "        width=700, height=400, title=\"Optimization History: F1 Score Over Trials\"\n",
    "    ).interactive()\n",
    "    \n",
    "    opt_history_path = f\"{output_dir}/optuna_optimization_history.svg\"\n",
    "    opt_history.save(opt_history_path)\n",
    "    print(f\"Saved: optuna_optimization_history.svg\")\n",
    "    \n",
    "    # Plot 2: Learning Rate Impact\n",
    "    lr_plot = alt.Chart(df).mark_circle(size=100).encode(\n",
    "        x=alt.X(\"lr:Q\", title=\"Learning Rate\", scale=alt.Scale(type=\"log\")),\n",
    "        y=alt.Y(\"f1:Q\", title=\"F1 Score\"),\n",
    "        color=alt.Color(\"epochs:O\", title=\"Num Epochs\"),\n",
    "        tooltip=[\"trial:Q\", alt.Tooltip(\"f1:Q\", format=\".4f\"), \"lr:Q\", \"epochs:O\"]\n",
    "    ).properties(\n",
    "        width=600, height=400, title=\"Learning Rate vs F1 Score\"\n",
    "    ).interactive()\n",
    "    \n",
    "    lr_path = f\"{output_dir}/optuna_learning_rate_impact.svg\"\n",
    "    lr_plot.save(lr_path)\n",
    "    print(f\"Saved: optuna_learning_rate_impact.svg\")\n",
    "    \n",
    "    # Plot 3: Batch Size Impact\n",
    "    bs_plot = alt.Chart(df).mark_boxplot().encode(\n",
    "        x=alt.X(\"batch_size:O\", title=\"Batch Size\"),\n",
    "        y=alt.Y(\"f1:Q\", title=\"F1 Score\"),\n",
    "        tooltip=[\"trial:Q\", alt.Tooltip(\"f1:Q\", format=\".4f\")]\n",
    "    ).properties(\n",
    "        width=500, height=400, title=\"Batch Size vs F1 Score\"\n",
    "    )\n",
    "    \n",
    "    bs_path = f\"{output_dir}/optuna_batch_size_impact.svg\"\n",
    "    bs_plot.save(bs_path)\n",
    "    print(f\"Saved: optuna_batch_size_impact.svg\")\n",
    "    \n",
    "    # Plot 4: Weight Decay Impact\n",
    "    wd_plot = alt.Chart(df).mark_circle(size=100).encode(\n",
    "        x=alt.X(\"weight_decay:Q\", title=\"Weight Decay\"),\n",
    "        y=alt.Y(\"f1:Q\", title=\"F1 Score\"),\n",
    "        color=alt.Color(\"f1:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        tooltip=[\"trial:Q\", alt.Tooltip(\"f1:Q\", format=\".4f\"), \"weight_decay:Q\"]\n",
    "    ).properties(\n",
    "        width=600, height=400, title=\"Weight Decay vs F1 Score\"\n",
    "    ).interactive()\n",
    "    \n",
    "    wd_path = f\"{output_dir}/optuna_weight_decay_impact.svg\"\n",
    "    wd_plot.save(wd_path)\n",
    "    print(f\"Saved: optuna_weight_decay_impact.svg\")\n",
    "    \n",
    "    # Plot 5: Epochs Impact\n",
    "    epochs_plot = alt.Chart(df).mark_boxplot().encode(\n",
    "        x=alt.X(\"epochs:O\", title=\"Number of Epochs\"),\n",
    "        y=alt.Y(\"f1:Q\", title=\"F1 Score\"),\n",
    "        tooltip=[\"trial:Q\", alt.Tooltip(\"f1:Q\", format=\".4f\")]\n",
    "    ).properties(\n",
    "        width=600, height=400, title=\"Number of Epochs vs F1 Score\"\n",
    "    )\n",
    "    \n",
    "    epochs_path = f\"{output_dir}/optuna_epochs_impact.svg\"\n",
    "    epochs_plot.save(epochs_path)\n",
    "    print(f\"Saved: optuna_epochs_impact.svg\")\n",
    "    \n",
    "    # Save trials dataframe\n",
    "    trials_csv = f\"{output_dir}/optuna_trials.csv\"\n",
    "    df.to_csv(trials_csv, index=False)\n",
    "    print(f\"Saved: optuna_trials.csv\")\n",
    "    \n",
    "    print(f\"\\nAll visualizations saved to: {output_dir}\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd678878-2cf7-484e-95e3-f3ce3ef34c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_optimization(train_df: pd.DataFrame, n_trials: int = 10):\n",
    "    base_config = BaseConfig()\n",
    "\n",
    "    print(\"\\n\" + \" \"*80)\n",
    "    print(\"STEP 1: OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\" \"*80)\n",
    "    print(f\"Number of trials: {n_trials}\")\n",
    "    print(f\"CV folds per trial: 5\")\n",
    "    print(f\"Total model trainings: {n_trials * 5}\")\n",
    "    print(\" \"*80 + \"\\n\")\n",
    "    \n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    pruner = MedianPruner(n_startup_trials=2, n_warmup_steps=10)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"classification_optimization\"\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: optuna_objective(trial, train_df, base_config),\n",
    "        n_trials=n_trials,\n",
    "        gc_after_trial=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST TRIAL\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Trial: {best_trial.number}\")\n",
    "    print(f\"Best F1 Score: {best_trial.value:.4f}\")\n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\n\" + \" \"*80)\n",
    "    print(\" STEP 2: VISUALIZE RESULTS\")\n",
    "    print(\" \"*80 + \"\\n\")\n",
    "    \n",
    "    trial_df = visualize_optuna_results(study)\n",
    "    \n",
    "    print(f\"Top 5 Trials:\")\n",
    "    print(trial_df.nlargest(5, \"f1\")[[\"trial\", \"f1\", \"lr\", \"batch_size\", \"epochs\"]])\n",
    "    \n",
    "    return study, best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79221b-6b31-49fc-a5d4-1ea828f27007",
   "metadata": {},
   "source": [
    "# Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f260b27-fc73-44e0-bb49-cedfc55284b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(best_trial, train_df: pd.DataFrame, base_config: BaseConfig):\n",
    "    print(\"\\n\" + \" \"*80)\n",
    "    print(\"STEP 3: TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
    "    print(\" \"*80)\n",
    "    \n",
    "    class FinalConfig(BaseConfig):\n",
    "        LEARNING_RATE = best_trial.params[\"learning_rate\"]\n",
    "        WEIGHT_DECAY = best_trial.params[\"weight_decay\"]\n",
    "        BATCH_SIZE = best_trial.params[\"batch_size\"]\n",
    "        NUM_EPOCHS = best_trial.params[\"num_epochs\"]\n",
    "        OUTPUT_DIR = f\"{base_config.OUTPUT_DIR}/final_model\"\n",
    "        RESULTS_DIR = f\"{base_config.RESULTS_DIR}final_model/\"\n",
    "    \n",
    "    final_config = FinalConfig()\n",
    "    \n",
    "    print(f\"\\nFinal Training Configuration:\")\n",
    "    print(f\"Learning Rate: {final_config.LEARNING_RATE:.2e}\")\n",
    "    print(f\"Weight Decay: {final_config.WEIGHT_DECAY:.4f}\")\n",
    "    print(f\"Batch Size: {final_config.BATCH_SIZE}\")\n",
    "    print(f\"Num Epochs: {final_config.NUM_EPOCHS}\\n\")\n",
    "    \n",
    "    final_model_path = run_train(train_df, final_config)\n",
    "    \n",
    "    return final_config, final_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1053add-195b-49e6-8c77-f529563e7ad6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84409907-2a56-4e27-8e17-5a629ad347b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    NUM_LABELS = 2\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CHECKPOINT_PATH = None # Will be set from best model\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "def run_inference(df: pd.DataFrame, config: InferenceConfig) -> Dict:\n",
    "    print(f\"Running inference on {len(df)} samples...\")\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        num_labels=config.NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(config.CHECKPOINT_PATH, map_location=config.DEVICE)\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"model_state_dict\"]\n",
    "        elif \"state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Loaded checkpoint from: {config.CHECKPOINT_PATH}\")\n",
    "    \n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    languages = df[\"lang\"].tolist()\n",
    "    \n",
    "    dataset = TestDataset(texts, tokenizer, config.MAX_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(config.DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n",
    "    \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "    \n",
    "    metrics = calculate_metrics(all_preds, labels, languages)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INFERENCE RESULTS ON TRAINING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"Macro Precision:{metrics['overall']['macro_precision']:.4f}\")\n",
    "    print(f\"Macro Recall:{metrics['overall']['macro_recall']:.4f}\")\n",
    "    print(f\"Macro F1:{metrics['overall']['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Language Metrics:\")\n",
    "    for lang in sorted([k for k in metrics.keys() if k != \"overall\"]):\n",
    "        print(f\"{lang.upper()}:\")\n",
    "        print(f\"Precision:{metrics[lang]['macro_precision']:.4f}\")\n",
    "        print(f\"Recall:{metrics[lang]['macro_recall']:.4f}\")\n",
    "        print(f\"F1:{metrics[lang]['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": all_preds,\n",
    "        \"probabilities\": all_probs,\n",
    "        \"labels\": labels,\n",
    "        \"languages\": languages,\n",
    "        \"metrics\": metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c839221-3f39-4bc6-8987-4f5e3d02f3c9",
   "metadata": {},
   "source": [
    "# Actual Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad529596-08b5-41d0-919b-001188336f3c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cf486a-ce37-4756-be70-9bff77a5c46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 5) (5976, 8)\n"
     ]
    }
   ],
   "source": [
    "original_data = train_df\n",
    "augmented_data = pd.read_csv(\"../data/augmented_multilingual_tweets.csv\")\n",
    "print(original_data.shape, augmented_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71c8d5b-44c1-4ae3-b073-405208dbecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8964, 5)\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.concat([original_data, augmented_data[list(original_data.columns)]], ignore_index=True)\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76632c-ab43-41e4-bac7-f26137435bce",
   "metadata": {},
   "source": [
    "### Optuna Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92bb3d27-ac3e-4167-9dfa-a23a84ff54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-04 19:44:58,211] A new study created in memory with name: classification_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "STEP 1: OPTUNA HYPERPARAMETER OPTIMIZATION\n",
      "                                                                                \n",
      "Number of trials: 10\n",
      "CV folds per trial: 5\n",
      "Total model trainings: 50\n",
      "                                                                                \n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 0\n",
      "================================================================================\n",
      "Learning Rate: 4.33e-05\n",
      "Weight Decay: 0.0951\n",
      "Batch Size: 8\n",
      "Num Epochs: 6\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:44:59,762 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:44:59,762 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 19:44:59,762 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.06it/s, loss=0.137]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.66it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.97it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.33it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 4: F1=0.4766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.60it/s, loss=0.11]  \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 5: F1=0.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.16it/s, loss=0.106]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 107.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 0, Epoch 6: F1=0.5855\n",
      "  Fold 1 F1: 0.5855\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:46:26,999 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:46:26,999 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 19:46:26,999 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.85it/s, loss=0.224]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 1: F1=0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.64it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.34it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.80it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 4: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.38it/s, loss=0.11] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 107.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 5: F1=0.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.85it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1, Epoch 6: F1=0.5992\n",
      "  Fold 2 F1: 0.5992\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:47:54,412 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:47:54,412 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 19:47:54,412 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.41it/s, loss=0.57] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 108.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.33it/s, loss=0.22] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 110.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.49it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 106.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.11it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 108.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 4: F1=0.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.82it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 111.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 5: F1=0.5546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.45it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 104.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 2, Epoch 6: F1=0.6148\n",
      "  Fold 3 F1: 0.6148\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:49:22,509 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:49:22,510 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 19:49:22,510 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.69it/s, loss=0.373]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 106.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.88it/s, loss=0.175]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 71.56it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 110.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.57it/s, loss=0.119]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 108.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 4: F1=0.4924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.53it/s, loss=0.113]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 5: F1=0.6592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.33it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 3, Epoch 6: F1=0.5981\n",
      "  Fold 4 F1: 0.6592\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:50:49,837 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 19:50:49,837 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 19:50:49,837 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.20it/s, loss=0.17] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 4, Epoch 1: F1=0.5353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:13<00:00, 67.51it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 224/224 [00:02<00:00, 110.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:13<00:00, 67.55it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 224/224 [00:02<00:00, 110.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 4, Epoch 3: F1=0.4696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 69.72it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.67it/s]\n",
      "[I 2025-12-04 19:51:49,604] Trial 0 finished with value: 0.49174287418015855 and parameters: {'learning_rate': 4.3284502212938785e-05, 'weight_decay': 0.09507143064099162, 'batch_size': 8, 'num_epochs': 6}. Best is trial 0 with value: 0.49174287418015855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 4, Epoch 4: F1=0.4996\n",
      "Error in fold 4: name 'logger' is not defined\n",
      "  Fold 5 F1: 0.0000\n",
      "\n",
      "Trial 0 - Average F1: 0.4917\n",
      "Fold scores: ['0.5855', '0.5992', '0.6148', '0.6592', '0.0000']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 1\n",
      "================================================================================\n",
      "Learning Rate: 1.26e-05\n",
      "Weight Decay: 0.0866\n",
      "Batch Size: 16\n",
      "Num Epochs: 12\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:51:50,435 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:51:50,435 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 19:51:50,435 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.19it/s, loss=0.262]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 0, Epoch 1: F1=0.1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:10<00:00, 43.47it/s, loss=0.218]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 0, Epoch 2: F1=0.5405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.91it/s, loss=0.162]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.43it/s, loss=0.154]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 0, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.50it/s, loss=0.144]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 67.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 0, Epoch 5: F1=0.4614\n",
      "Error in fold 0: name 'logger' is not defined\n",
      "  Fold 1 F1: 0.0000\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:52:48,523 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:52:48,524 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 19:52:48,524 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.46it/s, loss=0.14] \n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 1, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.70it/s, loss=0.138]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.37it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 66.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 1, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.63it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 1, Epoch 4: F1=0.4614\n",
      "Error in fold 1: name 'logger' is not defined\n",
      "  Fold 2 F1: 0.0000\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:53:34,866 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:53:34,867 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 19:53:34,867 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.81it/s, loss=0.214]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 2, Epoch 1: F1=0.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.81it/s, loss=0.174]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 2, Epoch 2: F1=0.4647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.21it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.22it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 2, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.88it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 2, Epoch 5: F1=0.4614\n",
      "Error in fold 2: name 'logger' is not defined\n",
      "  Fold 3 F1: 0.0000\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:54:31,878 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:54:31,878 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 19:54:31,878 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.77it/s, loss=0.417]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.57it/s, loss=0.319]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 67.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 2: F1=0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.22it/s, loss=0.16] \n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.09it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.40it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 65.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 5: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.84it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 3, Epoch 6: F1=0.4614\n",
      "Error in fold 3: name 'logger' is not defined\n",
      "  Fold 4 F1: 0.0000\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:55:40,900 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 19:55:40,901 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 19:55:40,901 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.75it/s, loss=0.15] \n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 68.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 4, Epoch 1: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.31it/s, loss=0.145]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 67.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.22it/s, loss=0.143]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 68.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 4, Epoch 3: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.03it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 65.80it/s]\n",
      "[I 2025-12-04 19:56:26,571] Trial 1 finished with value: 0.0 and parameters: {'learning_rate': 1.2551115172973821e-05, 'weight_decay': 0.08661761457749352, 'batch_size': 16, 'num_epochs': 12}. Best is trial 0 with value: 0.49174287418015855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 4, Epoch 4: F1=0.4615\n",
      "Error in fold 4: name 'logger' is not defined\n",
      "  Fold 5 F1: 0.0000\n",
      "\n",
      "Trial 1 - Average F1: 0.0000\n",
      "Fold scores: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 2\n",
      "================================================================================\n",
      "Learning Rate: 2.60e-04\n",
      "Weight Decay: 0.0212\n",
      "Batch Size: 32\n",
      "Num Epochs: 9\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:56:27,376 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:56:27,376 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 19:56:27,376 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.36it/s, loss=0.14] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 1: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.24it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 2: F1=0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.30it/s, loss=0.102] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 3: F1=0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.61it/s, loss=0.0907]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 4: F1=0.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.17it/s, loss=0.0848]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 5: F1=0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 27.29it/s, loss=0.0766]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 6: F1=0.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 27.85it/s, loss=0.0677]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 7: F1=0.7597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.32it/s, loss=0.0611]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 8: F1=0.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.13it/s, loss=0.0576]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 0, Epoch 9: F1=0.7833\n",
      "  Fold 1 F1: 0.7870\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:57:53,427 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:57:53,427 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 19:57:53,428 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.484]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 28.04it/s, loss=0.139]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 2: F1=0.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.73it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 3: F1=0.6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.74it/s, loss=0.0933]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 4: F1=0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.72it/s, loss=0.0838]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 5: F1=0.7049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.70it/s, loss=0.0766]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 6: F1=0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.71it/s, loss=0.0686]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 7: F1=0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.78it/s, loss=0.0643]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 8: F1=0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.67it/s, loss=0.0593]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 1, Epoch 9: F1=0.8113\n",
      "  Fold 2 F1: 0.8113\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 19:59:17,970 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 19:59:17,971 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 19:59:17,971 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 28.05it/s, loss=0.219]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 1: F1=0.3707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.14it/s, loss=0.116]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 2: F1=0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.101] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 3: F1=0.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.48it/s, loss=0.0923]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 4: F1=0.7175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.64it/s, loss=0.0834]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 5: F1=0.7340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.0761]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 6: F1=0.7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.73it/s, loss=0.0685]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 7: F1=0.7624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.75it/s, loss=0.0616]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 8: F1=0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.72it/s, loss=0.0566]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 2, Epoch 9: F1=0.7188\n",
      "  Fold 3 F1: 0.7705\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:00:42,794 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:00:42,795 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:00:42,795 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.62it/s, loss=0.575]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.144]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 2: F1=0.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.72it/s, loss=0.104]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 3: F1=0.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.0947]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 4: F1=0.7076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.0847]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 5: F1=0.7107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 27.84it/s, loss=0.0782]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 37.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 6: F1=0.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.13it/s, loss=0.0742]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 7: F1=0.6799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.53it/s, loss=0.0679]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 8: F1=0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.49it/s, loss=0.0573]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 3, Epoch 9: F1=0.8177\n",
      "  Fold 4 F1: 0.8177\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:02:07,837 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:02:07,837 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:02:07,837 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.48it/s, loss=0.304]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 1: F1=0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.50it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 2: F1=0.5057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.19it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 3: F1=0.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.35it/s, loss=0.0917]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 4: F1=0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.45it/s, loss=0.083] \n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 5: F1=0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.29it/s, loss=0.0777]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 6: F1=0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.41it/s, loss=0.0716]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 37.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 7: F1=0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.28it/s, loss=0.0626]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 8: F1=0.7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.50it/s, loss=0.0565]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.51it/s]\n",
      "[I 2025-12-04 20:03:32,428] Trial 2 finished with value: 0.7918504359347013 and parameters: {'learning_rate': 0.00025959425503112657, 'weight_decay': 0.021233911067827616, 'batch_size': 32, 'num_epochs': 9}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 4, Epoch 9: F1=0.7424\n",
      "  Fold 5 F1: 0.7728\n",
      "\n",
      "Trial 2 - Average F1: 0.7919\n",
      "Fold scores: ['0.7870', '0.8113', '0.7705', '0.8177', '0.7728']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 3\n",
      "================================================================================\n",
      "Learning Rate: 5.42e-05\n",
      "Weight Decay: 0.0291\n",
      "Batch Size: 8\n",
      "Num Epochs: 7\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:03:33,274 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:03:33,274 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:03:33,274 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.27it/s, loss=0.163]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 110.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 1: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.95it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 111.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.00it/s, loss=0.117]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.65it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 4: F1=0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.02it/s, loss=0.105] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 108.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 5: F1=0.5216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.46it/s, loss=0.104] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 6: F1=0.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.00it/s, loss=0.1]  \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 0, Epoch 7: F1=0.6356\n",
      "  Fold 1 F1: 0.6356\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:05:13,369 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:05:13,370 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:05:13,370 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.86it/s, loss=0.435]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.06it/s, loss=0.182]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.64it/s, loss=0.136]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 110.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 3: F1=0.4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.95it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 109.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 4: F1=0.5381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 73.51it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 5: F1=0.6219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.56it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 6: F1=0.6253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.54it/s, loss=0.107] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1, Epoch 7: F1=0.6953\n",
      "  Fold 2 F1: 0.6953\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:06:53,210 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:06:53,211 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:06:53,211 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.167]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.52it/s, loss=0.165]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.55it/s, loss=0.149]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 4: F1=0.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.50it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 5: F1=0.5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.112]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 6: F1=0.5863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.52it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 2, Epoch 7: F1=0.6393\n",
      "  Fold 3 F1: 0.6393\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:08:32,194 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:08:32,195 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:08:32,195 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.52it/s, loss=0.14] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.59it/s, loss=0.141]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.56it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.41it/s, loss=0.117]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 4: F1=0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.67it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 5: F1=0.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.40it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 6: F1=0.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.106] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 3, Epoch 7: F1=0.6262\n",
      "  Fold 4 F1: 0.6397\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:10:11,137 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:10:11,138 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:10:11,138 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.49it/s, loss=0.4]  \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 113.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 1: F1=0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.56it/s, loss=0.173]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.67it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 3: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.43it/s, loss=0.117]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 113.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 4: F1=0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.47it/s, loss=0.109]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 113.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 5: F1=0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.55it/s, loss=0.109]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 113.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 6: F1=0.6139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.55it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 113.28it/s]\n",
      "[I 2025-12-04 20:11:49,369] Trial 3 finished with value: 0.6484322783086813 and parameters: {'learning_rate': 5.4182823195332406e-05, 'weight_decay': 0.029122914019804193, 'batch_size': 8, 'num_epochs': 7}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 4, Epoch 7: F1=0.6322\n",
      "  Fold 5 F1: 0.6322\n",
      "\n",
      "Trial 3 - Average F1: 0.6484\n",
      "Fold scores: ['0.6356', '0.6953', '0.6393', '0.6397', '0.6322']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 4\n",
      "================================================================================\n",
      "Learning Rate: 5.95e-05\n",
      "Weight Decay: 0.0785\n",
      "Batch Size: 32\n",
      "Num Epochs: 5\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:11:50,190 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:11:50,190 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:11:50,190 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.68it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 0, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.57it/s, loss=0.11] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 0, Epoch 3: F1=0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.102]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 0, Epoch 4: F1=0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 28.09it/s, loss=0.0943]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 0, Epoch 5: F1=0.6342\n",
      "  Fold 1 F1: 0.6342\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:12:37,733 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:12:37,733 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:12:37,733 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.44it/s, loss=0.182]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1, Epoch 1: F1=0.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.51it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1, Epoch 2: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.49it/s, loss=0.107]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1, Epoch 3: F1=0.5339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.56it/s, loss=0.1]   \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1, Epoch 4: F1=0.6538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.51it/s, loss=0.0963]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1, Epoch 5: F1=0.6574\n",
      "  Fold 2 F1: 0.6574\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:13:25,204 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:13:25,204 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:13:25,204 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.50it/s, loss=0.145]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 2, Epoch 1: F1=0.4611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.57it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.52it/s, loss=0.109]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 2, Epoch 3: F1=0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.46it/s, loss=0.0997]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 2, Epoch 4: F1=0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.52it/s, loss=0.094] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 2, Epoch 5: F1=0.6110\n",
      "  Fold 3 F1: 0.6110\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:14:12,695 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:14:12,695 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:14:12,695 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.53it/s, loss=0.176]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 3, Epoch 1: F1=0.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.50it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.51it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 3, Epoch 3: F1=0.5407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.101] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 3, Epoch 4: F1=0.5660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.47it/s, loss=0.0944]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 3, Epoch 5: F1=0.5755\n",
      "  Fold 4 F1: 0.5755\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:15:00,141 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:15:00,142 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:15:00,142 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.51it/s, loss=0.137]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 4, Epoch 1: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.54it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.54it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 4, Epoch 3: F1=0.4962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 4, Epoch 4: F1=0.5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.42it/s, loss=0.0979]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.02it/s]\n",
      "[I 2025-12-04 20:15:46,871] Trial 4 finished with value: 0.6154495927977643 and parameters: {'learning_rate': 5.954553793888986e-05, 'weight_decay': 0.07851759613930137, 'batch_size': 32, 'num_epochs': 5}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 4, Epoch 5: F1=0.5992\n",
      "  Fold 5 F1: 0.5992\n",
      "\n",
      "Trial 4 - Average F1: 0.6154\n",
      "Fold scores: ['0.6342', '0.6574', '0.6110', '0.5755', '0.5992']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 5\n",
      "================================================================================\n",
      "Learning Rate: 1.08e-04\n",
      "Weight Decay: 0.0171\n",
      "Batch Size: 32\n",
      "Num Epochs: 11\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:15:47,701 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:15:47,701 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:15:47,701 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.59it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 1: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.56it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.60it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 3: F1=0.4727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.102]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 4: F1=0.5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.62it/s, loss=0.0975]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 5: F1=0.6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.62it/s, loss=0.0897]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 6: F1=0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.60it/s, loss=0.0826]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 7: F1=0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.57it/s, loss=0.0814]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 8: F1=0.6585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.58it/s, loss=0.077] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 9: F1=0.7201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.46it/s, loss=0.0744]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 10: F1=0.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.49it/s, loss=0.0675]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 0, Epoch 11: F1=0.7396\n",
      "  Fold 1 F1: 0.7396\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:17:31,027 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:17:31,028 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:17:31,028 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.53it/s, loss=0.147]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 1: F1=0.4784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.61it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.54it/s, loss=0.112]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 3: F1=0.4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.57it/s, loss=0.102]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 4: F1=0.5381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.58it/s, loss=0.0974]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 5: F1=0.6493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.59it/s, loss=0.0906]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 6: F1=0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.0863]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 7: F1=0.7184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.0847]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 8: F1=0.7590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.53it/s, loss=0.0782]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 9: F1=0.7587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.25it/s, loss=0.0744]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 10: F1=0.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.44it/s, loss=0.0699]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 1, Epoch 11: F1=0.7521\n",
      "Error in fold 1: name 'logger' is not defined\n",
      "  Fold 2 F1: 0.0000\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:19:14,578 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:19:14,578 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:19:14,579 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.30it/s, loss=0.478]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.56it/s, loss=0.156]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 2: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.56it/s, loss=0.109]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 3: F1=0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 4: F1=0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.67it/s, loss=0.0943]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 5: F1=0.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.0893]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 6: F1=0.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.0853]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 7: F1=0.6803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.55it/s, loss=0.0793]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 8: F1=0.7370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.0752]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 9: F1=0.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.60it/s, loss=0.0708]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 10: F1=0.7526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.62it/s, loss=0.0673]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 2, Epoch 11: F1=0.8030\n",
      "  Fold 3 F1: 0.8030\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:20:57,873 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:20:57,873 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:20:57,873 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.29] \n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 1: F1=0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.157]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.56it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 3: F1=0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.63it/s, loss=0.106]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 4: F1=0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.41it/s, loss=0.0991]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 5: F1=0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 28.03it/s, loss=0.0944]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 6: F1=0.6448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:08<00:00, 28.01it/s, loss=0.0868]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 38.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 7: F1=0.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.59it/s, loss=0.0827]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 8: F1=0.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.0786]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 9: F1=0.7477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.67it/s, loss=0.0726]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 10: F1=0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.64it/s, loss=0.0693]\n",
      "Validating: 100%|██████████| 57/57 [00:01<00:00, 39.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 3, Epoch 11: F1=0.7824\n",
      "  Fold 4 F1: 0.7824\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:22:41,523 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:22:41,523 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:22:41,523 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.186]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 1: F1=0.5113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.67it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 3: F1=0.4959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.64it/s, loss=0.101] \n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 4: F1=0.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.64it/s, loss=0.096] \n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 5: F1=0.6478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.65it/s, loss=0.0916]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 6: F1=0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.64it/s, loss=0.0898]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 7: F1=0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.68it/s, loss=0.0822]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 8: F1=0.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.0755]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 9: F1=0.7472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.0736]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 39.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 10: F1=0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:07<00:00, 28.66it/s, loss=0.0667]\n",
      "Validating: 100%|██████████| 56/56 [00:01<00:00, 38.87it/s]\n",
      "[I 2025-12-04 20:24:23,844] Trial 5 finished with value: 0.6190891776643856 and parameters: {'learning_rate': 0.00010769622478263136, 'weight_decay': 0.017052412368729154, 'batch_size': 32, 'num_epochs': 11}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Fold 4, Epoch 11: F1=0.7527\n",
      "  Fold 5 F1: 0.7704\n",
      "\n",
      "Trial 5 - Average F1: 0.6191\n",
      "Fold scores: ['0.7396', '0.0000', '0.8030', '0.7824', '0.7704']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 6\n",
      "================================================================================\n",
      "Learning Rate: 3.29e-05\n",
      "Weight Decay: 0.0098\n",
      "Batch Size: 8\n",
      "Num Epochs: 8\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:24:24,661 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:24:24,661 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:24:24,661 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.39it/s, loss=0.175]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 0, Epoch 1: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.46it/s, loss=0.141]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.51it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 0, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.42it/s, loss=0.119]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 0, Epoch 5: F1=0.4614\n",
      "Error in fold 0: name 'logger' is not defined\n",
      "  Fold 1 F1: 0.0000\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:25:35,615 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:25:35,616 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:25:35,616 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.46it/s, loss=0.146]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.51it/s, loss=0.136]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.40it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.50it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1, Epoch 4: F1=0.4614\n",
      "Error in fold 1: name 'logger' is not defined\n",
      "  Fold 2 F1: 0.0000\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:26:32,508 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:26:32,509 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:26:32,509 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.49it/s, loss=0.162]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 2, Epoch 1: F1=0.5138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.54it/s, loss=0.13] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.57it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.49it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 2, Epoch 4: F1=0.4614\n",
      "Error in fold 2: name 'logger' is not defined\n",
      "  Fold 3 F1: 0.0000\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:27:29,332 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:27:29,332 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:27:29,333 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.38it/s, loss=0.137]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 3, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.24it/s, loss=0.126]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 111.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.25it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 3, Epoch 4: F1=0.4614\n",
      "Error in fold 3: name 'logger' is not defined\n",
      "  Fold 4 F1: 0.0000\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:28:26,336 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:28:26,337 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:28:26,337 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.39it/s, loss=0.23] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 1: F1=0.3041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.31it/s, loss=0.143]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.29it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 3: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.50it/s, loss=0.116]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 4: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.41it/s, loss=0.111]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 5: F1=0.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.38it/s, loss=0.105] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 6: F1=0.4812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.46it/s, loss=0.106]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 7: F1=0.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.44it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.84it/s]\n",
      "[I 2025-12-04 20:30:18,804] Trial 6 finished with value: 0.1232348065783837 and parameters: {'learning_rate': 3.292529363110524e-05, 'weight_decay': 0.009767211400638388, 'batch_size': 8, 'num_epochs': 8}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 4, Epoch 8: F1=0.6162\n",
      "  Fold 5 F1: 0.6162\n",
      "\n",
      "Trial 6 - Average F1: 0.1232\n",
      "Fold scores: ['0.0000', '0.0000', '0.0000', '0.0000', '0.6162']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 7\n",
      "================================================================================\n",
      "Learning Rate: 1.14e-05\n",
      "Weight Decay: 0.0909\n",
      "Batch Size: 16\n",
      "Num Epochs: 9\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:30:19,635 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:30:19,635 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:30:19,635 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.51it/s, loss=0.399]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 0, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.52it/s, loss=0.285]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 0, Epoch 2: F1=0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.58it/s, loss=0.142]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.56it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 0, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.61it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 0, Epoch 5: F1=0.4614\n",
      "Error in fold 0: name 'logger' is not defined\n",
      "  Fold 1 F1: 0.0000\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:31:16,713 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:31:16,714 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:31:16,714 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.50it/s, loss=0.181]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1, Epoch 1: F1=0.4728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.61it/s, loss=0.156]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.49it/s, loss=0.134]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.59it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1, Epoch 4: F1=0.4614\n",
      "Error in fold 1: name 'logger' is not defined\n",
      "  Fold 2 F1: 0.0000\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:32:02,517 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:32:02,518 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:32:02,518 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.54it/s, loss=0.285]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 2, Epoch 1: F1=0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.52it/s, loss=0.223]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 2, Epoch 2: F1=0.5448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.52it/s, loss=0.155]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.45it/s, loss=0.143]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 2, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.48it/s, loss=0.135]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 2, Epoch 5: F1=0.4614\n",
      "Error in fold 2: name 'logger' is not defined\n",
      "  Fold 3 F1: 0.0000\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:32:59,681 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:32:59,681 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:32:59,682 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.55it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 3, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.44it/s, loss=0.127]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.58it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.60it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 3, Epoch 4: F1=0.4614\n",
      "Error in fold 3: name 'logger' is not defined\n",
      "  Fold 4 F1: 0.0000\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:33:45,495 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:33:45,495 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:33:45,496 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.51it/s, loss=0.7]  \n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 68.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 1: F1=0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.56it/s, loss=0.517]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 2: F1=0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.52it/s, loss=0.214]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 3: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.55it/s, loss=0.137]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 4: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.53it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 5: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.49it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 65.14it/s]\n",
      "[I 2025-12-04 20:34:53,296] Trial 7 finished with value: 0.0 and parameters: {'learning_rate': 1.1439974749291259e-05, 'weight_decay': 0.0909320402078782, 'batch_size': 16, 'num_epochs': 9}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 4, Epoch 6: F1=0.4615\n",
      "Error in fold 4: name 'logger' is not defined\n",
      "  Fold 5 F1: 0.0000\n",
      "\n",
      "Trial 7 - Average F1: 0.0000\n",
      "Fold scores: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 8\n",
      "================================================================================\n",
      "Learning Rate: 8.49e-05\n",
      "Weight Decay: 0.0185\n",
      "Batch Size: 8\n",
      "Num Epochs: 12\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:34:54,145 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:34:54,145 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:34:54,145 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 72.88it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 0, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.11it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.20it/s, loss=0.124]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 0, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.22it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 0, Epoch 4: F1=0.4614\n",
      "Error in fold 0: name 'logger' is not defined\n",
      "  Fold 1 F1: 0.0000\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:35:51,479 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:35:51,479 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:35:51,479 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.26it/s, loss=0.14] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 1: F1=0.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.41it/s, loss=0.131]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.37it/s, loss=0.125]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.38it/s, loss=0.117]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 4: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.30it/s, loss=0.109]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 5: F1=0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.34it/s, loss=0.103] \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 6: F1=0.5795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.25it/s, loss=0.104] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 7: F1=0.6599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.36it/s, loss=0.101] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 8: F1=0.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.33it/s, loss=0.102] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 9: F1=0.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.42it/s, loss=0.0945]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 10: F1=0.6632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.34it/s, loss=0.0951]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 11: F1=0.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.30it/s, loss=0.09]  \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1, Epoch 12: F1=0.7246\n",
      "  Fold 2 F1: 0.7246\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:38:41,096 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:38:41,097 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:38:41,097 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.41it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.47it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.29it/s, loss=0.119]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.30it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 4: F1=0.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.31it/s, loss=0.107]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 5: F1=0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.38it/s, loss=0.107]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 6: F1=0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.35it/s, loss=0.103] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 7: F1=0.5656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.33it/s, loss=0.104]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 8: F1=0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.31it/s, loss=0.1]   \n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 9: F1=0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.23it/s, loss=0.0975]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 10: F1=0.6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.46it/s, loss=0.0934]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 11: F1=0.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.35it/s, loss=0.0958]\n",
      "Validating: 100%|██████████| 225/225 [00:02<00:00, 112.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 2, Epoch 12: F1=0.7081\n",
      "  Fold 3 F1: 0.7081\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:41:30,637 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:41:30,637 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:41:30,638 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.34it/s, loss=0.15] \n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 113.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 3, Epoch 1: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.48it/s, loss=0.142]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 3, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.35it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 3, Epoch 3: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.33it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 225/225 [00:01<00:00, 112.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 3, Epoch 4: F1=0.4614\n",
      "Error in fold 3: name 'logger' is not defined\n",
      "  Fold 4 F1: 0.0000\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:42:27,596 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:42:27,596 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:42:27,596 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.27it/s, loss=0.25] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 1: F1=0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.36it/s, loss=0.15] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.43it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 3: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.30it/s, loss=0.122]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 4: F1=0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.51it/s, loss=0.115]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 5: F1=0.5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.37it/s, loss=0.11] \n",
      "Validating: 100%|██████████| 224/224 [00:02<00:00, 111.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 6: F1=0.5805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.27it/s, loss=0.108] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 7: F1=0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.40it/s, loss=0.108]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 8: F1=0.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.45it/s, loss=0.101] \n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 9: F1=0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.34it/s, loss=0.0957]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 10: F1=0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.45it/s, loss=0.0965]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 11: F1=0.6643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 897/897 [00:12<00:00, 74.36it/s, loss=0.0932]\n",
      "Validating: 100%|██████████| 224/224 [00:01<00:00, 112.53it/s]\n",
      "[I 2025-12-04 20:45:16,356] Trial 8 finished with value: 0.4320221269194707 and parameters: {'learning_rate': 8.488762161408708e-05, 'weight_decay': 0.018485445552552705, 'batch_size': 8, 'num_epochs': 12}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 4, Epoch 12: F1=0.7274\n",
      "  Fold 5 F1: 0.7274\n",
      "\n",
      "Trial 8 - Average F1: 0.4320\n",
      "Fold scores: ['0.0000', '0.7246', '0.7081', '0.0000', '0.7274']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Trial 9\n",
      "================================================================================\n",
      "Learning Rate: 1.04e-04\n",
      "Weight Decay: 0.0922\n",
      "Batch Size: 16\n",
      "Num Epochs: 7\n",
      "\n",
      "  Fold 1/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:45:17,166 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:45:17,166 - INFO - Language weights: {'es': 0.9847470941344993, 'en': 1.0008296471794096, 'it': 1.0144232586860908}\n",
      "2025-12-04 20:45:17,166 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.43it/s, loss=0.173]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 1: F1=0.4816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.39it/s, loss=0.128]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.39it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 3: F1=0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.44it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 4: F1=0.6119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.48it/s, loss=0.0967]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 5: F1=0.6521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.54it/s, loss=0.0907]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 6: F1=0.6692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.50it/s, loss=0.0861]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 0, Epoch 7: F1=0.7351\n",
      "  Fold 1 F1: 0.7351\n",
      "\n",
      "  Fold 2/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:46:36,964 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:46:36,965 - INFO - Language weights: {'it': 0.990547924474408, 'en': 1.0000803940404794, 'es': 1.0093716814851124}\n",
      "2025-12-04 20:46:36,965 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.42it/s, loss=0.566]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 1: F1=0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.52it/s, loss=0.158]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 2: F1=0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.47it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 3: F1=0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.41it/s, loss=0.104]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 4: F1=0.6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.47it/s, loss=0.099] \n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 5: F1=0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.40it/s, loss=0.0914]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 6: F1=0.6307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.51it/s, loss=0.088] \n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 1, Epoch 7: F1=0.7500\n",
      "  Fold 2 F1: 0.7500\n",
      "\n",
      "  Fold 3/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:47:56,769 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:47:56,769 - INFO - Language weights: {'it': 0.9959626621507102, 'en': 1.0005485094858537, 'es': 1.003488828363436}\n",
      "2025-12-04 20:47:56,770 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.36it/s, loss=0.374]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 1: F1=0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.49it/s, loss=0.166]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 2: F1=0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.44it/s, loss=0.129]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 3: F1=0.5224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.36it/s, loss=0.112]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 4: F1=0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.39it/s, loss=0.104]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 5: F1=0.6602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.36it/s, loss=0.0954]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 6: F1=0.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.47it/s, loss=0.0893]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 69.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 2, Epoch 7: F1=0.6221\n",
      "  Fold 3 F1: 0.6982\n",
      "\n",
      "  Fold 4/5: Train=7171, Val=1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:49:16,617 - INFO - Label weights: {0: 0.5835774739583334, 1: 3.491236611489776}\n",
      "2025-12-04 20:49:16,617 - INFO - Language weights: {'en': 0.9905452280968097, 'es': 0.9996594041813475, 'it': 1.0097953677218428}\n",
      "2025-12-04 20:49:16,618 - INFO - Pos weight (for BCE): 5.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.37it/s, loss=0.275]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 1: F1=0.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.64it/s, loss=0.139]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 67.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 2: F1=0.4812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.18it/s, loss=0.117]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 68.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 3: F1=0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.23it/s, loss=0.105]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 65.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 4: F1=0.6123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.16it/s, loss=0.0994]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 67.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 5: F1=0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.34it/s, loss=0.0951]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 70.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 6: F1=0.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.29it/s, loss=0.0897]\n",
      "Validating: 100%|██████████| 113/113 [00:01<00:00, 67.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 3, Epoch 7: F1=0.6777\n",
      "  Fold 4 F1: 0.6962\n",
      "\n",
      "  Fold 5/5: Train=7172, Val=1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:50:37,772 - INFO - Label weights: {0: 0.5836588541666666, 1: 3.4883268482490273}\n",
      "2025-12-04 20:50:37,772 - INFO - Language weights: {'it': 0.9894548630811957, 'es': 1.0027361364111447, 'en': 1.0078090005076596}\n",
      "2025-12-04 20:50:37,773 - INFO - Pos weight (for BCE): 5.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze: Embeddings + First 10 Encoder Layers\n",
      "Trainable: Classification Head + Remaining Encoder Layers\n",
      "Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.06it/s, loss=0.998]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 64.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 1: F1=0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.44it/s, loss=0.237]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 2: F1=0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.06it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 65.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 3: F1=0.5304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.05it/s, loss=0.106]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 67.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 4: F1=0.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 45.79it/s, loss=0.0973]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 5: F1=0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.63it/s, loss=0.0925]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 6: F1=0.7076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 449/449 [00:09<00:00, 46.56it/s, loss=0.0877]\n",
      "Validating: 100%|██████████| 112/112 [00:01<00:00, 69.49it/s]\n",
      "[I 2025-12-04 20:51:57,842] Trial 9 finished with value: 0.7174014345909159 and parameters: {'learning_rate': 0.0001037084466895453, 'weight_decay': 0.09218742350231168, 'batch_size': 16, 'num_epochs': 7}. Best is trial 2 with value: 0.7918504359347013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 4, Epoch 7: F1=0.6957\n",
      "  Fold 5 F1: 0.7076\n",
      "\n",
      "Trial 9 - Average F1: 0.7174\n",
      "Fold scores: ['0.7351', '0.7500', '0.6982', '0.6962', '0.7076']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEST TRIAL\n",
      "================================================================================\n",
      "Trial: 2\n",
      "Best F1 Score: 0.7919\n",
      "\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.00025959425503112657\n",
      "  weight_decay: 0.021233911067827616\n",
      "  batch_size: 32\n",
      "  num_epochs: 9\n",
      "================================================================================\n",
      "\n",
      "\n",
      "                                                                                \n",
      " STEP 2: VISUALIZE RESULTS\n",
      "                                                                                \n",
      "\n",
      "\n",
      "Generating visualizations...\n",
      "✓ Saved: optuna_optimization_history.svg\n",
      "✓ Saved: optuna_learning_rate_impact.svg\n",
      "✓ Saved: optuna_batch_size_impact.svg\n",
      "✓ Saved: optuna_weight_decay_impact.svg\n",
      "✓ Saved: optuna_epochs_impact.svg\n",
      "✓ Saved: optuna_trials.csv\n",
      "\n",
      "All visualizations saved to: ../figures\n",
      "\n",
      "Top 5 Trials:\n",
      "   trial        f1        lr  batch_size  epochs\n",
      "2      2  0.791850  0.000260          32       9\n",
      "9      9  0.717401  0.000104          16       7\n",
      "3      3  0.648432  0.000054           8       7\n",
      "5      5  0.619089  0.000108          32      11\n",
      "4      4  0.615450  0.000060          32       5\n"
     ]
    }
   ],
   "source": [
    "base_config = BaseConfig()\n",
    "study, best_trial = run_optuna_optimization(merged_data, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd020970-5d54-4247-97f4-338ae978f230",
   "metadata": {},
   "source": [
    "### Final Model Training w.r.t Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e4e8028-539f-481f-96ab-25b8c552a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 20:53:11,567 - INFO - Fold 0: Train=3824, Val=956\n",
      "2025-12-04 20:53:11,567 - INFO -   Train label dist: {0: 3276, 1: 548}\n",
      "2025-12-04 20:53:11,568 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 20:53:11,573 - INFO - Fold 1: Train=3824, Val=957\n",
      "2025-12-04 20:53:11,574 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 20:53:11,575 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 20:53:11,581 - INFO - Fold 2: Train=3824, Val=957\n",
      "2025-12-04 20:53:11,582 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 20:53:11,583 - INFO -   Train lang dist: {'es': 1352, 'en': 1256, 'it': 1216}\n",
      "2025-12-04 20:53:11,589 - INFO - Fold 3: Train=3824, Val=957\n",
      "2025-12-04 20:53:11,589 - INFO -   Train label dist: {0: 3278, 1: 546}\n",
      "2025-12-04 20:53:11,590 - INFO -   Train lang dist: {'es': 1352, 'en': 1255, 'it': 1217}\n",
      "2025-12-04 20:53:11,594 - INFO - Fold 4: Train=3824, Val=957\n",
      "2025-12-04 20:53:11,594 - INFO -   Train label dist: {0: 3277, 1: 547}\n",
      "2025-12-04 20:53:11,594 - INFO -   Train lang dist: {'es': 1351, 'en': 1256, 'it': 1217}\n",
      "2025-12-04 20:53:11,595 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:53:11,595 - INFO - Fold 1/5\n",
      "2025-12-04 20:53:11,595 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "STEP 3: TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
      "                                                                                \n",
      "\n",
      "Final Training Configuration:\n",
      "  Learning Rate: 2.60e-04\n",
      "  Weight Decay: 0.0212\n",
      "  Batch Size: 32\n",
      "  Num Epochs: 9\n",
      "\n",
      "\n",
      "FOLD 0:\n",
      "  Train: 548 positive samples\n",
      "  Val:   137 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:53:12,316 - INFO - Froze: Embeddings + First 10 Encoder Layers\n",
      "2025-12-04 20:53:12,317 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 20:53:12,317 - INFO - Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n",
      "2025-12-04 20:53:12,318 - INFO - Label weights: {0: 0.5836385836385837, 1: 3.489051094890511}\n",
      "2025-12-04 20:53:12,318 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 20:53:12,318 - INFO - Pos weight (for BCE): 5.9781\n",
      "2025-12-04 20:53:12,319 - INFO - \n",
      "Epoch 1/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.72it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.62it/s]\n",
      "2025-12-04 20:53:17,458 - INFO - Train Loss: 0.1312\n",
      "2025-12-04 20:53:17,458 - INFO - Val Loss: 0.1140\n",
      "2025-12-04 20:53:17,458 - INFO - Overall - Precision: 0.4283, Recall: 0.5000, F1: 0.4614\n",
      "2025-12-04 20:53:17,458 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 20:53:17,459 - INFO - es - Precision: 0.4303, Recall: 0.5000, F1: 0.4625\n",
      "2025-12-04 20:53:17,459 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:53:18,148 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_0_f1_0.4614.pt (F1: 0.4614, Fold: 0, Epoch: 0)\n",
      "2025-12-04 20:53:18,148 - INFO - \n",
      "Epoch 2/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.54it/s, loss=0.116]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.55it/s]\n",
      "2025-12-04 20:53:23,296 - INFO - Train Loss: 0.1145\n",
      "2025-12-04 20:53:23,296 - INFO - Val Loss: 0.0963\n",
      "2025-12-04 20:53:23,296 - INFO - Overall - Precision: 0.6520, Recall: 0.5115, F1: 0.4883\n",
      "2025-12-04 20:53:23,297 - INFO - en - Precision: 0.9153, Recall: 0.5093, F1: 0.4719\n",
      "2025-12-04 20:53:23,297 - INFO - es - Precision: 0.5978, Recall: 0.5072, F1: 0.4815\n",
      "2025-12-04 20:53:23,297 - INFO - it - Precision: 0.6433, Recall: 0.5222, F1: 0.5163\n",
      "2025-12-04 20:53:23,960 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_1_f1_0.4883.pt (F1: 0.4883, Fold: 0, Epoch: 1)\n",
      "2025-12-04 20:53:23,960 - INFO - \n",
      "Epoch 3/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.69it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.19it/s]\n",
      "2025-12-04 20:53:29,092 - INFO - Train Loss: 0.1012\n",
      "2025-12-04 20:53:29,092 - INFO - Val Loss: 0.0866\n",
      "2025-12-04 20:53:29,093 - INFO - Overall - Precision: 0.7427, Recall: 0.5620, F1: 0.5758\n",
      "2025-12-04 20:53:29,093 - INFO - en - Precision: 0.7716, Recall: 0.5922, F1: 0.6141\n",
      "2025-12-04 20:53:29,093 - INFO - es - Precision: 0.7373, Recall: 0.5569, F1: 0.5688\n",
      "2025-12-04 20:53:29,093 - INFO - it - Precision: 0.6433, Recall: 0.5222, F1: 0.5163\n",
      "2025-12-04 20:53:29,167 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_0_f1_0.4614.pt\n",
      "2025-12-04 20:53:29,847 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_2_f1_0.5758.pt (F1: 0.5758, Fold: 0, Epoch: 2)\n",
      "2025-12-04 20:53:29,847 - INFO - \n",
      "Epoch 4/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.42it/s, loss=0.0913]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.31it/s]\n",
      "2025-12-04 20:53:35,018 - INFO - Train Loss: 0.0898\n",
      "2025-12-04 20:53:35,018 - INFO - Val Loss: 0.0834\n",
      "2025-12-04 20:53:35,018 - INFO - Overall - Precision: 0.8024, Recall: 0.5821, F1: 0.6062\n",
      "2025-12-04 20:53:35,018 - INFO - en - Precision: 0.8871, Recall: 0.5999, F1: 0.6275\n",
      "2025-12-04 20:53:35,019 - INFO - es - Precision: 0.7785, Recall: 0.6173, F1: 0.6510\n",
      "2025-12-04 20:53:35,019 - INFO - it - Precision: 0.6087, Recall: 0.5102, F1: 0.4932\n",
      "2025-12-04 20:53:35,090 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_1_f1_0.4883.pt\n",
      "2025-12-04 20:53:35,761 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_3_f1_0.6062.pt (F1: 0.6062, Fold: 0, Epoch: 3)\n",
      "2025-12-04 20:53:35,761 - INFO - \n",
      "Epoch 5/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.58it/s, loss=0.086] \n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 36.80it/s]\n",
      "2025-12-04 20:53:40,940 - INFO - Train Loss: 0.0845\n",
      "2025-12-04 20:53:40,941 - INFO - Val Loss: 0.0782\n",
      "2025-12-04 20:53:40,941 - INFO - Overall - Precision: 0.8043, Recall: 0.6228, F1: 0.6592\n",
      "2025-12-04 20:53:40,941 - INFO - en - Precision: 0.8042, Recall: 0.6791, F1: 0.7154\n",
      "2025-12-04 20:53:40,941 - INFO - es - Precision: 0.8744, Recall: 0.6242, F1: 0.6665\n",
      "2025-12-04 20:53:40,941 - INFO - it - Precision: 0.6589, Recall: 0.5342, F1: 0.5371\n",
      "2025-12-04 20:53:41,013 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_2_f1_0.5758.pt\n",
      "2025-12-04 20:53:41,701 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_4_f1_0.6592.pt (F1: 0.6592, Fold: 0, Epoch: 4)\n",
      "2025-12-04 20:53:41,701 - INFO - \n",
      "Epoch 6/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.44it/s, loss=0.0829]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.90it/s]\n",
      "2025-12-04 20:53:46,703 - INFO - Train Loss: 0.0815\n",
      "2025-12-04 20:53:46,703 - INFO - Val Loss: 0.0886\n",
      "2025-12-04 20:53:46,703 - INFO - Overall - Precision: 0.8082, Recall: 0.6636, F1: 0.7037\n",
      "2025-12-04 20:53:46,703 - INFO - en - Precision: 0.8517, Recall: 0.6941, F1: 0.7378\n",
      "2025-12-04 20:53:46,703 - INFO - es - Precision: 0.8318, Recall: 0.7309, F1: 0.7680\n",
      "2025-12-04 20:53:46,704 - INFO - it - Precision: 0.6109, Recall: 0.5305, F1: 0.5322\n",
      "2025-12-04 20:53:46,775 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_3_f1_0.6062.pt\n",
      "2025-12-04 20:53:47,444 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_5_f1_0.7037.pt (F1: 0.7037, Fold: 0, Epoch: 5)\n",
      "2025-12-04 20:53:47,445 - INFO - \n",
      "Epoch 7/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.44it/s, loss=0.0706]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.89it/s]\n",
      "2025-12-04 20:53:52,448 - INFO - Train Loss: 0.0694\n",
      "2025-12-04 20:53:52,448 - INFO - Val Loss: 0.1006\n",
      "2025-12-04 20:53:52,448 - INFO - Overall - Precision: 0.9120, Recall: 0.5687, F1: 0.5871\n",
      "2025-12-04 20:53:52,448 - INFO - en - Precision: 0.9248, Recall: 0.5741, F1: 0.5884\n",
      "2025-12-04 20:53:52,448 - INFO - es - Precision: 0.8978, Recall: 0.6047, F1: 0.6416\n",
      "2025-12-04 20:53:52,448 - INFO - it - Precision: 0.9424, Recall: 0.5139, F1: 0.4965\n",
      "2025-12-04 20:53:52,449 - INFO - \n",
      "Epoch 8/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.89it/s, loss=0.0668]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 36.86it/s]\n",
      "2025-12-04 20:53:57,577 - INFO - Train Loss: 0.0657\n",
      "2025-12-04 20:53:57,577 - INFO - Val Loss: 0.0695\n",
      "2025-12-04 20:53:57,577 - INFO - Overall - Precision: 0.8352, Recall: 0.7512, F1: 0.7841\n",
      "2025-12-04 20:53:57,578 - INFO - en - Precision: 0.8323, Recall: 0.7917, F1: 0.8097\n",
      "2025-12-04 20:53:57,578 - INFO - es - Precision: 0.8706, Recall: 0.8160, F1: 0.8401\n",
      "2025-12-04 20:53:57,578 - INFO - it - Precision: 0.7597, Recall: 0.6018, F1: 0.6339\n",
      "2025-12-04 20:53:57,649 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_4_f1_0.6592.pt\n",
      "2025-12-04 20:53:58,344 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_7_f1_0.7841.pt (F1: 0.7841, Fold: 0, Epoch: 7)\n",
      "2025-12-04 20:53:58,344 - INFO - \n",
      "Epoch 9/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.55it/s, loss=0.0608]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.32it/s]\n",
      "2025-12-04 20:54:03,516 - INFO - Train Loss: 0.0598\n",
      "2025-12-04 20:54:03,516 - INFO - Val Loss: 0.0954\n",
      "2025-12-04 20:54:03,516 - INFO - Overall - Precision: 0.7566, Recall: 0.8289, F1: 0.7845\n",
      "2025-12-04 20:54:03,516 - INFO - en - Precision: 0.7899, Recall: 0.8702, F1: 0.8195\n",
      "2025-12-04 20:54:03,517 - INFO - es - Precision: 0.7619, Recall: 0.8775, F1: 0.7998\n",
      "2025-12-04 20:54:03,517 - INFO - it - Precision: 0.6943, Recall: 0.6989, F1: 0.6966\n",
      "2025-12-04 20:54:03,588 - INFO - Deleted checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_5_f1_0.7037.pt\n",
      "2025-12-04 20:54:04,260 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt (F1: 0.7845, Fold: 0, Epoch: 8)\n",
      "2025-12-04 20:54:04,260 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:54:04,260 - INFO - Fold 2/5\n",
      "2025-12-04 20:54:04,260 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 1:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:54:04,959 - INFO - Froze: Embeddings + First 10 Encoder Layers\n",
      "2025-12-04 20:54:04,960 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 20:54:04,960 - INFO - Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n",
      "2025-12-04 20:54:04,962 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 20:54:04,962 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 20:54:04,962 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 20:54:04,963 - INFO - \n",
      "Epoch 1/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.57it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.29it/s]\n",
      "2025-12-04 20:54:10,120 - INFO - Train Loss: 0.1310\n",
      "2025-12-04 20:54:10,120 - INFO - Val Loss: 0.1140\n",
      "2025-12-04 20:54:10,121 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 20:54:10,121 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 20:54:10,121 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:54:10,121 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:54:10,797 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 1, Epoch: 0)\n",
      "2025-12-04 20:54:10,797 - INFO - \n",
      "Epoch 2/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.62it/s, loss=0.119]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.83it/s]\n",
      "2025-12-04 20:54:15,947 - INFO - Train Loss: 0.1168\n",
      "2025-12-04 20:54:15,947 - INFO - Val Loss: 0.0976\n",
      "2025-12-04 20:54:15,948 - INFO - Overall - Precision: 0.9288, Recall: 0.5072, F1: 0.4760\n",
      "2025-12-04 20:54:15,948 - INFO - en - Precision: 0.9167, Recall: 0.5185, F1: 0.4903\n",
      "2025-12-04 20:54:15,948 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:54:15,948 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:54:16,637 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_1_f1_0.4760.pt (F1: 0.4760, Fold: 1, Epoch: 1)\n",
      "2025-12-04 20:54:16,637 - INFO - \n",
      "Epoch 3/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.44it/s, loss=0.103]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.20it/s]\n",
      "2025-12-04 20:54:21,808 - INFO - Train Loss: 0.1011\n",
      "2025-12-04 20:54:21,809 - INFO - Val Loss: 0.0882\n",
      "2025-12-04 20:54:21,809 - INFO - Overall - Precision: 0.7871, Recall: 0.6303, F1: 0.6660\n",
      "2025-12-04 20:54:21,809 - INFO - en - Precision: 0.7983, Recall: 0.6864, F1: 0.7211\n",
      "2025-12-04 20:54:21,809 - INFO - es - Precision: 0.8430, Recall: 0.6494, F1: 0.6935\n",
      "2025-12-04 20:54:21,809 - INFO - it - Precision: 0.5858, Recall: 0.5185, F1: 0.5121\n",
      "2025-12-04 20:54:22,483 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_2_f1_0.6660.pt (F1: 0.6660, Fold: 1, Epoch: 2)\n",
      "2025-12-04 20:54:22,483 - INFO - \n",
      "Epoch 4/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.59it/s, loss=0.0921]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.20it/s]\n",
      "2025-12-04 20:54:27,630 - INFO - Train Loss: 0.0905\n",
      "2025-12-04 20:54:27,630 - INFO - Val Loss: 0.0822\n",
      "2025-12-04 20:54:27,631 - INFO - Overall - Precision: 0.7822, Recall: 0.6948, F1: 0.7262\n",
      "2025-12-04 20:54:27,631 - INFO - en - Precision: 0.8361, Recall: 0.7844, F1: 0.8065\n",
      "2025-12-04 20:54:27,631 - INFO - es - Precision: 0.7879, Recall: 0.6911, F1: 0.7246\n",
      "2025-12-04 20:54:27,631 - INFO - it - Precision: 0.6356, Recall: 0.5647, F1: 0.5795\n",
      "2025-12-04 20:54:28,304 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_3_f1_0.7262.pt (F1: 0.7262, Fold: 1, Epoch: 3)\n",
      "2025-12-04 20:54:28,304 - INFO - \n",
      "Epoch 5/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.47it/s, loss=0.0843]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.12it/s]\n",
      "2025-12-04 20:54:33,472 - INFO - Train Loss: 0.0829\n",
      "2025-12-04 20:54:33,472 - INFO - Val Loss: 0.0845\n",
      "2025-12-04 20:54:33,473 - INFO - Overall - Precision: 0.8914, Recall: 0.5712, F1: 0.5908\n",
      "2025-12-04 20:54:33,473 - INFO - en - Precision: 0.8648, Recall: 0.6165, F1: 0.6499\n",
      "2025-12-04 20:54:33,473 - INFO - es - Precision: 0.9367, Recall: 0.5625, F1: 0.5773\n",
      "2025-12-04 20:54:33,473 - INFO - it - Precision: 0.9424, Recall: 0.5139, F1: 0.4965\n",
      "2025-12-04 20:54:33,473 - INFO - \n",
      "Epoch 6/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.62it/s, loss=0.079] \n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.02it/s]\n",
      "2025-12-04 20:54:38,619 - INFO - Train Loss: 0.0777\n",
      "2025-12-04 20:54:38,619 - INFO - Val Loss: 0.0718\n",
      "2025-12-04 20:54:38,619 - INFO - Overall - Precision: 0.8270, Recall: 0.7347, F1: 0.7695\n",
      "2025-12-04 20:54:38,619 - INFO - en - Precision: 0.8455, Recall: 0.8195, F1: 0.8316\n",
      "2025-12-04 20:54:38,620 - INFO - es - Precision: 0.8558, Recall: 0.7779, F1: 0.8097\n",
      "2025-12-04 20:54:38,620 - INFO - it - Precision: 0.6682, Recall: 0.5463, F1: 0.5561\n",
      "2025-12-04 20:54:39,303 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_5_f1_0.7695.pt (F1: 0.7695, Fold: 1, Epoch: 5)\n",
      "2025-12-04 20:54:39,304 - INFO - \n",
      "Epoch 7/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.64it/s, loss=0.0716]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.23it/s]\n",
      "2025-12-04 20:54:44,442 - INFO - Train Loss: 0.0704\n",
      "2025-12-04 20:54:44,443 - INFO - Val Loss: 0.0730\n",
      "2025-12-04 20:54:44,443 - INFO - Overall - Precision: 0.7719, Recall: 0.7654, F1: 0.7686\n",
      "2025-12-04 20:54:44,443 - INFO - en - Precision: 0.7948, Recall: 0.8281, F1: 0.8097\n",
      "2025-12-04 20:54:44,443 - INFO - es - Precision: 0.7965, Recall: 0.7484, F1: 0.7691\n",
      "2025-12-04 20:54:44,443 - INFO - it - Precision: 0.7057, Recall: 0.6906, F1: 0.6977\n",
      "2025-12-04 20:54:44,443 - INFO - \n",
      "Epoch 8/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.28it/s, loss=0.0666]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.98it/s]\n",
      "2025-12-04 20:54:49,645 - INFO - Train Loss: 0.0655\n",
      "2025-12-04 20:54:49,645 - INFO - Val Loss: 0.0682\n",
      "2025-12-04 20:54:49,645 - INFO - Overall - Precision: 0.7643, Recall: 0.7876, F1: 0.7751\n",
      "2025-12-04 20:54:49,646 - INFO - en - Precision: 0.8264, Recall: 0.8543, F1: 0.8392\n",
      "2025-12-04 20:54:49,646 - INFO - es - Precision: 0.7680, Recall: 0.8041, F1: 0.7841\n",
      "2025-12-04 20:54:49,646 - INFO - it - Precision: 0.6635, Recall: 0.6674, F1: 0.6654\n",
      "2025-12-04 20:54:50,331 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_1_epoch_7_f1_0.7751.pt (F1: 0.7751, Fold: 1, Epoch: 7)\n",
      "2025-12-04 20:54:50,331 - INFO - \n",
      "Epoch 9/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.60it/s, loss=0.0608]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.92it/s]\n",
      "2025-12-04 20:54:55,482 - INFO - Train Loss: 0.0598\n",
      "2025-12-04 20:54:55,482 - INFO - Val Loss: 0.1297\n",
      "2025-12-04 20:54:55,482 - INFO - Overall - Precision: 0.8432, Recall: 0.6660, F1: 0.7112\n",
      "2025-12-04 20:54:55,482 - INFO - en - Precision: 0.8288, Recall: 0.6995, F1: 0.7389\n",
      "2025-12-04 20:54:55,482 - INFO - es - Precision: 0.8721, Recall: 0.7014, F1: 0.7513\n",
      "2025-12-04 20:54:55,482 - INFO - it - Precision: 0.8051, Recall: 0.5657, F1: 0.5872\n",
      "2025-12-04 20:54:55,482 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:54:55,483 - INFO - Fold 3/5\n",
      "2025-12-04 20:54:55,483 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 2:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:54:56,193 - INFO - Froze: Embeddings + First 10 Encoder Layers\n",
      "2025-12-04 20:54:56,193 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 20:54:56,194 - INFO - Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n",
      "2025-12-04 20:54:56,195 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 20:54:56,195 - INFO - Language weights: {'es': 0.9409476243674836, 'en': 1.0128671880134061, 'it': 1.0461851876191102}\n",
      "2025-12-04 20:54:56,195 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 20:54:56,196 - INFO - \n",
      "Epoch 1/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.15it/s, loss=0.135]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.30it/s]\n",
      "2025-12-04 20:55:01,262 - INFO - Train Loss: 0.1326\n",
      "2025-12-04 20:55:01,262 - INFO - Val Loss: 0.1139\n",
      "2025-12-04 20:55:01,262 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 20:55:01,263 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 20:55:01,263 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:55:01,263 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:55:01,961 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_2_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 2, Epoch: 0)\n",
      "2025-12-04 20:55:01,961 - INFO - \n",
      "Epoch 2/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.67it/s, loss=0.118]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.34it/s]\n",
      "2025-12-04 20:55:07,093 - INFO - Train Loss: 0.1163\n",
      "2025-12-04 20:55:07,093 - INFO - Val Loss: 0.1076\n",
      "2025-12-04 20:55:07,093 - INFO - Overall - Precision: 0.9283, Recall: 0.5036, F1: 0.4686\n",
      "2025-12-04 20:55:07,093 - INFO - en - Precision: 0.9153, Recall: 0.5093, F1: 0.4719\n",
      "2025-12-04 20:55:07,093 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:55:07,094 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:55:07,774 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_2_epoch_1_f1_0.4686.pt (F1: 0.4686, Fold: 2, Epoch: 1)\n",
      "2025-12-04 20:55:07,774 - INFO - \n",
      "Epoch 3/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.10it/s, loss=0.102]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.71it/s]\n",
      "2025-12-04 20:55:12,832 - INFO - Train Loss: 0.1007\n",
      "2025-12-04 20:55:12,832 - INFO - Val Loss: 0.0885\n",
      "2025-12-04 20:55:12,832 - INFO - Overall - Precision: 0.8260, Recall: 0.5724, F1: 0.5921\n",
      "2025-12-04 20:55:12,832 - INFO - en - Precision: 0.8375, Recall: 0.6146, F1: 0.6463\n",
      "2025-12-04 20:55:12,832 - INFO - es - Precision: 0.9328, Recall: 0.5312, F1: 0.5228\n",
      "2025-12-04 20:55:12,832 - INFO - it - Precision: 0.7603, Recall: 0.5639, F1: 0.5836\n",
      "2025-12-04 20:55:13,525 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_2_epoch_2_f1_0.5921.pt (F1: 0.5921, Fold: 2, Epoch: 2)\n",
      "2025-12-04 20:55:13,526 - INFO - \n",
      "Epoch 4/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.93it/s, loss=0.0918]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.00it/s]\n",
      "2025-12-04 20:55:18,624 - INFO - Train Loss: 0.0902\n",
      "2025-12-04 20:55:18,624 - INFO - Val Loss: 0.0811\n",
      "2025-12-04 20:55:18,625 - INFO - Overall - Precision: 0.7959, Recall: 0.6774, F1: 0.7144\n",
      "2025-12-04 20:55:18,625 - INFO - en - Precision: 0.8159, Recall: 0.7142, F1: 0.7489\n",
      "2025-12-04 20:55:18,625 - INFO - es - Precision: 0.8497, Recall: 0.6893, F1: 0.7357\n",
      "2025-12-04 20:55:18,625 - INFO - it - Precision: 0.6896, Recall: 0.6064, F1: 0.6303\n",
      "2025-12-04 20:55:19,288 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_2_epoch_3_f1_0.7144.pt (F1: 0.7144, Fold: 2, Epoch: 3)\n",
      "2025-12-04 20:55:19,288 - INFO - \n",
      "Epoch 5/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.40it/s, loss=0.0864]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.77it/s]\n",
      "2025-12-04 20:55:24,300 - INFO - Train Loss: 0.0850\n",
      "2025-12-04 20:55:24,300 - INFO - Val Loss: 0.0831\n",
      "2025-12-04 20:55:24,300 - INFO - Overall - Precision: 0.8997, Recall: 0.6141, F1: 0.6537\n",
      "2025-12-04 20:55:24,300 - INFO - en - Precision: 0.9127, Recall: 0.6647, F1: 0.7132\n",
      "2025-12-04 20:55:24,300 - INFO - es - Precision: 0.9434, Recall: 0.6146, F1: 0.6565\n",
      "2025-12-04 20:55:24,301 - INFO - it - Precision: 0.7450, Recall: 0.5379, F1: 0.5424\n",
      "2025-12-04 20:55:24,301 - INFO - \n",
      "Epoch 6/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.20it/s, loss=0.0762]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.28it/s]\n",
      "2025-12-04 20:55:29,352 - INFO - Train Loss: 0.0750\n",
      "2025-12-04 20:55:29,352 - INFO - Val Loss: 0.0781\n",
      "2025-12-04 20:55:29,353 - INFO - Overall - Precision: 0.8391, Recall: 0.6690, F1: 0.7137\n",
      "2025-12-04 20:55:29,353 - INFO - en - Precision: 0.8699, Recall: 0.7311, F1: 0.7755\n",
      "2025-12-04 20:55:29,353 - INFO - es - Precision: 0.8553, Recall: 0.6997, F1: 0.7466\n",
      "2025-12-04 20:55:29,353 - INFO - it - Precision: 0.6589, Recall: 0.5342, F1: 0.5371\n",
      "2025-12-04 20:55:29,353 - INFO - \n",
      "Epoch 7/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.49it/s, loss=0.0725]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.97it/s]\n",
      "2025-12-04 20:55:34,522 - INFO - Train Loss: 0.0713\n",
      "2025-12-04 20:55:34,522 - INFO - Val Loss: 0.0769\n",
      "2025-12-04 20:55:34,523 - INFO - Overall - Precision: 0.8717, Recall: 0.6407, F1: 0.6859\n",
      "2025-12-04 20:55:34,523 - INFO - en - Precision: 0.8517, Recall: 0.6941, F1: 0.7378\n",
      "2025-12-04 20:55:34,523 - INFO - es - Precision: 0.9489, Recall: 0.6562, F1: 0.7112\n",
      "2025-12-04 20:55:34,523 - INFO - it - Precision: 0.7450, Recall: 0.5379, F1: 0.5424\n",
      "2025-12-04 20:55:34,523 - INFO - Early stopping triggered at epoch 6 (patience: 3)\n",
      "2025-12-04 20:55:34,524 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:55:34,524 - INFO - Fold 4/5\n",
      "2025-12-04 20:55:34,524 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 3:\n",
      "  Train: 546 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:55:35,232 - INFO - Froze: Embeddings + First 10 Encoder Layers\n",
      "2025-12-04 20:55:35,233 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 20:55:35,233 - INFO - Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n",
      "2025-12-04 20:55:35,234 - INFO - Label weights: {0: 0.5832824893227577, 1: 3.501831501831502}\n",
      "2025-12-04 20:55:35,235 - INFO - Language weights: {'es': 0.9409641154628656, 'en': 1.0136920192078043, 'it': 1.0453438653293299}\n",
      "2025-12-04 20:55:35,235 - INFO - Pos weight (for BCE): 6.0037\n",
      "2025-12-04 20:55:35,236 - INFO - \n",
      "Epoch 1/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.55it/s, loss=0.132]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.96it/s]\n",
      "2025-12-04 20:55:40,230 - INFO - Train Loss: 0.1301\n",
      "2025-12-04 20:55:40,231 - INFO - Val Loss: 0.1142\n",
      "2025-12-04 20:55:40,231 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 20:55:40,231 - INFO - en - Precision: 0.4127, Recall: 0.5000, F1: 0.4522\n",
      "2025-12-04 20:55:40,231 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:55:40,231 - INFO - it - Precision: 0.4424, Recall: 0.5000, F1: 0.4695\n",
      "2025-12-04 20:55:40,889 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 3, Epoch: 0)\n",
      "2025-12-04 20:55:40,890 - INFO - \n",
      "Epoch 2/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.75it/s, loss=0.12] \n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.20it/s]\n",
      "2025-12-04 20:55:46,032 - INFO - Train Loss: 0.1184\n",
      "2025-12-04 20:55:46,032 - INFO - Val Loss: 0.0955\n",
      "2025-12-04 20:55:46,033 - INFO - Overall - Precision: 0.6783, Recall: 0.5030, F1: 0.4682\n",
      "2025-12-04 20:55:46,033 - INFO - en - Precision: 0.9140, Recall: 0.5091, F1: 0.4708\n",
      "2025-12-04 20:55:46,033 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:55:46,033 - INFO - it - Precision: 0.4422, Recall: 0.4981, F1: 0.4685\n",
      "2025-12-04 20:55:46,703 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_1_f1_0.4682.pt (F1: 0.4682, Fold: 3, Epoch: 1)\n",
      "2025-12-04 20:55:46,703 - INFO - \n",
      "Epoch 3/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.70it/s, loss=0.104]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.47it/s]\n",
      "2025-12-04 20:55:51,829 - INFO - Train Loss: 0.1024\n",
      "2025-12-04 20:55:51,830 - INFO - Val Loss: 0.0878\n",
      "2025-12-04 20:55:51,830 - INFO - Overall - Precision: 0.7859, Recall: 0.5404, F1: 0.5402\n",
      "2025-12-04 20:55:51,830 - INFO - en - Precision: 0.8230, Recall: 0.5689, F1: 0.5797\n",
      "2025-12-04 20:55:51,830 - INFO - es - Precision: 0.9328, Recall: 0.5312, F1: 0.5228\n",
      "2025-12-04 20:55:51,830 - INFO - it - Precision: 0.5683, Recall: 0.5087, F1: 0.4931\n",
      "2025-12-04 20:55:52,510 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_2_f1_0.5402.pt (F1: 0.5402, Fold: 3, Epoch: 2)\n",
      "2025-12-04 20:55:52,511 - INFO - \n",
      "Epoch 4/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.31it/s, loss=0.0987]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.01it/s]\n",
      "2025-12-04 20:55:57,550 - INFO - Train Loss: 0.0971\n",
      "2025-12-04 20:55:57,550 - INFO - Val Loss: 0.0805\n",
      "2025-12-04 20:55:57,551 - INFO - Overall - Precision: 0.7876, Recall: 0.6370, F1: 0.6733\n",
      "2025-12-04 20:55:57,551 - INFO - en - Precision: 0.8087, Recall: 0.7009, F1: 0.7358\n",
      "2025-12-04 20:55:57,551 - INFO - es - Precision: 0.9062, Recall: 0.6233, F1: 0.6666\n",
      "2025-12-04 20:55:57,551 - INFO - it - Precision: 0.6268, Recall: 0.5547, F1: 0.5672\n",
      "2025-12-04 20:55:58,247 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_3_f1_0.6733.pt (F1: 0.6733, Fold: 3, Epoch: 3)\n",
      "2025-12-04 20:55:58,247 - INFO - \n",
      "Epoch 5/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.45it/s, loss=0.0868]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.40it/s]\n",
      "2025-12-04 20:56:03,435 - INFO - Train Loss: 0.0854\n",
      "2025-12-04 20:56:03,435 - INFO - Val Loss: 0.0911\n",
      "2025-12-04 20:56:03,435 - INFO - Overall - Precision: 0.8524, Recall: 0.5839, F1: 0.6097\n",
      "2025-12-04 20:56:03,435 - INFO - en - Precision: 0.8552, Recall: 0.6397, F1: 0.6784\n",
      "2025-12-04 20:56:03,435 - INFO - es - Precision: 0.9394, Recall: 0.5833, F1: 0.6106\n",
      "2025-12-04 20:56:03,436 - INFO - it - Precision: 0.4421, Recall: 0.4963, F1: 0.4676\n",
      "2025-12-04 20:56:03,436 - INFO - \n",
      "Epoch 6/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.41it/s, loss=0.0788]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.08it/s]\n",
      "2025-12-04 20:56:08,614 - INFO - Train Loss: 0.0775\n",
      "2025-12-04 20:56:08,614 - INFO - Val Loss: 0.0811\n",
      "2025-12-04 20:56:08,614 - INFO - Overall - Precision: 0.7970, Recall: 0.6906, F1: 0.7263\n",
      "2025-12-04 20:56:08,614 - INFO - en - Precision: 0.8439, Recall: 0.7645, F1: 0.7955\n",
      "2025-12-04 20:56:08,614 - INFO - es - Precision: 0.8184, Recall: 0.7050, F1: 0.7435\n",
      "2025-12-04 20:56:08,614 - INFO - it - Precision: 0.6268, Recall: 0.5547, F1: 0.5672\n",
      "2025-12-04 20:56:09,293 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_5_f1_0.7263.pt (F1: 0.7263, Fold: 3, Epoch: 5)\n",
      "2025-12-04 20:56:09,294 - INFO - \n",
      "Epoch 7/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.36it/s, loss=0.0706]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.93it/s]\n",
      "2025-12-04 20:56:14,484 - INFO - Train Loss: 0.0695\n",
      "2025-12-04 20:56:14,484 - INFO - Val Loss: 0.0815\n",
      "2025-12-04 20:56:14,485 - INFO - Overall - Precision: 0.8588, Recall: 0.6467, F1: 0.6918\n",
      "2025-12-04 20:56:14,485 - INFO - en - Precision: 0.8550, Recall: 0.7248, F1: 0.7666\n",
      "2025-12-04 20:56:14,485 - INFO - es - Precision: 0.9462, Recall: 0.6354, F1: 0.6847\n",
      "2025-12-04 20:56:14,485 - INFO - it - Precision: 0.6963, Recall: 0.5373, F1: 0.5423\n",
      "2025-12-04 20:56:14,485 - INFO - \n",
      "Epoch 8/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.23it/s, loss=0.0668]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.13it/s]\n",
      "2025-12-04 20:56:19,690 - INFO - Train Loss: 0.0657\n",
      "2025-12-04 20:56:19,691 - INFO - Val Loss: 0.0796\n",
      "2025-12-04 20:56:19,691 - INFO - Overall - Precision: 0.8246, Recall: 0.6877, F1: 0.7295\n",
      "2025-12-04 20:56:19,691 - INFO - en - Precision: 0.8679, Recall: 0.7521, F1: 0.7926\n",
      "2025-12-04 20:56:19,691 - INFO - es - Precision: 0.9028, Recall: 0.6841, F1: 0.7384\n",
      "2025-12-04 20:56:19,691 - INFO - it - Precision: 0.6632, Recall: 0.5938, F1: 0.6139\n",
      "2025-12-04 20:56:20,355 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_3_epoch_7_f1_0.7295.pt (F1: 0.7295, Fold: 3, Epoch: 7)\n",
      "2025-12-04 20:56:20,355 - INFO - \n",
      "Epoch 9/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.35it/s, loss=0.0596]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.89it/s]\n",
      "2025-12-04 20:56:25,546 - INFO - Train Loss: 0.0586\n",
      "2025-12-04 20:56:25,546 - INFO - Val Loss: 0.0950\n",
      "2025-12-04 20:56:25,547 - INFO - Overall - Precision: 0.7887, Recall: 0.6502, F1: 0.6873\n",
      "2025-12-04 20:56:25,547 - INFO - en - Precision: 0.8376, Recall: 0.7138, F1: 0.7533\n",
      "2025-12-04 20:56:25,547 - INFO - es - Precision: 0.8191, Recall: 0.6181, F1: 0.6548\n",
      "2025-12-04 20:56:25,547 - INFO - it - Precision: 0.6750, Recall: 0.5957, F1: 0.6176\n",
      "2025-12-04 20:56:25,547 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:56:25,547 - INFO - Fold 5/5\n",
      "2025-12-04 20:56:25,547 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 4:\n",
      "  Train: 547 positive samples\n",
      "  Val:   138 positive samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../fine_tuned_models_mlm/twitter-xlm-roberta-base/final_MLM_model/model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-12-04 20:56:26,252 - INFO - Froze: Embeddings + First 10 Encoder Layers\n",
      "2025-12-04 20:56:26,252 - INFO - Trainable: Classification Head + Remaining Encoder Layers\n",
      "2025-12-04 20:56:26,253 - INFO - Trainable parameters: 14,767,874 / 278,045,186 (5.31%)\n",
      "2025-12-04 20:56:26,254 - INFO - Label weights: {0: 0.5834604821483064, 1: 3.495429616087751}\n",
      "2025-12-04 20:56:26,254 - INFO - Language weights: {'es': 0.9416953224870754, 'en': 1.0129222776114961, 'it': 1.045382399901429}\n",
      "2025-12-04 20:56:26,254 - INFO - Pos weight (for BCE): 5.9909\n",
      "2025-12-04 20:56:26,255 - INFO - \n",
      "Epoch 1/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.22it/s, loss=0.133]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 37.64it/s]\n",
      "2025-12-04 20:56:31,325 - INFO - Train Loss: 0.1305\n",
      "2025-12-04 20:56:31,325 - INFO - Val Loss: 0.1138\n",
      "2025-12-04 20:56:31,325 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 20:56:31,325 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 20:56:31,326 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:56:31,326 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:56:32,003 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_4_epoch_0_f1_0.4611.pt (F1: 0.4611, Fold: 4, Epoch: 0)\n",
      "2025-12-04 20:56:32,003 - INFO - \n",
      "Epoch 2/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.66it/s, loss=0.121]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.67it/s]\n",
      "2025-12-04 20:56:37,129 - INFO - Train Loss: 0.1190\n",
      "2025-12-04 20:56:37,129 - INFO - Val Loss: 0.0999\n",
      "2025-12-04 20:56:37,129 - INFO - Overall - Precision: 0.4279, Recall: 0.5000, F1: 0.4611\n",
      "2025-12-04 20:56:37,130 - INFO - en - Precision: 0.4140, Recall: 0.5000, F1: 0.4530\n",
      "2025-12-04 20:56:37,130 - INFO - es - Precision: 0.4290, Recall: 0.5000, F1: 0.4618\n",
      "2025-12-04 20:56:37,130 - INFO - it - Precision: 0.4410, Recall: 0.5000, F1: 0.4686\n",
      "2025-12-04 20:56:37,130 - INFO - \n",
      "Epoch 3/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 27.99it/s, loss=0.102] \n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.97it/s]\n",
      "2025-12-04 20:56:42,199 - INFO - Train Loss: 0.1003\n",
      "2025-12-04 20:56:42,199 - INFO - Val Loss: 0.0864\n",
      "2025-12-04 20:56:42,199 - INFO - Overall - Precision: 0.7590, Recall: 0.5820, F1: 0.6047\n",
      "2025-12-04 20:56:42,199 - INFO - en - Precision: 0.7627, Recall: 0.6401, F1: 0.6705\n",
      "2025-12-04 20:56:42,199 - INFO - es - Precision: 0.7124, Recall: 0.5452, F1: 0.5498\n",
      "2025-12-04 20:56:42,200 - INFO - it - Precision: 0.8202, Recall: 0.5398, F1: 0.5452\n",
      "2025-12-04 20:56:42,863 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_4_epoch_2_f1_0.6047.pt (F1: 0.6047, Fold: 4, Epoch: 2)\n",
      "2025-12-04 20:56:42,863 - INFO - \n",
      "Epoch 4/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.17it/s, loss=0.0939]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.05it/s]\n",
      "2025-12-04 20:56:47,923 - INFO - Train Loss: 0.0924\n",
      "2025-12-04 20:56:47,923 - INFO - Val Loss: 0.0846\n",
      "2025-12-04 20:56:47,923 - INFO - Overall - Precision: 0.7573, Recall: 0.6206, F1: 0.6520\n",
      "2025-12-04 20:56:47,924 - INFO - en - Precision: 0.7558, Recall: 0.7246, F1: 0.7383\n",
      "2025-12-04 20:56:47,924 - INFO - es - Precision: 0.7555, Recall: 0.5660, F1: 0.5822\n",
      "2025-12-04 20:56:47,924 - INFO - it - Precision: 0.9439, Recall: 0.5278, F1: 0.5229\n",
      "2025-12-04 20:56:48,599 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_4_epoch_3_f1_0.6520.pt (F1: 0.6520, Fold: 4, Epoch: 3)\n",
      "2025-12-04 20:56:48,600 - INFO - \n",
      "Epoch 5/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.53it/s, loss=0.0919]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.91it/s]\n",
      "2025-12-04 20:56:53,589 - INFO - Train Loss: 0.0903\n",
      "2025-12-04 20:56:53,589 - INFO - Val Loss: 0.0761\n",
      "2025-12-04 20:56:53,589 - INFO - Overall - Precision: 0.7489, Recall: 0.6887, F1: 0.7121\n",
      "2025-12-04 20:56:53,589 - INFO - en - Precision: 0.7776, Recall: 0.7304, F1: 0.7501\n",
      "2025-12-04 20:56:53,589 - INFO - es - Precision: 0.7739, Recall: 0.6981, F1: 0.7265\n",
      "2025-12-04 20:56:53,589 - INFO - it - Precision: 0.6621, Recall: 0.6129, F1: 0.6303\n",
      "2025-12-04 20:56:54,317 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_4_epoch_4_f1_0.7121.pt (F1: 0.7121, Fold: 4, Epoch: 4)\n",
      "2025-12-04 20:56:54,318 - INFO - \n",
      "Epoch 6/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.51it/s, loss=0.0776]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.76it/s]\n",
      "2025-12-04 20:56:59,313 - INFO - Train Loss: 0.0763\n",
      "2025-12-04 20:56:59,313 - INFO - Val Loss: 0.0753\n",
      "2025-12-04 20:56:59,314 - INFO - Overall - Precision: 0.8357, Recall: 0.6448, F1: 0.6874\n",
      "2025-12-04 20:56:59,314 - INFO - en - Precision: 0.8218, Recall: 0.7068, F1: 0.7440\n",
      "2025-12-04 20:56:59,314 - INFO - es - Precision: 0.8559, Recall: 0.6702, F1: 0.7175\n",
      "2025-12-04 20:56:59,314 - INFO - it - Precision: 0.9424, Recall: 0.5139, F1: 0.4965\n",
      "2025-12-04 20:56:59,314 - INFO - \n",
      "Epoch 7/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.07it/s, loss=0.0759]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.84it/s]\n",
      "2025-12-04 20:57:04,372 - INFO - Train Loss: 0.0746\n",
      "2025-12-04 20:57:04,373 - INFO - Val Loss: 0.0931\n",
      "2025-12-04 20:57:04,373 - INFO - Overall - Precision: 0.8422, Recall: 0.6111, F1: 0.6469\n",
      "2025-12-04 20:57:04,373 - INFO - en - Precision: 0.8466, Recall: 0.6848, F1: 0.7278\n",
      "2025-12-04 20:57:04,373 - INFO - es - Precision: 0.8584, Recall: 0.6007, F1: 0.6342\n",
      "2025-12-04 20:57:04,373 - INFO - it - Precision: 0.6922, Recall: 0.5120, F1: 0.4948\n",
      "2025-12-04 20:57:04,373 - INFO - \n",
      "Epoch 8/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.37it/s, loss=0.0639]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 38.95it/s]\n",
      "2025-12-04 20:57:09,385 - INFO - Train Loss: 0.0628\n",
      "2025-12-04 20:57:09,386 - INFO - Val Loss: 0.0717\n",
      "2025-12-04 20:57:09,386 - INFO - Overall - Precision: 0.8139, Recall: 0.7359, F1: 0.7664\n",
      "2025-12-04 20:57:09,386 - INFO - en - Precision: 0.8140, Recall: 0.7952, F1: 0.8041\n",
      "2025-12-04 20:57:09,386 - INFO - es - Precision: 0.8769, Recall: 0.7119, F1: 0.7620\n",
      "2025-12-04 20:57:09,387 - INFO - it - Precision: 0.7525, Recall: 0.6759, F1: 0.7042\n",
      "2025-12-04 20:57:10,056 - INFO - Saved checkpoint: ../fine_tuned_models/mlm/final_model/checkpoints/fold_4_epoch_7_f1_0.7664.pt (F1: 0.7664, Fold: 4, Epoch: 7)\n",
      "2025-12-04 20:57:10,056 - INFO - \n",
      "Epoch 9/9\n",
      "Training: 100%|██████████| 120/120 [00:04<00:00, 28.22it/s, loss=0.0649]\n",
      "Validating: 100%|██████████| 30/30 [00:00<00:00, 39.03it/s]\n",
      "2025-12-04 20:57:15,089 - INFO - Train Loss: 0.0638\n",
      "2025-12-04 20:57:15,089 - INFO - Val Loss: 0.1104\n",
      "2025-12-04 20:57:15,089 - INFO - Overall - Precision: 0.9088, Recall: 0.6358, F1: 0.6826\n",
      "2025-12-04 20:57:15,089 - INFO - en - Precision: 0.8847, Recall: 0.6536, F1: 0.6977\n",
      "2025-12-04 20:57:15,089 - INFO - es - Precision: 0.9294, Recall: 0.6962, F1: 0.7547\n",
      "2025-12-04 20:57:15,089 - INFO - it - Precision: 0.9439, Recall: 0.5278, F1: 0.5229\n",
      "2025-12-04 20:57:15,091 - INFO - \n",
      "Results saved to: ../results/fine_tuned_mlm/final_model/training_results.csv\n",
      "2025-12-04 20:57:15,091 - INFO - \n",
      "================================================================================\n",
      "2025-12-04 20:57:15,091 - INFO - Best Models Saved:\n",
      "2025-12-04 20:57:15,091 - INFO - ================================================================================\n",
      "2025-12-04 20:57:15,091 - INFO - Fold 0, Epoch 8: F1=0.7845 -> ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt\n",
      "2025-12-04 20:57:15,091 - INFO - Fold 0, Epoch 7: F1=0.7841 -> ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_7_f1_0.7841.pt\n"
     ]
    }
   ],
   "source": [
    "final_config, final_model_path = train_final_model(best_trial, augmented_data, base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec615d-fc0c-4e7f-bb4a-cdb45d30a918",
   "metadata": {},
   "source": [
    "### Inference with Peformance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed94e72d-108e-4398-be0c-327860e18551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 8964 samples...\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_7_f1_0.7841.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 281/281 [00:07<00:00, 38.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE RESULTS ON TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "Overall Metrics:\n",
      "  Macro Precision: 0.8027\n",
      "  Macro Recall:    0.7472\n",
      "  Macro F1:        0.7706\n",
      "\n",
      "Per-Language Metrics:\n",
      "  EN:\n",
      "    Precision: 0.7634\n",
      "    Recall:    0.7952\n",
      "    F1:        0.7777\n",
      "  ES:\n",
      "    Precision: 0.8446\n",
      "    Recall:    0.8117\n",
      "    F1:        0.8268\n",
      "  IT:\n",
      "    Precision: 0.8677\n",
      "    Recall:    0.6347\n",
      "    F1:        0.6785\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_config = InferenceConfig()\n",
    "inference_config.CHECKPOINT_PATH = str(final_model_path)\n",
    "\n",
    "inference_results = run_inference(merged_data, inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee71f095-7b9d-488f-b083-ee2a81071f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Trials: 10\n",
      "Best Trial: 2\n",
      "Best F1 Score (CV): 0.7919\n",
      "Final Inference F1: 0.7706\n",
      "\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.00025959425503112657\n",
      "  weight_decay: 0.021233911067827616\n",
      "  batch_size: 32\n",
      "  num_epochs: 9\n",
      "\n",
      "Model saved at: ../fine_tuned_models/mlm/final_model\n",
      "Results saved at: ../results/fine_tuned_mlm/final_model/\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nOptuna Trials: {len(study.trials)}\")\n",
    "print(f\"Best Trial: {best_trial.number}\")\n",
    "print(f\"Best F1 Score (CV): {best_trial.value:.4f}\")\n",
    "print(f\"Final Inference F1: {inference_results['metrics']['overall']['macro_f1']:.4f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, val in best_trial.params.items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "print(f\"\\nModel saved at: {final_config.OUTPUT_DIR}\")\n",
    "print(f\"Results saved at: {final_config.RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3c979-8053-4eaa-8351-388be4ffb278",
   "metadata": {},
   "source": [
    "# Prediction Threshold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b883f053-efd0-46ff-b40c-7ac3ec827442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b846991b-2bb8-4d87-a3b8-c648b9201775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability stats:\n",
      "Min:0.0010\n",
      "Max:0.9996\n",
      "Mean:0.2448\n",
      "Median:0.1993\n",
      " threshold       f1  precision   recall\n",
      "       0.1 0.417996   0.598311 0.664659\n",
      "       0.2 0.574726   0.628261 0.761282\n",
      "       0.3 0.690419   0.674178 0.810448\n",
      "       0.4 0.759620   0.734950 0.798832\n",
      "       0.5 0.770560   0.802662 0.747208\n",
      "       0.6 0.722379   0.852444 0.675470\n",
      "       0.7 0.632847   0.870776 0.599615\n",
      "       0.8 0.564557   0.882454 0.554968\n",
      "       0.9 0.511879   0.902955 0.525441\n",
      "\n",
      "================================================================================\n",
      "LANGUAGE-SPECIFIC OPTIMAL THRESHOLDS\n",
      "================================================================================\n",
      "\n",
      "EN:\n",
      "Optimal Threshold: 0.50\n",
      "F1 Score: 0.7777\n",
      "Precision: 0.7634\n",
      "Recall: 0.7952\n",
      "\n",
      "ES:\n",
      "Optimal Threshold: 0.45\n",
      "F1 Score: 0.8314\n",
      "Precision: 0.8177\n",
      "Recall: 0.8473\n",
      "\n",
      "IT:\n",
      "Optimal Threshold: 0.35\n",
      "F1 Score: 0.7710\n",
      "Precision: 0.7807\n",
      "Recall: 0.7624\n",
      "\n",
      "================================================================================\n",
      "APPLYING LANGUAGE-SPECIFIC THRESHOLDS\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Default Threshold (0.5):\n",
      "F1: 0.7706\n",
      "Precision: 0.8027\n",
      "Recall: 0.7472\n",
      "\n",
      "Language-Specific Thresholds:\n",
      "  EN: 0.50  ES: 0.45  IT: 0.35\n",
      "F1: 0.7940\n",
      "Precision: 0.7869\n",
      "Recall: 0.8016\n",
      "\n",
      "Overall Improvement:\n",
      "F1 Change: +0.0234 (+3.0%)\n",
      "Precision Change: -0.0157\n",
      "Recall Change: +0.0544\n",
      "\n",
      "================================================================================\n",
      "PER-LANGUAGE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "EN:\n",
      "Default (0.5):\n",
      "F1: 0.7777, Precision: 0.7634, Recall: 0.7952\n",
      "Language-Specific (0.50):\n",
      "F1: 0.7777, Precision: 0.7634, Recall: 0.7952\n",
      "Improvement: +0.0000 (+0.0%)\n",
      "\n",
      "ES:\n",
      "Default (0.5):\n",
      "F1: 0.8268, Precision: 0.8446, Recall: 0.8117\n",
      "Language-Specific (0.45):\n",
      "F1: 0.8314, Precision: 0.8177, Recall: 0.8473\n",
      "Improvement: +0.0045 (+0.5%)\n",
      "\n",
      "IT:\n",
      "Default (0.5):\n",
      "F1: 0.6785, Precision: 0.8677, Recall: 0.6347\n",
      "Language-Specific (0.35):\n",
      "F1: 0.7710, Precision: 0.7807, Recall: 0.7624\n",
      "Improvement: +0.0925 (+13.6%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED CLASSIFICATION REPORT - LANGUAGE-SPECIFIC THRESHOLDS\n",
      "================================================================================\n",
      "\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.93      0.94      7680\n",
      "     Class 1       0.63      0.67      0.65      1284\n",
      "\n",
      "    accuracy                           0.90      8964\n",
      "   macro avg       0.79      0.80      0.79      8964\n",
      "weighted avg       0.90      0.90      0.90      8964\n",
      "\n",
      "\n",
      "Per-Language Classification Reports:\n",
      "\n",
      "EN (Threshold: 0.50):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.92      0.93      2560\n",
      "     Class 1       0.58      0.67      0.62       428\n",
      "\n",
      "    accuracy                           0.88      2988\n",
      "   macro avg       0.76      0.80      0.78      2988\n",
      "weighted avg       0.89      0.88      0.89      2988\n",
      "\n",
      "\n",
      "ES (Threshold: 0.45):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.96      0.94      0.95      2560\n",
      "     Class 1       0.68      0.75      0.71       428\n",
      "\n",
      "    accuracy                           0.91      2988\n",
      "   macro avg       0.82      0.85      0.83      2988\n",
      "weighted avg       0.92      0.91      0.92      2988\n",
      "\n",
      "\n",
      "IT (Threshold: 0.35):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.94      0.94      2560\n",
      "     Class 1       0.63      0.58      0.61       428\n",
      "\n",
      "    accuracy                           0.89      2988\n",
      "   macro avg       0.78      0.76      0.77      2988\n",
      "weighted avg       0.89      0.89      0.89      2988\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "language  optimal_threshold\n",
      "      en               0.50\n",
      "      es               0.45\n",
      "      it               0.35\n"
     ]
    }
   ],
   "source": [
    "probs = np.array(inference_results['probabilities'])  # Shape: (N, 2)\n",
    "probs_class_1 = probs[:, 1]  # Get positive class probabilities\n",
    "labels = np.array(inference_results['labels'])\n",
    "languages = inference_results['languages']\n",
    "\n",
    "print(f\"Probability stats:\")\n",
    "print(f\"Min:{probs_class_1.min():.4f}\")\n",
    "print(f\"Max:{probs_class_1.max():.4f}\")\n",
    "print(f\"Mean:{probs_class_1.mean():.4f}\")\n",
    "print(f\"Median:{np.median(probs_class_1):.4f}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for threshold in np.arange(0.1, 1.0, 0.1):\n",
    "    # Apply threshold\n",
    "    preds = (probs_class_1 >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(results)\n",
    "print(threshold_df.to_string(index=False))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LANGUAGE-SPECIFIC OPTIMAL THRESHOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "language_optimal_thresholds = {}\n",
    "\n",
    "for lang in sorted(set(languages)):\n",
    "    lang_mask = np.array(languages) == lang\n",
    "    lang_probs = probs_class_1[lang_mask]\n",
    "    lang_labels = np.array(labels)[lang_mask]\n",
    "    \n",
    "    best_threshold = None\n",
    "    best_f1 = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    \n",
    "    # Test all thresholds for this language\n",
    "    for threshold in np.arange(0.05, 1.0, 0.05):\n",
    "        preds = (lang_probs >= threshold).astype(int)\n",
    "        f1 = f1_score(lang_labels, preds, average=\"macro\", zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision_score(lang_labels, preds, average=\"macro\", zero_division=0)\n",
    "            best_recall = recall_score(lang_labels, preds, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    language_optimal_thresholds[lang] = best_threshold\n",
    "    \n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    print(f\"Optimal Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"Precision: {best_precision:.4f}\")\n",
    "    print(f\"Recall: {best_recall:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING LANGUAGE-SPECIFIC THRESHOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "language_specific_preds = np.zeros_like(probs_class_1, dtype=int)\n",
    "\n",
    "for lang in sorted(set(languages)):\n",
    "    lang_mask = np.array(languages) == lang\n",
    "    threshold = language_optimal_thresholds[lang]\n",
    "    language_specific_preds[lang_mask] = (probs_class_1[lang_mask] >= threshold).astype(int)\n",
    "\n",
    "print(\"\\nOVERALL METRICS COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "default_preds = (probs_class_1 >= 0.5).astype(int)\n",
    "default_f1 = f1_score(labels, default_preds, average=\"macro\", zero_division=0)\n",
    "default_precision = precision_score(labels, default_preds, average=\"macro\", zero_division=0)\n",
    "default_recall = recall_score(labels, default_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "langspec_f1 = f1_score(labels, language_specific_preds, average=\"macro\", zero_division=0)\n",
    "langspec_precision = precision_score(labels, language_specific_preds, average=\"macro\", zero_division=0)\n",
    "langspec_recall = recall_score(labels, language_specific_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"\\nDefault Threshold (0.5):\")\n",
    "print(f\"F1: {default_f1:.4f}\")\n",
    "print(f\"Precision: {default_precision:.4f}\")\n",
    "print(f\"Recall: {default_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nLanguage-Specific Thresholds:\")\n",
    "for lang in sorted(set(languages)):\n",
    "    print(f\"  {lang.upper()}: {language_optimal_thresholds[lang]:.2f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"F1: {langspec_f1:.4f}\")\n",
    "print(f\"Precision: {langspec_precision:.4f}\")\n",
    "print(f\"Recall: {langspec_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Improvement:\")\n",
    "print(f\"F1 Change: {langspec_f1 - default_f1:+.4f} ({(langspec_f1/default_f1 - 1)*100:+.1f}%)\")\n",
    "print(f\"Precision Change: {langspec_precision - default_precision:+.4f}\")\n",
    "print(f\"Recall Change: {langspec_recall - default_recall:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-LANGUAGE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for lang in sorted(set(languages)):\n",
    "    lang_mask = np.array(languages) == lang\n",
    "    lang_labels = np.array(labels)[lang_mask]\n",
    "    \n",
    "    default_lang_preds = default_preds[lang_mask]\n",
    "    default_lang_f1 = f1_score(lang_labels, default_lang_preds, average=\"macro\", zero_division=0)\n",
    "    default_lang_precision = precision_score(lang_labels, default_lang_preds, average=\"macro\", zero_division=0)\n",
    "    default_lang_recall = recall_score(lang_labels, default_lang_preds, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    langspec_lang_preds = language_specific_preds[lang_mask]\n",
    "    langspec_lang_f1 = f1_score(lang_labels, langspec_lang_preds, average=\"macro\", zero_division=0)\n",
    "    langspec_lang_precision = precision_score(lang_labels, langspec_lang_preds, average=\"macro\", zero_division=0)\n",
    "    langspec_lang_recall = recall_score(lang_labels, langspec_lang_preds, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    print(f\"Default (0.5):\")\n",
    "    print(f\"F1: {default_lang_f1:.4f}, Precision: {default_lang_precision:.4f}, Recall: {default_lang_recall:.4f}\")\n",
    "    print(f\"Language-Specific ({language_optimal_thresholds[lang]:.2f}):\")\n",
    "    print(f\"F1: {langspec_lang_f1:.4f}, Precision: {langspec_lang_precision:.4f}, Recall: {langspec_lang_recall:.4f}\")\n",
    "    print(f\"Improvement: {langspec_lang_f1 - default_lang_f1:+.4f} ({(langspec_lang_f1/default_lang_f1 - 1)*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT - LANGUAGE-SPECIFIC THRESHOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nOverall Classification Report:\")\n",
    "print(classification_report(labels, language_specific_preds, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "print(\"\\nPer-Language Classification Reports:\")\n",
    "for lang in sorted(set(languages)):\n",
    "    lang_mask = np.array(languages) == lang\n",
    "    lang_labels = np.array(labels)[lang_mask]\n",
    "    lang_preds = language_specific_preds[lang_mask]\n",
    "    \n",
    "    print(f\"\\n{lang.upper()} (Threshold: {language_optimal_thresholds[lang]:.2f}):\")\n",
    "    print(classification_report(lang_labels, lang_preds, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'language': list(language_optimal_thresholds.keys()),\n",
    "    'optimal_threshold': list(language_optimal_thresholds.values())\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67dc4d5-ae71-47b2-9f08-9a6b4fd22c96",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb87c1b-d653-46af-b3ad-358d52968eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetune.finetuner import run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8672e1c6-b630-4e47-be2f-922857c80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    NUM_LABELS = 2\n",
    "    CHECKPOINT_PATH = \"../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt\"\n",
    "    \n",
    "    # YOUR OPTIMAL THRESHOLDS\n",
    "    LANGUAGE_THRESHOLDS = {\n",
    "        'en': 0.50,\n",
    "        'es': 0.50,\n",
    "        'it': 0.50\n",
    "    }\n",
    "\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class InferenceConfigThreshold:\n",
    "    MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "    NUM_LABELS = 2\n",
    "    # CHECKPOINT_PATH = \"../fine_tuned_models/base/final_model/checkpoints/fold_0_epoch_8_f1_0.7829.pt\"\n",
    "    CHECKPOINT_PATH = \"../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt\"\n",
    "    \n",
    "    # YOUR OPTIMAL THRESHOLDS\n",
    "    LANGUAGE_THRESHOLDS = {\n",
    "        'en': 0.50,\n",
    "        'es': 0.45,\n",
    "        'it': 0.35\n",
    "    }\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4952d6a5-6b5b-402b-ad31-c3a3e5ba3e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files: ['es_test.csv', 'it_test.csv', 'en_test.csv']\n"
     ]
    }
   ],
   "source": [
    "test_root = \"../data/test_sets/Test Set/\"\n",
    "test_files = [file for file in os.listdir(test_root) if (file.endswith(\".csv\") and (\"test\" in file))]\n",
    "print(f\"training files: {test_files}\")\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for file in test_files:\n",
    "    temp_df = pd.read_csv(os.path.join(test_root, file))\n",
    "    if \"en\" in file:\n",
    "        temp_df[\"bio\"] = [None] * temp_df.shape[0]\n",
    "    test_df = pd.concat([test_df, temp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a3d8ed-0f8f-4c61-9703-d6b307aa502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06bff44b-5c44-4c2c-bb76-2206ea1a6161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 1995 samples...\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 63/63 [00:01<00:00, 39.00it/s]\n"
     ]
    }
   ],
   "source": [
    "results = run_test(test_df, InferenceConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66091f3b-7b7a-4bae-af0a-c4783bb7c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1995, 3)\n"
     ]
    }
   ],
   "source": [
    "submission_folder = \"../submissions/\"\n",
    "os.makedirs(submission_folder, exist_ok=True)\n",
    "submission_file = os.path.join(submission_folder, \"multipride2025_KIT-TIP-NLP_2.tsv\")\n",
    "final_submission = dict()\n",
    "final_submission[\"id\"] = results[\"ids\"]\n",
    "final_submission[\"label\"] = results[\"predictions\"]\n",
    "final_submission[\"lang\"] = results[\"languages\"]\n",
    "\n",
    "\n",
    "final_submission = pd.DataFrame.from_dict(final_submission)\n",
    "print(final_submission.shape)\n",
    "final_submission.to_csv(submission_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11324d3-05bb-4bba-9bdb-cc7a49a77046",
   "metadata": {},
   "source": [
    "### For ablation Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612bb7b1-f534-4819-a254-6e09645c9193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 1995 samples...\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: ../fine_tuned_models/mlm/final_model/checkpoints/fold_0_epoch_8_f1_0.7845.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 63/63 [00:01<00:00, 39.00it/s]\n"
     ]
    }
   ],
   "source": [
    "results = run_test(test_df, InferenceConfigThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a8754ef-b8fa-40f1-be7e-520ebe5a3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1995, 3)\n"
     ]
    }
   ],
   "source": [
    "submission_file = os.path.join(submission_folder, \"multipride2025_KIT-TIP-NLP_4.tsv\")\n",
    "final_submission = dict()\n",
    "final_submission[\"id\"] = results[\"ids\"]\n",
    "final_submission[\"label\"] = results[\"predictions\"]\n",
    "final_submission[\"lang\"] = results[\"languages\"]\n",
    "\n",
    "\n",
    "final_submission = pd.DataFrame.from_dict(final_submission)\n",
    "print(final_submission.shape)\n",
    "final_submission.to_csv(submission_file, sep=\"\\t\", index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fc235-6dec-459c-b188-52f3381163e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipride",
   "language": "python",
   "name": "multipride"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
